{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b3a4ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\jz421\\\\Desktop\\\\GlobalLocal', 'C:\\\\Users\\\\jz421\\\\Desktop\\\\GlobalLocal\\\\IEEG_Pipelines', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\python311.zip', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\DLLs', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg', '', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\win32', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages\\\\Pythonwin', 'C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/', 'C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/', 'C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/', 'C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/', 'C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/', 'C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/', 'C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/', 'C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/', 'C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/', 'C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print(sys.path)\n",
    "sys.path.append(\"C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/\") #need to do this cuz otherwise ieeg isn't added to path...\n",
    "\n",
    "# Get the absolute path to the directory containing the current script\n",
    "# For GlobalLocal/src/analysis/preproc/make_epoched_data.py, this is GlobalLocal/src/analysis/preproc\n",
    "try:\n",
    "    # This will work if running as a .py script\n",
    "    current_file_path = os.path.abspath(__file__)\n",
    "    current_script_dir = os.path.dirname(current_file_path)\n",
    "except NameError:\n",
    "    # This will be executed if __file__ is not defined (e.g., in a Jupyter Notebook)\n",
    "    # os.getcwd() often gives the directory of the notebook,\n",
    "    # or the directory from which the Jupyter server was started.\n",
    "    current_script_dir = os.getcwd()\n",
    "\n",
    "# Navigate up three levels to get to the 'GlobalLocal' directory\n",
    "project_root = os.path.abspath(os.path.join(current_script_dir, '..', '..', '..'))\n",
    "\n",
    "# Add the 'GlobalLocal' directory to sys.path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root) # insert at the beginning to prioritize it\n",
    "    \n",
    "from functools import partial\n",
    "from ieeg.navigate import channel_outlier_marker, trial_ieeg, crop_empty_data, \\\n",
    "    outliers_to_nan\n",
    "from ieeg.io import raw_from_layout, get_data\n",
    "from ieeg.timefreq.utils import crop_pad\n",
    "from ieeg.timefreq import gamma\n",
    "from ieeg.calc.scaling import rescale\n",
    "import mne\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ieeg.calc.reshape import make_data_same\n",
    "from ieeg.calc.stats import time_perm_cluster\n",
    "from ieeg.calc.mat import LabeledArray, combine\n",
    "from ieeg.viz.parula import parula_map\n",
    "\n",
    "# TODO: hmm fix these utils imports, import the funcs individually. 6/1/25.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas import read_csv\n",
    "import scipy.stats as stats\n",
    "import joblib\n",
    "\n",
    "from scipy.ndimage import label\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# rsa toolbox imports\n",
    "from rsatoolbox.io.mne import read_epochs\n",
    "from rsatoolbox.data.ops import merge_datasets\n",
    "from rsatoolbox.rdm import calc_rdm_movie\n",
    "from rsatoolbox.rdm.calc import _parse_input\n",
    "from rsatoolbox.util.build_rdm import _build_rdms\n",
    "from rsatoolbox.rdm import compare\n",
    "from rsatoolbox.vis import show_rdm\n",
    "from rsatoolbox.vis.timecourse import plot_timecourse\n",
    "\n",
    "from os.path import join, expanduser, basename\n",
    "import glob, json\n",
    "import numpy, tqdm, mne, pandas\n",
    "import rsatoolbox\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import copy\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from ieeg.decoding.decoders import PcaLdaClassification\n",
    "from ieeg.calc.oversample import MinimumNaNSplit\n",
    "from ieeg.calc.fast import mixup\n",
    "\n",
    "from src.analysis.config import experiment_conditions\n",
    "\n",
    "from src.analysis.utils.labeled_array_utils import (\n",
    "    put_data_in_labeled_array_per_roi_subject,\n",
    "    remove_nans_from_labeled_array,\n",
    "    remove_nans_from_all_roi_labeled_arrays,\n",
    "    concatenate_conditions_by_string,\n",
    "    get_data_in_time_range\n",
    ")\n",
    "\n",
    "from src.analysis.decoding.decoding import (\n",
    "    process_and_balance_data_for_decoding, \n",
    "    get_and_plot_confusion_matrix_for_rois_jim,\n",
    "    Decoder, \n",
    "    windower,\n",
    "    get_confusion_matrices_for_rois_time_window_decoding_jim,\n",
    "    compute_accuracies,\n",
    "    perform_time_perm_cluster_test_for_accuracies,\n",
    "    plot_accuracies\n",
    ")\n",
    "\n",
    "from src.analysis.spec.wavelet_functions import get_uncorrected_wavelets, get_uncorrected_multitaper, get_sig_tfr_differences, plot_mask_pages\n",
    "from src.analysis.spec.subjects_tfr_objects_functions import load_or_make_subjects_tfr_objects\n",
    "\n",
    "from src.analysis.utils.general_utils import (\n",
    "    make_or_load_subjects_electrodes_to_ROIs_dict,\n",
    "    get_good_data,\n",
    "    get_sig_chans_per_subject,\n",
    "    make_sig_electrodes_per_subject_and_roi_dict,\n",
    "    calculate_total_electrodes,\n",
    "    check_sampling_rates\n",
    ")\n",
    "\n",
    "import mne.time_frequency\n",
    "from ieeg.calc.scaling import rescale\n",
    "from ieeg.timefreq.utils import wavelet_scaleogram, crop_pad\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from typing import Union, List, Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d07116",
   "metadata": {},
   "source": [
    "#### 0. Load data.   \n",
    "Need a way to load in the frequency information too, not just trials x channels x timepoints. Because I'm going to use frequency as a decoding feature too. For each electrode, for each training set, I think I can just mask the multitaper with the significant clusters and use that as the decoding feature, and then concatenate across electrodes to build the full training matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c04940ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load the subjects' electrodes-to-ROIs dictionary...\n",
      "Loaded data from C:\\Users\\jz421\\Desktop\\GlobalLocal\\src\\analysis\\config\\subjects_electrodestoROIs_dict.json\n",
      "Dictionary loaded successfully. Ready to proceed!\n"
     ]
    }
   ],
   "source": [
    "# subjects = ['D0057','D0059', 'D0063', 'D0065', 'D0069', 'D0071', 'D0077', 'D0090', 'D0094', 'D0100', 'D0102', 'D0103', 'D0107A', 'D0110', 'D0116', 'D0117', 'D0121']\n",
    "\n",
    "# params - these will become input variables once i functionalize this stuff\n",
    "subjects = ['D0057']\n",
    "signal_times = [-1.0, 1.5]\n",
    "acc_trials_only = False\n",
    "error_trials_only = False\n",
    "stat_func = partial(ttest_ind, equal_var=False, nan_policy='omit')\n",
    "p_thresh = 0.05\n",
    "ignore_adjacency = 1 # ignore the channels dimension for clusters, just find clusters over frequency and time\n",
    "n_perm = 20\n",
    "n_jobs = 1\n",
    "freqs = np.arange(2, 200., 4.)\n",
    "n_cycles = freqs / 2\n",
    "return_itc = False\n",
    "time_bandwidth=10 \n",
    "spec_method = 'multitaper'\n",
    "average=False\n",
    "seed=None\n",
    "tails=2\n",
    "n_splits=2\n",
    "random_state=42\n",
    "\n",
    "# load in subjects electrodes to rois dict. If it doesn't already exist, make it and then load it.\n",
    "config_dir = r'C:\\Users\\jz421\\Desktop\\GlobalLocal\\src\\analysis\\config'\n",
    "subjects_electrodestoROIs_dict = make_or_load_subjects_electrodes_to_ROIs_dict(subjects, task='GlobalLocal', LAB_root=None, save_dir=config_dir, \n",
    "                                                filename='subjects_electrodestoROIs_dict.json')\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "\n",
    "# get box directory depending on OS\n",
    "if os.name == 'nt': # windows\n",
    "    LAB_root = os.path.join(HOME, \"Box\", \"CoganLab\")\n",
    "else: # mac\n",
    "    LAB_root = os.path.join(HOME, \"Library\", \"CloudStorage\", \"Box-Box\", \"CoganLab\")\n",
    "\n",
    "layout = get_data(\"GlobalLocal\", root=LAB_root)\n",
    "\n",
    "task='GlobalLocal'\n",
    "conditions = experiment_conditions.stimulus_switch_type_conditions # set this to whichever conditions you're running\n",
    "\n",
    "stimulus_locked = True  #toggle\n",
    "response_locked = not stimulus_locked\n",
    "\n",
    "if stimulus_locked:\n",
    "    # epochs_root_file = \"Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_4.0-8.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False\"\n",
    "    epochs_root_file = \"Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False\"\n",
    "    # epochs_root_file = \"Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_0.0-30.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False\"\n",
    "\n",
    "elif response_locked:\n",
    "    # epochs_root_file = \"Response_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_4.0-8.0_padLength_0.5s_stat_func_ttest_ind\"\n",
    "    epochs_root_file = \"Response_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind\"\n",
    "\n",
    "condition_names = list(conditions.keys()) # get the condition names as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "531435b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if conditions == experiment_conditions.stimulus_conditions:\n",
    "    conditions_save_name = 'stimulus_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_experiment_conditions:\n",
    "    conditions_save_name = 'stimulus_experiment_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_main_effect_conditions:\n",
    "    conditions_save_name = 'stimulus_main_effect_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_lwpc_conditions:\n",
    "    conditions_save_name = 'stimulus_lwpc_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_lwps_conditions:\n",
    "    conditions_save_name = 'stimulus_lwps_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_big_letter_conditions:\n",
    "    conditions_save_name = 'stimulus_big_letter_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_small_letter_conditions:\n",
    "    conditions_save_name = 'stimulus_small_letter_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_task_conditions:\n",
    "    conditions_save_name = 'stimulus_task_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_congruency_conditions:\n",
    "    conditions_save_name = 'stimulus_congruency_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_switch_type_conditions:\n",
    "    conditions_save_name = 'stimulus_switch_type_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "\n",
    "elif conditions == experiment_conditions.response_conditions:\n",
    "    conditions_save_name = 'response_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.response_experiment_conditions:\n",
    "    conditions_save_name = 'response_experiment_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.response_big_letter_conditions:\n",
    "    conditions_save_name = 'response_big_letter_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.response_small_letter_conditions:\n",
    "    conditions_save_name = 'response_small_letter_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.response_task_conditions:\n",
    "    conditions_save_name = 'response_task_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.response_congruency_conditions:\n",
    "    conditions_save_name = 'response_congruency_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.response_switch_type_conditions:\n",
    "    conditions_save_name = 'response_switch_type_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54dfc3",
   "metadata": {},
   "source": [
    "load stimulus significant channels. Compare ROI electrodes in next cell to these to see if they're included.\n",
    "\n",
    "maybe do response significant channels too/instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ce4b49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded significant channels for subject D0057\n"
     ]
    }
   ],
   "source": [
    "sig_chans_per_subject = get_sig_chans_per_subject(subjects, epochs_root_file, task='GlobalLocal', LAB_root=None)\n",
    "\n",
    "# Now sig_chans_per_subject dictionary is populated with significant channels for each subject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c70bea6",
   "metadata": {},
   "source": [
    "get the significant electrodes across subjects for each ROI of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43db4297",
   "metadata": {},
   "source": [
    "dlPFC based on Yamagishi et al 2016 definition is G_front_middle, G_front_sup, S_front_inf, S_front_middle, S_front_sup\n",
    "ACC based on Destrieux et al 2010 definition is G_and_S_cingul-Ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af7055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For subject D0057, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['RAI6', 'RAI12', 'RAI13', 'RAI14', 'RAI15', 'RAI16', 'RPI15', 'RPI14', 'RAMF10', 'RAMF11', 'RAMF12', 'RAMF13', 'RAMF14', 'RAIF11', 'RAIF12', 'RAIF13', 'RAIF14']\n",
      "For subject D0059, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['LMMF9', 'LMMF11', 'LMMF10', 'LMMF12', 'LPSF16']\n",
      "For subject D0063, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['LASF10', 'LASF11', 'LASF14', 'LASF15', 'LASF16', 'LMSF5', 'LMSF6', 'LMSF12', 'LPSF10', 'LPSF12', 'ROF16', 'RAI16', 'RAMF11', 'RAMF12', 'RAMF13', 'RAMF14', 'RMMF13', 'RMMF14', 'RAI4', 'RAI6', 'RAI5', 'RAI10', 'RAI11', 'RASF15', 'RASF16', 'RMSF8', 'RMSF9', 'RMSF10', 'RMSF11', 'RMSF12', 'RMSF7', 'RAMF8', 'RAMF9', 'RAMF10', 'RMMF9', 'RMMF10']\n",
      "For subject D0065, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['RASF13', 'RASF14', 'RASF15', 'RASF16', 'RMSF11', 'RMSF12', 'RMSF13', 'RMSF14', 'RI7']\n",
      "For subject D0069, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['LOF8']\n",
      "For subject D0071, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['RFO14', 'RFO16', 'RIA4', 'RIP6', 'RIA5', 'RIA11', 'RIA12', 'RIA13', 'RIA14', 'RIA16', 'RIP14', 'RIP15', 'RIP16']\n",
      "For subject D0077, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: []\n",
      "For subject D0090, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['RFO15', 'RIA6', 'RIA11', 'RIA12', 'RIA14', 'RIA15', 'RIA16', 'RIP7']\n",
      "For subject D0094, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['LFO12', 'LFO13', 'LFO15', 'LFO16', 'LFAM8', 'LFAM9', 'LFAM10', 'LFAM13', 'LFAM14', 'LFPM10', 'LFPM11', 'LFPM12', 'LPAS1', 'LIA16', 'LFAI3', 'LFAI4', 'LFAI5', 'LFPI10', 'LPAI9', 'LPAI10', 'LIA4', 'LIA5', 'LFAI9', 'LFAI10', 'LIA11', 'LIA12', 'LIA13', 'LIA14']\n",
      "For subject D0100, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: []\n",
      "For subject D0102, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['RFO13', 'RFO14', 'RFAM15', 'RFAI2', 'RFAI3']\n",
      "For subject D0103, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['LFAM8', 'LFAM9', 'LAI13', 'LAI14', 'LFAM15', 'LAI4', 'LAI7', 'LAI8', 'LAI18', 'LFO15', 'LFAI4']\n",
      "For subject D0107A, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['RFOA16', 'RFOA17', 'RFOA18', 'RFAI3', 'RIA5', 'RIA6', 'RFAM7', 'RFAM9', 'RFAM10', 'RFAM11', 'RFAM12', 'RFMM9', 'RFMM11', 'RFMM12', 'RFMM13', 'RFMM14', 'RFMM15', 'RFMM8', 'RIA12', 'RIA13', 'RIA14', 'RIA15', 'RIA16', 'RIA17', 'RIA18']\n",
      "For subject D0110, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['LASF10', 'LOF13', 'LOF15', 'LINS4', 'LINS5']\n",
      "For subject D0116, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['LFOA14', 'LFOA17', 'LFOA18', 'LFOP15', 'LFAM10', 'LFAM11', 'LFMM14', 'LAI12', 'LAI16', 'LFAM12', 'LAI7', 'LAI8', 'LFMI3', 'LFMI5', 'LFMI6', 'RFOA14', 'RFOA15', 'RFOA16', 'RFOP15', 'RFMM10', 'RFMM11', 'RFMM12', 'RAI7', 'RAI11', 'RAI12', 'RAI14', 'RAI15', 'RAI16', 'RAI17', 'RFMI4']\n",
      "For subject D0117, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['RMOF15', 'RINSA15', 'RINSA16', 'RLOF10', 'RLOF11', 'RLOF16', 'RINSA3', 'RINSA11', 'RINSA12', 'RINSA13', 'RINSA14']\n",
      "For subject D0121, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['LFOA15', 'LFOP12', 'LFO5', 'LFO6', 'LFO8', 'LFO9', 'LFO12', 'LFO13', 'LFO14', 'LFO15', 'LFO16', 'LFO17', 'LFO18', 'LFA1', 'LFA2', 'LFP16', 'LFP17', 'LFPS1', 'LFPS2', 'LFPP1', 'LFOP13', 'LFOP15', 'LFOP16', 'LFOP18', 'LFA7', 'LFA8', 'LFAS6', 'LFAM8', 'LFAM9', 'LFA9', 'LFA10', 'LFA11', 'LFA12', 'LFAM10', 'LFAM11', 'LFAM12', 'LFMM8', 'LFMM9', 'LFMM10', 'LFMM11', 'LFMM12', 'LFAS7', 'LFAS8', 'LFAS9', 'LFAS10', 'LFMS7', 'LFMS9', 'LFMS10', 'LFMS11', 'LFMS12', 'LFPS6', 'LFPS7', 'LFPS9', 'LFPS10', 'LFPP7', 'LFPP9', 'LFPP10', 'LFMI4', 'LFMI5', 'LFMI8', 'LFMI9']\n",
      "Subject D0057 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: ['RAI6', 'RAMF14']\n",
      "Subject D0059 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0063 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0065 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0069 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0071 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0077 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0090 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0094 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0100 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0102 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0103 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0107A significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0110 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0116 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0117 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0121 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "For subject D0057, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['RPIT1']\n",
      "For subject D0059, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['LPT13']\n",
      "For subject D0063, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['RMMT1', 'RMMT2']\n",
      "For subject D0065, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['RPMT1', 'RPIT1', 'RPIT2']\n",
      "For subject D0069, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: []\n",
      "For subject D0071, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['RTPI1', 'RO1', 'RO2', 'RO10']\n",
      "For subject D0077, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['RTPI2', 'ROPI5', 'ROPI6', 'ROPI8', 'ROPM9', 'ROPM11', 'ROPM12', 'ROPM1', 'ROPM8']\n",
      "For subject D0090, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['RTPO1', 'RTPI1', 'RTPI2']\n",
      "For subject D0094, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: []\n",
      "For subject D0100, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['LTOJ1', 'LTPI1', 'LOAI1', 'LOPI1', 'LOPI4', 'LOPM1', 'LTOJ3', 'LTOJ4', 'LOMM3', 'LOMM4', 'LOMM5', 'LOPM2', 'LTOJ13', 'LOAI12', 'LOAI13', 'LOAI14', 'LOAI15', 'LOMI8', 'LOMI9', 'LOMI11', 'LOMI12', 'LOPI9', 'LOPI10', 'LOAM15', 'LOMM15', 'LOPM10', 'LOMS13', 'LOMS14', 'LOMS15', 'LOPS9', 'LOPS11', 'LOPS12', 'LPPI14', 'LPPI15', 'LOMM13', 'LOMM14', 'LOMS11', 'LOMS12', 'LOPM8', 'LOPM9', 'LPPI7', 'LPPI8', 'LPPI9']\n",
      "For subject D0102, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['RTAI6', 'RTPI1']\n",
      "For subject D0103, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['LTLI3', 'LTPI2', 'LTPI3', 'LTPI4']\n",
      "For subject D0107A, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['RTPI3']\n",
      "For subject D0110, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: []\n",
      "For subject D0116, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: []\n",
      "For subject D0117, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: []\n",
      "For subject D0121, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: []\n",
      "Subject D0057 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: ['RPIT1']\n",
      "Subject D0059 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0063 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0065 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0069 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0071 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0077 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0090 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0094 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0100 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0102 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0103 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0107A significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0110 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0116 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0117 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0121 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n"
     ]
    }
   ],
   "source": [
    "# rois_dict = {\n",
    "#     'dlpfc': [\"G_front_middle\", \"G_front_sup\", \"S_front_inf\", \"S_front_middle\", \"S_front_sup\"],\n",
    "#     'acc': [\"G_and_S_cingul-Ant\", \"G_and_S_cingul-Mid-Ant\"],\n",
    "#     'parietal': [\"G_parietal_sup\", \"S_intrapariet_and_P_trans\", \"G_pariet_inf-Angular\", \"G_pariet_inf-Supramar\"],\n",
    "#     'lpfc': [\"G_front_inf-Opercular\", \"G_front_inf-Orbital\", \"G_front_inf-Triangul\", \"G_front_middle\", \"G_front_sup\", \"Lat_Fis-ant-Horizont\", \"Lat_Fis-ant-Vertical\", \"S_circular_insula_ant\", \"S_circular_insula_sup\", \"S_front_inf\", \"S_front_middle\", \"S_front_sup\"],\n",
    "#     'v1': [\"G_oc-temp_med-Lingual\", \"S_calcarine\", \"G_cuneus\"],\n",
    "#     'occ': [\"G_cuneus\", \"G_and_S_occipital_inf\", \"G_occipital_middle\", \"G_occipital_sup\", \"G_oc-temp_lat-fusifor\", \"G_oc-temp_med-Lingual\", \"Pole_occipital\", \"S_calcarine\", \"S_oc_middle_and_Lunatus\", \"S_oc_sup_and_transversal\", \"S_occipital_ant\"]\n",
    "# }\n",
    "\n",
    "# rois_dict = {\n",
    "#     'lpfc': [\"G_front_inf-Opercular\", \"G_front_inf-Orbital\", \"G_front_inf-Triangul\", \"G_front_middle\", \"G_front_sup\", \"Lat_Fis-ant-Horizont\", \"Lat_Fis-ant-Vertical\", \"S_circular_insula_ant\", \"S_circular_insula_sup\", \"S_front_inf\", \"S_front_middle\", \"S_front_sup\"]\n",
    "# }\n",
    "\n",
    "rois_dict = {\n",
    "    'lpfc': [\"G_front_inf-Opercular\", \"G_front_inf-Orbital\", \"G_front_inf-Triangul\", \"G_front_middle\", \"G_front_sup\", \"Lat_Fis-ant-Horizont\", \"Lat_Fis-ant-Vertical\", \"S_circular_insula_ant\", \"S_circular_insula_sup\", \"S_front_inf\", \"S_front_middle\", \"S_front_sup\"],\n",
    "    'occ': [\"G_cuneus\", \"G_and_S_occipital_inf\", \"G_occipital_middle\", \"G_occipital_sup\", \"G_oc-temp_lat-fusifor\", \"G_oc-temp_med-Lingual\", \"Pole_occipital\", \"S_calcarine\", \"S_oc_middle_and_Lunatus\", \"S_oc_sup_and_transversal\", \"S_occipital_ant\"]\n",
    "}\n",
    "\n",
    "rois = list(rois_dict.keys())\n",
    "all_electrodes_per_subject_roi, sig_electrodes_per_subject_roi = make_sig_electrodes_per_subject_and_roi_dict(rois_dict, subjects_electrodestoROIs_dict, sig_chans_per_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "280f6688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sig elecs: 74\n"
     ]
    }
   ],
   "source": [
    "print('total sig elecs:', sum(len(sig_chans_per_subject[sub]) for sub in sig_chans_per_subject))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e40e3c2",
   "metadata": {},
   "source": [
    "get number of sig and all electrodes per subject and across subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf3fd540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject D0057, ROI lpfc, Num of Sig Electrodes: 2, Num of All Electrodes: 17\n",
      "Subject D0057, ROI occ, Num of Sig Electrodes: 1, Num of All Electrodes: 1\n"
     ]
    }
   ],
   "source": [
    "for roi in rois:\n",
    "    for sub in subjects:\n",
    "        sig_elecs = sig_electrodes_per_subject_roi.get(roi, {}).get(sub, [])\n",
    "        all_elecs = all_electrodes_per_subject_roi.get(roi, {}).get(sub, [])\n",
    "        print(f\"Subject {sub}, ROI {roi}, Num of Sig Electrodes: {len(sig_elecs)}, Num of All Electrodes: {len(all_elecs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb5ec907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of significant lpfc electrodes across all subjects: 2\n",
      "Total number of lpfc electrodes across all subjects: 265\n",
      "Total number of significant occ electrodes across all subjects: 1\n",
      "Total number of occ electrodes across all subjects: 73\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "total_electrodes_info = calculate_total_electrodes(sig_electrodes_per_subject_roi, all_electrodes_per_subject_roi)\n",
    "for roi, counts in total_electrodes_info.items():\n",
    "    print(f\"Total number of significant {roi} electrodes across all subjects:\", counts['total_significant_electrodes'])\n",
    "    print(f\"Total number of {roi} electrodes across all subjects:\", counts['total_electrodes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459a911b",
   "metadata": {},
   "source": [
    "#### 1. For each electrode, make multitaper using all training trials, for both conditions to be compared (this can only do two conditions, can't do more rn)\n",
    "\n",
    "TODO: Need to loop over the trials and use some as training set, and have some held out as a test set that isn't used to find clusters. Maybe do this in the stats step though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e60e7a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing TFR data. Loading from: C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\spec\\multitaper\\subjects_tfr_objects\\stimulus_switch_type_conditions_Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False_1_subjects_multitaper\n"
     ]
    }
   ],
   "source": [
    "subjects_tfr_objects = load_or_make_subjects_tfr_objects(\n",
    "    layout=layout,\n",
    "    spec_method=spec_method,\n",
    "    conditions_save_name=conditions_save_name,\n",
    "    subjects=subjects,\n",
    "    conditions=conditions,\n",
    "    signal_times=signal_times,\n",
    "    freqs=freqs,\n",
    "    n_cycles=n_cycles,\n",
    "    time_bandwidth=time_bandwidth,\n",
    "    return_itc=return_itc,\n",
    "    n_jobs=n_jobs,\n",
    "    average=average,\n",
    "    acc_trials_only=acc_trials_only,\n",
    "    error_trials_only=error_trials_only \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f756323",
   "metadata": {},
   "source": [
    "#### 2. Find clusters that are significantly different between the two conditions, in the multitaper spectrogram  \n",
    "do this for all channels per subject, and then also for all channels in an roi across subjects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5798bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sig_tfr_differences_per_subject(\n",
    "    subjects_tfr_objects: dict,\n",
    "    condition_names: list[str],\n",
    "    stat_func: callable,\n",
    "    p_thresh: float = 0.05,\n",
    "    p_cluster: float = None,\n",
    "    n_perm: int = 1000,\n",
    "    tails: int = 1,\n",
    "    axis: int = 0,\n",
    "    ignore_adjacency: int | tuple[int, ...] = 1,\n",
    "    n_jobs: int = 1,\n",
    "    seed: int = None):\n",
    "    \"\"\"\n",
    "    Performs TFR statistical analysis for each subject individually.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    subjects_tfr_objects : dict\n",
    "        A dictionary structured as {subject_id: {condition_name: tfr_object}}.\n",
    "    condition_names : list[str]\n",
    "        A list of two condition names to be compared.\n",
    "    stat_func: callable, optional\n",
    "        The statistical function to use for significance testing. You should probably use partial(ttest_ind, equal_var=False).\n",
    "    p_thresh : float\n",
    "        The p-value threshold to use for determining significant time points.\n",
    "    p_cluster : float, optional\n",
    "        The p-value threshold to use for determining significant clusters.\n",
    "    n_perm : int, optional\n",
    "        The number of permutations to perform.\n",
    "    tails : int, optional\n",
    "        The number of tails to use. 1 for one-tailed, 2 for two-tailed.\n",
    "    axis : int, optional\n",
    "        The axis to perform the permutation test across. Also known as the\n",
    "        observations axis\n",
    "    ignore_adjacency : int or tuple of ints, optional\n",
    "        The axis or axes to ignore when finding clusters. For example, if\n",
    "        sig1.shape = (trials, channels, time), and you want to find clusters\n",
    "        across time, but not channels, you would set ignore_adjacency = 1.\n",
    "    n_jobs : int, optional\n",
    "        The number of jobs to run in parallel. -1 for all processors. Default\n",
    "        is -1.\n",
    "    seed : int, optional\n",
    "        The random seed to use for the permutation test. Default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sub_masks : dict\n",
    "        Dictionary mapping subject IDs to binary mask arrays indicating \n",
    "        significant differences between conditions.\n",
    "    sub_pvals : dict\n",
    "        Dictionary mapping subject IDs to p-values for each cluster found\n",
    "        in the statistical comparison.\n",
    "    \"\"\"\n",
    "    if len(condition_names) != 2:\n",
    "        raise ValueError(\"This function requires exactly two conditions for comparison.\")\n",
    "\n",
    "    sub_masks = {}\n",
    "    sub_pvals = {}\n",
    "    cond1, cond2 = condition_names[0], condition_names[1]\n",
    "\n",
    "    for sub, tfrs in subjects_tfr_objects.items():\n",
    "        print(f\"Processing statistics for subject: {sub}\")\n",
    "        \n",
    "        mask, pvals = get_sig_tfr_differences(\n",
    "            tfr_data_cond1=tfrs[cond1],\n",
    "            tfr_data_cond2=tfrs[cond2],\n",
    "            stat_func=stat_func,\n",
    "            p_thresh=p_thresh,\n",
    "            p_cluster=p_cluster,\n",
    "            n_perm=n_perm,\n",
    "            tails=tails,\n",
    "            axis=axis,\n",
    "            ignore_adjacency=ignore_adjacency,\n",
    "            n_jobs=n_jobs,\n",
    "            seed=seed\n",
    "        )\n",
    "        \n",
    "        sub_masks[sub] = mask\n",
    "        sub_pvals[sub] = pvals\n",
    "\n",
    "    return sub_masks, sub_pvals\n",
    "\n",
    "def get_sig_tfr_differences_per_roi(\n",
    "    subjects_tfr_objects: dict,\n",
    "    electrodes_per_subject_roi: dict,\n",
    "    condition_names: list[str],\n",
    "    stat_func: callable,\n",
    "    p_thresh: float = 0.05,\n",
    "    p_cluster: float = None,\n",
    "    n_perm: int = 1000,\n",
    "    tails: int = 1,\n",
    "    axis: int = 0,\n",
    "    ignore_adjacency: int | tuple[int, ...] = 1,\n",
    "    n_jobs: int = 1,\n",
    "    seed: int = None):\n",
    "    \"\"\"\n",
    "    Performs TFR statistical analysis for each ROI by combining subjects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    subjects_tfr_objects : dict\n",
    "        Dictionary of TFR data: {subject_id: {condition_name: tfr_object}}.\n",
    "    electrodes_per_roi : dict\n",
    "        Dictionary mapping ROIs to rois and electrodes: {roi_name: {subject_id: [elecs]}}.\n",
    "    condition_names : list[str]\n",
    "        A list of two condition names to compare.\n",
    "    stat_func: callable, optional\n",
    "        The statistical function to use for significance testing. You should probably use partial(ttest_ind, equal_var=False).\n",
    "    p_thresh : float\n",
    "        The p-value threshold to use for determining significant time points.\n",
    "    p_cluster : float, optional\n",
    "        The p-value threshold to use for determining significant clusters.\n",
    "    n_perm : int, optional\n",
    "        The number of permutations to perform.\n",
    "    tails : int, optional\n",
    "        The number of tails to use. 1 for one-tailed, 2 for two-tailed.\n",
    "    axis : int, optional\n",
    "        The axis to perform the permutation test across. Also known as the\n",
    "        observations axis\n",
    "    ignore_adjacency : int or tuple of ints, optional\n",
    "        The axis or axes to ignore when finding clusters. For example, if\n",
    "        sig1.shape = (trials, channels, time), and you want to find clusters\n",
    "        across time, but not channels, you would set ignore_adjacency = 1.\n",
    "    n_jobs : int, optional\n",
    "        The number of jobs to run in parallel. -1 for all processors. Default\n",
    "        is -1.\n",
    "    seed : int, optional\n",
    "        The random seed to use for the permutation test. Default is None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    roi_masks : dict\n",
    "        Dictionary mapping ROI names to concatenated binary mask arrays.\n",
    "        For each ROI, masks from all subjects with electrodes in that ROI\n",
    "        are concatenated along axis 0 (subjects dimension), resulting in\n",
    "        a combined mask array with shape (n_subjects_in_roi, *original_dims).\n",
    "    roi_pvals : dict\n",
    "        Dictionary mapping ROI names to concatenated p-value arrays.\n",
    "        For each ROI, p-values from all subjects with electrodes in that ROI\n",
    "        are concatenated along axis 0, matching the structure of roi_masks.\n",
    "        Empty arrays are returned for ROIs with no subject data.\n",
    "    \"\"\"\n",
    "    if len(condition_names) != 2:\n",
    "        raise ValueError(\"This function requires exactly two conditions for comparison.\")\n",
    "\n",
    "    roi_masks = {}\n",
    "    roi_pvals = {}\n",
    "    cond1, cond2 = condition_names[0], condition_names[1]\n",
    "\n",
    "    for roi, subjects_in_roi in electrodes_per_subject_roi.items():\n",
    "        print(f\"Processing statistics for ROI: {roi}\")\n",
    "        \n",
    "        subject_masks_for_roi = []\n",
    "        subject_pvals_for_roi = []\n",
    "\n",
    "        for sub, tfrs in subjects_tfr_objects.items():\n",
    "            elecs = subjects_in_roi.get(sub, [])\n",
    "            if not elecs:\n",
    "                continue\n",
    "\n",
    "            mask, pvals = get_sig_tfr_differences(\n",
    "                tfr_data_cond1=tfrs[cond1],\n",
    "                tfr_data_cond2=tfrs[cond2],\n",
    "                stat_func=stat_func,\n",
    "                elecs_to_pick=elecs,\n",
    "                p_thresh=p_thresh,\n",
    "                p_cluster=p_cluster,\n",
    "                n_perm=n_perm,\n",
    "                tails=tails,\n",
    "                axis=axis,\n",
    "                ignore_adjacency=ignore_adjacency,\n",
    "                n_jobs=n_jobs,\n",
    "                seed=seed\n",
    "            )\n",
    "\n",
    "            subject_masks_for_roi.append(mask)\n",
    "            subject_pvals_for_roi.append(pvals)\n",
    "\n",
    "        if subject_masks_for_roi:\n",
    "            roi_masks[roi] = np.concatenate(subject_masks_for_roi, axis=0)\n",
    "            roi_pvals[roi] = np.concatenate(subject_pvals_for_roi, axis=0)\n",
    "        else:\n",
    "            roi_masks[roi] = np.array([])\n",
    "            roi_pvals[roi] = np.array([])\n",
    "\n",
    "    return roi_masks, roi_pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da78fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing statistics for subject: D0057\n",
      "stat_func returns a tuple. Taking the first element\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jz421\\AppData\\Local\\anaconda3\\envs\\ieeg\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:1081: RuntimeWarning: divide by zero encountered in divide\n",
      "  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n",
      "c:\\Users\\jz421\\AppData\\Local\\anaconda3\\envs\\ieeg\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:1081: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=1)]: Done   2 tasks      | elapsed:   37.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 tasks      | elapsed:   55.4s\n",
      "[Parallel(n_jobs=1)]: Done   4 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=1)]: Done   5 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=1)]: Done   6 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=1)]: Done   7 tasks      | elapsed: 14.6min\n",
      "[Parallel(n_jobs=1)]: Done   8 tasks      | elapsed: 14.9min\n",
      "[Parallel(n_jobs=1)]: Done   9 tasks      | elapsed: 15.2min\n",
      "[Parallel(n_jobs=1)]: Done  10 tasks      | elapsed: 15.5min\n",
      "[Parallel(n_jobs=1)]: Done  11 tasks      | elapsed: 28.2min\n",
      "[Parallel(n_jobs=1)]: Done  12 tasks      | elapsed: 28.5min\n",
      "[Parallel(n_jobs=1)]: Done  13 tasks      | elapsed: 28.8min\n",
      "[Parallel(n_jobs=1)]: Done  14 tasks      | elapsed: 29.1min\n",
      "[Parallel(n_jobs=1)]: Done  15 tasks      | elapsed: 42.5min\n",
      "[Parallel(n_jobs=1)]: Done  16 tasks      | elapsed: 54.8min\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed: 55.1min\n",
      "[Parallel(n_jobs=1)]: Done  18 tasks      | elapsed: 55.5min\n",
      "[Parallel(n_jobs=1)]: Done  19 tasks      | elapsed: 55.8min\n",
      "[Parallel(n_jobs=1)]: Done  20 tasks      | elapsed: 56.1min\n",
      "[Parallel(n_jobs=1)]: Done  21 tasks      | elapsed: 56.4min\n",
      "[Parallel(n_jobs=1)]: Done  22 tasks      | elapsed: 56.7min\n",
      "[Parallel(n_jobs=1)]: Done  23 tasks      | elapsed: 57.1min\n",
      "[Parallel(n_jobs=1)]: Done  24 tasks      | elapsed: 57.4min\n",
      "[Parallel(n_jobs=1)]: Done  25 tasks      | elapsed: 69.5min\n",
      "[Parallel(n_jobs=1)]: Done  26 tasks      | elapsed: 69.8min\n",
      "[Parallel(n_jobs=1)]: Done  27 tasks      | elapsed: 70.2min\n",
      "[Parallel(n_jobs=1)]: Done  28 tasks      | elapsed: 82.4min\n",
      "[Parallel(n_jobs=1)]: Done  29 tasks      | elapsed: 93.8min\n",
      "[Parallel(n_jobs=1)]: Done  30 tasks      | elapsed: 104.5min\n",
      "[Parallel(n_jobs=1)]: Done  31 tasks      | elapsed: 115.1min\n",
      "[Parallel(n_jobs=1)]: Done  32 tasks      | elapsed: 115.4min\n",
      "[Parallel(n_jobs=1)]: Done  33 tasks      | elapsed: 115.7min\n",
      "[Parallel(n_jobs=1)]: Done  34 tasks      | elapsed: 116.0min\n",
      "[Parallel(n_jobs=1)]: Done  35 tasks      | elapsed: 116.3min\n",
      "[Parallel(n_jobs=1)]: Done  36 tasks      | elapsed: 116.6min\n",
      "[Parallel(n_jobs=1)]: Done  37 tasks      | elapsed: 116.9min\n",
      "[Parallel(n_jobs=1)]: Done  38 tasks      | elapsed: 117.2min\n",
      "[Parallel(n_jobs=1)]: Done  39 tasks      | elapsed: 117.5min\n",
      "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed: 117.8min\n",
      "[Parallel(n_jobs=1)]: Done  41 tasks      | elapsed: 128.5min\n",
      "[Parallel(n_jobs=1)]: Done  42 tasks      | elapsed: 128.8min\n",
      "[Parallel(n_jobs=1)]: Done  43 tasks      | elapsed: 129.1min\n",
      "[Parallel(n_jobs=1)]: Done  44 tasks      | elapsed: 129.4min\n",
      "[Parallel(n_jobs=1)]: Done  45 tasks      | elapsed: 129.7min\n",
      "[Parallel(n_jobs=1)]: Done  46 tasks      | elapsed: 130.0min\n",
      "[Parallel(n_jobs=1)]: Done  47 tasks      | elapsed: 130.3min\n",
      "[Parallel(n_jobs=1)]: Done  48 tasks      | elapsed: 130.6min\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed: 130.9min\n",
      "[Parallel(n_jobs=1)]: Done  50 tasks      | elapsed: 131.2min\n",
      "[Parallel(n_jobs=1)]: Done  51 tasks      | elapsed: 131.5min\n",
      "[Parallel(n_jobs=1)]: Done  52 tasks      | elapsed: 131.8min\n",
      "[Parallel(n_jobs=1)]: Done  53 tasks      | elapsed: 132.1min\n",
      "[Parallel(n_jobs=1)]: Done  54 tasks      | elapsed: 142.8min\n",
      "[Parallel(n_jobs=1)]: Done  55 tasks      | elapsed: 143.1min\n",
      "[Parallel(n_jobs=1)]: Done  56 tasks      | elapsed: 143.5min\n",
      "[Parallel(n_jobs=1)]: Done  57 tasks      | elapsed: 143.8min\n",
      "[Parallel(n_jobs=1)]: Done  58 tasks      | elapsed: 144.1min\n",
      "[Parallel(n_jobs=1)]: Done  59 tasks      | elapsed: 144.4min\n",
      "[Parallel(n_jobs=1)]: Done  60 tasks      | elapsed: 144.7min\n",
      "[Parallel(n_jobs=1)]: Done  61 tasks      | elapsed: 145.1min\n",
      "[Parallel(n_jobs=1)]: Done  62 tasks      | elapsed: 145.4min\n",
      "[Parallel(n_jobs=1)]: Done  63 tasks      | elapsed: 145.7min\n",
      "[Parallel(n_jobs=1)]: Done  64 tasks      | elapsed: 146.0min\n",
      "[Parallel(n_jobs=1)]: Done  65 tasks      | elapsed: 146.4min\n",
      "[Parallel(n_jobs=1)]: Done  66 tasks      | elapsed: 146.7min\n",
      "[Parallel(n_jobs=1)]: Done  67 tasks      | elapsed: 147.0min\n",
      "[Parallel(n_jobs=1)]: Done  68 tasks      | elapsed: 147.3min\n",
      "[Parallel(n_jobs=1)]: Done  69 tasks      | elapsed: 147.6min\n",
      "[Parallel(n_jobs=1)]: Done  70 tasks      | elapsed: 147.9min\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed: 148.3min\n",
      "[Parallel(n_jobs=1)]: Done  72 tasks      | elapsed: 148.6min\n",
      "[Parallel(n_jobs=1)]: Done  73 tasks      | elapsed: 159.3min\n",
      "[Parallel(n_jobs=1)]: Done  74 tasks      | elapsed: 159.6min\n",
      "[Parallel(n_jobs=1)]: Done  75 tasks      | elapsed: 160.0min\n",
      "[Parallel(n_jobs=1)]: Done  76 tasks      | elapsed: 160.3min\n",
      "[Parallel(n_jobs=1)]: Done  77 tasks      | elapsed: 160.6min\n",
      "[Parallel(n_jobs=1)]: Done  78 tasks      | elapsed: 160.9min\n",
      "[Parallel(n_jobs=1)]: Done  79 tasks      | elapsed: 161.2min\n",
      "[Parallel(n_jobs=1)]: Done  80 tasks      | elapsed: 161.5min\n",
      "[Parallel(n_jobs=1)]: Done  81 tasks      | elapsed: 161.9min\n",
      "[Parallel(n_jobs=1)]: Done  82 tasks      | elapsed: 162.2min\n",
      "[Parallel(n_jobs=1)]: Done  83 tasks      | elapsed: 162.5min\n",
      "[Parallel(n_jobs=1)]: Done  84 tasks      | elapsed: 162.8min\n",
      "[Parallel(n_jobs=1)]: Done  85 tasks      | elapsed: 173.5min\n",
      "[Parallel(n_jobs=1)]: Done  86 tasks      | elapsed: 173.8min\n",
      "[Parallel(n_jobs=1)]: Done  87 tasks      | elapsed: 174.1min\n",
      "[Parallel(n_jobs=1)]: Done  88 tasks      | elapsed: 174.4min\n",
      "[Parallel(n_jobs=1)]: Done  89 tasks      | elapsed: 185.6min\n",
      "[Parallel(n_jobs=1)]: Done  90 tasks      | elapsed: 185.9min\n",
      "[Parallel(n_jobs=1)]: Done  91 tasks      | elapsed: 186.3min\n",
      "[Parallel(n_jobs=1)]: Done  92 tasks      | elapsed: 186.6min\n",
      "[Parallel(n_jobs=1)]: Done  93 tasks      | elapsed: 186.9min\n",
      "[Parallel(n_jobs=1)]: Done  94 tasks      | elapsed: 198.8min\n",
      "[Parallel(n_jobs=1)]: Done  95 tasks      | elapsed: 209.8min\n",
      "[Parallel(n_jobs=1)]: Done  96 tasks      | elapsed: 210.1min\n",
      "[Parallel(n_jobs=1)]: Done  97 tasks      | elapsed: 220.7min\n",
      "[Parallel(n_jobs=1)]: Done  98 tasks      | elapsed: 221.0min\n",
      "[Parallel(n_jobs=1)]: Done  99 tasks      | elapsed: 221.3min\n",
      "[Parallel(n_jobs=1)]: Done 100 tasks      | elapsed: 221.6min\n",
      "[Parallel(n_jobs=1)]: Done 101 tasks      | elapsed: 221.9min\n",
      "[Parallel(n_jobs=1)]: Done 102 tasks      | elapsed: 222.2min\n",
      "[Parallel(n_jobs=1)]: Done 103 tasks      | elapsed: 222.6min\n",
      "[Parallel(n_jobs=1)]: Done 104 tasks      | elapsed: 222.9min\n",
      "[Parallel(n_jobs=1)]: Done 105 tasks      | elapsed: 223.2min\n",
      "[Parallel(n_jobs=1)]: Done 106 tasks      | elapsed: 223.5min\n",
      "[Parallel(n_jobs=1)]: Done 107 tasks      | elapsed: 223.8min\n",
      "[Parallel(n_jobs=1)]: Done 108 tasks      | elapsed: 224.1min\n",
      "[Parallel(n_jobs=1)]: Done 109 tasks      | elapsed: 224.4min\n",
      "[Parallel(n_jobs=1)]: Done 110 tasks      | elapsed: 224.7min\n",
      "[Parallel(n_jobs=1)]: Done 111 tasks      | elapsed: 225.1min\n",
      "[Parallel(n_jobs=1)]: Done 112 tasks      | elapsed: 225.4min\n",
      "[Parallel(n_jobs=1)]: Done 113 tasks      | elapsed: 225.7min\n",
      "[Parallel(n_jobs=1)]: Done 114 tasks      | elapsed: 226.0min\n",
      "[Parallel(n_jobs=1)]: Done 115 tasks      | elapsed: 226.3min\n",
      "[Parallel(n_jobs=1)]: Done 116 tasks      | elapsed: 226.7min\n",
      "[Parallel(n_jobs=1)]: Done 117 tasks      | elapsed: 227.0min\n",
      "[Parallel(n_jobs=1)]: Done 118 tasks      | elapsed: 227.3min\n",
      "[Parallel(n_jobs=1)]: Done 119 tasks      | elapsed: 227.6min\n",
      "[Parallel(n_jobs=1)]: Done 120 tasks      | elapsed: 238.5min\n",
      "[Parallel(n_jobs=1)]: Done 121 tasks      | elapsed: 238.8min\n",
      "[Parallel(n_jobs=1)]: Done 122 tasks      | elapsed: 239.2min\n",
      "[Parallel(n_jobs=1)]: Done 123 tasks      | elapsed: 239.5min\n",
      "[Parallel(n_jobs=1)]: Done 124 tasks      | elapsed: 239.8min\n",
      "[Parallel(n_jobs=1)]: Done 125 tasks      | elapsed: 240.1min\n",
      "[Parallel(n_jobs=1)]: Done 126 tasks      | elapsed: 240.4min\n",
      "[Parallel(n_jobs=1)]: Done 127 tasks      | elapsed: 240.7min\n",
      "[Parallel(n_jobs=1)]: Done 128 tasks      | elapsed: 241.0min\n",
      "[Parallel(n_jobs=1)]: Done 129 tasks      | elapsed: 251.9min\n",
      "[Parallel(n_jobs=1)]: Done 130 tasks      | elapsed: 252.2min\n",
      "[Parallel(n_jobs=1)]: Done 131 tasks      | elapsed: 252.6min\n",
      "[Parallel(n_jobs=1)]: Done 132 tasks      | elapsed: 252.9min\n",
      "[Parallel(n_jobs=1)]: Done 133 tasks      | elapsed: 253.2min\n",
      "[Parallel(n_jobs=1)]: Done 134 tasks      | elapsed: 253.5min\n",
      "[Parallel(n_jobs=1)]: Done 135 tasks      | elapsed: 253.8min\n",
      "[Parallel(n_jobs=1)]: Done 136 tasks      | elapsed: 254.2min\n",
      "[Parallel(n_jobs=1)]: Done 137 tasks      | elapsed: 254.5min\n",
      "[Parallel(n_jobs=1)]: Done 138 tasks      | elapsed: 254.8min\n",
      "[Parallel(n_jobs=1)]: Done 139 tasks      | elapsed: 255.1min\n",
      "[Parallel(n_jobs=1)]: Done 140 tasks      | elapsed: 255.5min\n",
      "[Parallel(n_jobs=1)]: Done 141 tasks      | elapsed: 255.8min\n",
      "[Parallel(n_jobs=1)]: Done 142 tasks      | elapsed: 256.2min\n",
      "[Parallel(n_jobs=1)]: Done 143 tasks      | elapsed: 256.5min\n",
      "[Parallel(n_jobs=1)]: Done 144 tasks      | elapsed: 256.8min\n",
      "[Parallel(n_jobs=1)]: Done 145 tasks      | elapsed: 268.0min\n",
      "[Parallel(n_jobs=1)]: Done 146 tasks      | elapsed: 268.3min\n",
      "[Parallel(n_jobs=1)]: Done 147 tasks      | elapsed: 268.6min\n",
      "[Parallel(n_jobs=1)]: Done 148 tasks      | elapsed: 268.9min\n",
      "[Parallel(n_jobs=1)]: Done 149 tasks      | elapsed: 269.2min\n",
      "[Parallel(n_jobs=1)]: Done 150 tasks      | elapsed: 280.4min\n",
      "[Parallel(n_jobs=1)]: Done 151 tasks      | elapsed: 280.7min\n",
      "[Parallel(n_jobs=1)]: Done 152 tasks      | elapsed: 291.6min\n",
      "[Parallel(n_jobs=1)]: Done 153 tasks      | elapsed: 302.2min\n",
      "[Parallel(n_jobs=1)]: Done 154 tasks      | elapsed: 302.5min\n",
      "[Parallel(n_jobs=1)]: Done 155 tasks      | elapsed: 302.9min\n",
      "[Parallel(n_jobs=1)]: Done 156 tasks      | elapsed: 303.2min\n",
      "[Parallel(n_jobs=1)]: Done 157 tasks      | elapsed: 303.5min\n",
      "[Parallel(n_jobs=1)]: Done 158 tasks      | elapsed: 303.8min\n",
      "[Parallel(n_jobs=1)]: Done 159 tasks      | elapsed: 304.2min\n",
      "[Parallel(n_jobs=1)]: Done 160 tasks      | elapsed: 304.5min\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed: 304.8min\n",
      "[Parallel(n_jobs=1)]: Done 162 tasks      | elapsed: 305.1min\n",
      "[Parallel(n_jobs=1)]: Done 163 tasks      | elapsed: 305.5min\n",
      "[Parallel(n_jobs=1)]: Done 164 tasks      | elapsed: 305.8min\n",
      "[Parallel(n_jobs=1)]: Done 165 tasks      | elapsed: 306.1min\n",
      "[Parallel(n_jobs=1)]: Done 166 tasks      | elapsed: 306.4min\n",
      "[Parallel(n_jobs=1)]: Done 167 tasks      | elapsed: 306.8min\n",
      "[Parallel(n_jobs=1)]: Done 168 tasks      | elapsed: 307.1min\n",
      "[Parallel(n_jobs=1)]: Done 169 tasks      | elapsed: 307.4min\n",
      "[Parallel(n_jobs=1)]: Done 170 tasks      | elapsed: 307.7min\n",
      "[Parallel(n_jobs=1)]: Done 171 tasks      | elapsed: 308.0min\n",
      "[Parallel(n_jobs=1)]: Done 172 tasks      | elapsed: 319.0min\n",
      "[Parallel(n_jobs=1)]: Done 173 tasks      | elapsed: 319.3min\n",
      "[Parallel(n_jobs=1)]: Done 174 tasks      | elapsed: 330.2min\n",
      "[Parallel(n_jobs=1)]: Done 175 tasks      | elapsed: 341.0min\n",
      "[Parallel(n_jobs=1)]: Done 175 tasks      | elapsed: 341.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing statistics for subject: D0057\n",
      "stat_func returns a tuple. Taking the first element\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jz421\\AppData\\Local\\anaconda3\\envs\\ieeg\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:1081: RuntimeWarning: divide by zero encountered in divide\n",
      "  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n",
      "c:\\Users\\jz421\\AppData\\Local\\anaconda3\\envs\\ieeg\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:1081: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:   17.4s\n",
      "[Parallel(n_jobs=1)]: Done   2 tasks      | elapsed:   33.5s\n",
      "[Parallel(n_jobs=1)]: Done   3 tasks      | elapsed:   49.5s\n",
      "[Parallel(n_jobs=1)]: Done   4 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=1)]: Done   5 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=1)]: Done   6 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=1)]: Done   7 tasks      | elapsed: 12.3min\n",
      "[Parallel(n_jobs=1)]: Done   8 tasks      | elapsed: 12.5min\n",
      "[Parallel(n_jobs=1)]: Done   9 tasks      | elapsed: 12.8min\n",
      "[Parallel(n_jobs=1)]: Done  10 tasks      | elapsed: 13.1min\n",
      "[Parallel(n_jobs=1)]: Done  11 tasks      | elapsed: 23.8min\n",
      "[Parallel(n_jobs=1)]: Done  12 tasks      | elapsed: 24.0min\n",
      "[Parallel(n_jobs=1)]: Done  13 tasks      | elapsed: 24.3min\n",
      "[Parallel(n_jobs=1)]: Done  14 tasks      | elapsed: 24.6min\n",
      "[Parallel(n_jobs=1)]: Done  15 tasks      | elapsed: 35.4min\n",
      "[Parallel(n_jobs=1)]: Done  16 tasks      | elapsed: 46.3min\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed: 46.6min\n",
      "[Parallel(n_jobs=1)]: Done  18 tasks      | elapsed: 46.9min\n",
      "[Parallel(n_jobs=1)]: Done  19 tasks      | elapsed: 47.1min\n",
      "[Parallel(n_jobs=1)]: Done  20 tasks      | elapsed: 47.4min\n",
      "[Parallel(n_jobs=1)]: Done  21 tasks      | elapsed: 47.7min\n",
      "[Parallel(n_jobs=1)]: Done  22 tasks      | elapsed: 47.9min\n",
      "[Parallel(n_jobs=1)]: Done  23 tasks      | elapsed: 48.2min\n",
      "[Parallel(n_jobs=1)]: Done  24 tasks      | elapsed: 48.5min\n",
      "[Parallel(n_jobs=1)]: Done  25 tasks      | elapsed: 59.3min\n",
      "[Parallel(n_jobs=1)]: Done  26 tasks      | elapsed: 59.6min\n",
      "[Parallel(n_jobs=1)]: Done  27 tasks      | elapsed: 59.9min\n",
      "[Parallel(n_jobs=1)]: Done  28 tasks      | elapsed: 70.5min\n",
      "[Parallel(n_jobs=1)]: Done  29 tasks      | elapsed: 81.3min\n",
      "[Parallel(n_jobs=1)]: Done  30 tasks      | elapsed: 92.2min\n",
      "[Parallel(n_jobs=1)]: Done  31 tasks      | elapsed: 103.0min\n",
      "[Parallel(n_jobs=1)]: Done  32 tasks      | elapsed: 103.3min\n",
      "[Parallel(n_jobs=1)]: Done  33 tasks      | elapsed: 103.6min\n",
      "[Parallel(n_jobs=1)]: Done  34 tasks      | elapsed: 103.9min\n",
      "[Parallel(n_jobs=1)]: Done  35 tasks      | elapsed: 104.1min\n",
      "[Parallel(n_jobs=1)]: Done  36 tasks      | elapsed: 104.4min\n",
      "[Parallel(n_jobs=1)]: Done  37 tasks      | elapsed: 104.7min\n",
      "[Parallel(n_jobs=1)]: Done  38 tasks      | elapsed: 104.9min\n",
      "[Parallel(n_jobs=1)]: Done  39 tasks      | elapsed: 105.2min\n",
      "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed: 105.5min\n",
      "[Parallel(n_jobs=1)]: Done  41 tasks      | elapsed: 116.2min\n",
      "[Parallel(n_jobs=1)]: Done  42 tasks      | elapsed: 116.5min\n",
      "[Parallel(n_jobs=1)]: Done  43 tasks      | elapsed: 116.7min\n",
      "[Parallel(n_jobs=1)]: Done  44 tasks      | elapsed: 117.0min\n",
      "[Parallel(n_jobs=1)]: Done  45 tasks      | elapsed: 117.3min\n",
      "[Parallel(n_jobs=1)]: Done  46 tasks      | elapsed: 117.5min\n",
      "[Parallel(n_jobs=1)]: Done  47 tasks      | elapsed: 117.8min\n",
      "[Parallel(n_jobs=1)]: Done  48 tasks      | elapsed: 118.1min\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed: 118.4min\n",
      "[Parallel(n_jobs=1)]: Done  50 tasks      | elapsed: 118.6min\n",
      "[Parallel(n_jobs=1)]: Done  51 tasks      | elapsed: 118.9min\n",
      "[Parallel(n_jobs=1)]: Done  52 tasks      | elapsed: 119.2min\n",
      "[Parallel(n_jobs=1)]: Done  53 tasks      | elapsed: 119.4min\n",
      "[Parallel(n_jobs=1)]: Done  54 tasks      | elapsed: 130.1min\n",
      "[Parallel(n_jobs=1)]: Done  55 tasks      | elapsed: 130.3min\n",
      "[Parallel(n_jobs=1)]: Done  56 tasks      | elapsed: 130.6min\n",
      "[Parallel(n_jobs=1)]: Done  57 tasks      | elapsed: 130.9min\n",
      "[Parallel(n_jobs=1)]: Done  58 tasks      | elapsed: 131.2min\n",
      "[Parallel(n_jobs=1)]: Done  59 tasks      | elapsed: 131.4min\n",
      "[Parallel(n_jobs=1)]: Done  60 tasks      | elapsed: 131.7min\n",
      "[Parallel(n_jobs=1)]: Done  61 tasks      | elapsed: 132.0min\n",
      "[Parallel(n_jobs=1)]: Done  62 tasks      | elapsed: 132.2min\n",
      "[Parallel(n_jobs=1)]: Done  63 tasks      | elapsed: 132.5min\n",
      "[Parallel(n_jobs=1)]: Done  64 tasks      | elapsed: 132.8min\n",
      "[Parallel(n_jobs=1)]: Done  65 tasks      | elapsed: 133.1min\n",
      "[Parallel(n_jobs=1)]: Done  66 tasks      | elapsed: 133.3min\n",
      "[Parallel(n_jobs=1)]: Done  67 tasks      | elapsed: 133.6min\n",
      "[Parallel(n_jobs=1)]: Done  68 tasks      | elapsed: 133.9min\n",
      "[Parallel(n_jobs=1)]: Done  69 tasks      | elapsed: 134.1min\n",
      "[Parallel(n_jobs=1)]: Done  70 tasks      | elapsed: 134.4min\n",
      "[Parallel(n_jobs=1)]: Done  71 tasks      | elapsed: 134.7min\n",
      "[Parallel(n_jobs=1)]: Done  72 tasks      | elapsed: 134.9min\n",
      "[Parallel(n_jobs=1)]: Done  73 tasks      | elapsed: 145.7min\n",
      "[Parallel(n_jobs=1)]: Done  74 tasks      | elapsed: 146.0min\n",
      "[Parallel(n_jobs=1)]: Done  75 tasks      | elapsed: 146.3min\n",
      "[Parallel(n_jobs=1)]: Done  76 tasks      | elapsed: 146.6min\n",
      "[Parallel(n_jobs=1)]: Done  77 tasks      | elapsed: 146.8min\n",
      "[Parallel(n_jobs=1)]: Done  78 tasks      | elapsed: 147.1min\n",
      "[Parallel(n_jobs=1)]: Done  79 tasks      | elapsed: 147.4min\n",
      "[Parallel(n_jobs=1)]: Done  80 tasks      | elapsed: 147.6min\n",
      "[Parallel(n_jobs=1)]: Done  81 tasks      | elapsed: 147.9min\n",
      "[Parallel(n_jobs=1)]: Done  82 tasks      | elapsed: 148.2min\n",
      "[Parallel(n_jobs=1)]: Done  83 tasks      | elapsed: 148.5min\n",
      "[Parallel(n_jobs=1)]: Done  84 tasks      | elapsed: 148.7min\n",
      "[Parallel(n_jobs=1)]: Done  85 tasks      | elapsed: 159.3min\n",
      "[Parallel(n_jobs=1)]: Done  86 tasks      | elapsed: 159.6min\n",
      "[Parallel(n_jobs=1)]: Done  87 tasks      | elapsed: 159.9min\n",
      "[Parallel(n_jobs=1)]: Done  88 tasks      | elapsed: 160.1min\n",
      "[Parallel(n_jobs=1)]: Done  89 tasks      | elapsed: 170.7min\n",
      "[Parallel(n_jobs=1)]: Done  90 tasks      | elapsed: 171.0min\n",
      "[Parallel(n_jobs=1)]: Done  91 tasks      | elapsed: 171.3min\n",
      "[Parallel(n_jobs=1)]: Done  92 tasks      | elapsed: 171.5min\n",
      "[Parallel(n_jobs=1)]: Done  93 tasks      | elapsed: 171.8min\n",
      "[Parallel(n_jobs=1)]: Done  94 tasks      | elapsed: 182.9min\n",
      "[Parallel(n_jobs=1)]: Done  95 tasks      | elapsed: 193.6min\n",
      "[Parallel(n_jobs=1)]: Done  96 tasks      | elapsed: 193.9min\n",
      "[Parallel(n_jobs=1)]: Done  97 tasks      | elapsed: 204.7min\n",
      "[Parallel(n_jobs=1)]: Done  98 tasks      | elapsed: 205.0min\n",
      "[Parallel(n_jobs=1)]: Done  99 tasks      | elapsed: 205.3min\n",
      "[Parallel(n_jobs=1)]: Done 100 tasks      | elapsed: 205.6min\n",
      "[Parallel(n_jobs=1)]: Done 101 tasks      | elapsed: 205.9min\n",
      "[Parallel(n_jobs=1)]: Done 102 tasks      | elapsed: 206.2min\n",
      "[Parallel(n_jobs=1)]: Done 103 tasks      | elapsed: 206.4min\n",
      "[Parallel(n_jobs=1)]: Done 104 tasks      | elapsed: 206.7min\n",
      "[Parallel(n_jobs=1)]: Done 105 tasks      | elapsed: 207.0min\n",
      "[Parallel(n_jobs=1)]: Done 106 tasks      | elapsed: 207.3min\n",
      "[Parallel(n_jobs=1)]: Done 107 tasks      | elapsed: 207.5min\n",
      "[Parallel(n_jobs=1)]: Done 108 tasks      | elapsed: 207.8min\n",
      "[Parallel(n_jobs=1)]: Done 109 tasks      | elapsed: 208.1min\n",
      "[Parallel(n_jobs=1)]: Done 110 tasks      | elapsed: 208.4min\n",
      "[Parallel(n_jobs=1)]: Done 111 tasks      | elapsed: 208.6min\n",
      "[Parallel(n_jobs=1)]: Done 112 tasks      | elapsed: 208.9min\n",
      "[Parallel(n_jobs=1)]: Done 113 tasks      | elapsed: 209.2min\n",
      "[Parallel(n_jobs=1)]: Done 114 tasks      | elapsed: 209.4min\n",
      "[Parallel(n_jobs=1)]: Done 115 tasks      | elapsed: 209.7min\n",
      "[Parallel(n_jobs=1)]: Done 116 tasks      | elapsed: 210.0min\n",
      "[Parallel(n_jobs=1)]: Done 117 tasks      | elapsed: 210.2min\n",
      "[Parallel(n_jobs=1)]: Done 118 tasks      | elapsed: 210.5min\n",
      "[Parallel(n_jobs=1)]: Done 119 tasks      | elapsed: 210.8min\n",
      "[Parallel(n_jobs=1)]: Done 120 tasks      | elapsed: 221.3min\n",
      "[Parallel(n_jobs=1)]: Done 121 tasks      | elapsed: 221.6min\n",
      "[Parallel(n_jobs=1)]: Done 122 tasks      | elapsed: 221.9min\n",
      "[Parallel(n_jobs=1)]: Done 123 tasks      | elapsed: 222.1min\n",
      "[Parallel(n_jobs=1)]: Done 124 tasks      | elapsed: 222.4min\n",
      "[Parallel(n_jobs=1)]: Done 125 tasks      | elapsed: 222.7min\n",
      "[Parallel(n_jobs=1)]: Done 126 tasks      | elapsed: 222.9min\n",
      "[Parallel(n_jobs=1)]: Done 127 tasks      | elapsed: 223.2min\n",
      "[Parallel(n_jobs=1)]: Done 128 tasks      | elapsed: 223.5min\n",
      "[Parallel(n_jobs=1)]: Done 129 tasks      | elapsed: 234.1min\n",
      "[Parallel(n_jobs=1)]: Done 130 tasks      | elapsed: 234.3min\n",
      "[Parallel(n_jobs=1)]: Done 131 tasks      | elapsed: 234.6min\n",
      "[Parallel(n_jobs=1)]: Done 132 tasks      | elapsed: 234.9min\n",
      "[Parallel(n_jobs=1)]: Done 133 tasks      | elapsed: 235.2min\n",
      "[Parallel(n_jobs=1)]: Done 134 tasks      | elapsed: 235.4min\n",
      "[Parallel(n_jobs=1)]: Done 135 tasks      | elapsed: 235.7min\n",
      "[Parallel(n_jobs=1)]: Done 136 tasks      | elapsed: 236.0min\n",
      "[Parallel(n_jobs=1)]: Done 137 tasks      | elapsed: 236.2min\n",
      "[Parallel(n_jobs=1)]: Done 138 tasks      | elapsed: 236.5min\n",
      "[Parallel(n_jobs=1)]: Done 139 tasks      | elapsed: 236.8min\n",
      "[Parallel(n_jobs=1)]: Done 140 tasks      | elapsed: 237.0min\n",
      "[Parallel(n_jobs=1)]: Done 141 tasks      | elapsed: 237.3min\n",
      "[Parallel(n_jobs=1)]: Done 142 tasks      | elapsed: 237.6min\n",
      "[Parallel(n_jobs=1)]: Done 143 tasks      | elapsed: 237.8min\n",
      "[Parallel(n_jobs=1)]: Done 144 tasks      | elapsed: 238.1min\n",
      "[Parallel(n_jobs=1)]: Done 145 tasks      | elapsed: 248.9min\n",
      "[Parallel(n_jobs=1)]: Done 146 tasks      | elapsed: 249.2min\n",
      "[Parallel(n_jobs=1)]: Done 147 tasks      | elapsed: 249.4min\n",
      "[Parallel(n_jobs=1)]: Done 148 tasks      | elapsed: 249.7min\n",
      "[Parallel(n_jobs=1)]: Done 149 tasks      | elapsed: 250.0min\n",
      "[Parallel(n_jobs=1)]: Done 150 tasks      | elapsed: 260.7min\n",
      "[Parallel(n_jobs=1)]: Done 151 tasks      | elapsed: 261.0min\n",
      "[Parallel(n_jobs=1)]: Done 152 tasks      | elapsed: 271.6min\n",
      "[Parallel(n_jobs=1)]: Done 153 tasks      | elapsed: 282.3min\n",
      "[Parallel(n_jobs=1)]: Done 154 tasks      | elapsed: 282.6min\n",
      "[Parallel(n_jobs=1)]: Done 155 tasks      | elapsed: 282.9min\n",
      "[Parallel(n_jobs=1)]: Done 156 tasks      | elapsed: 283.1min\n",
      "[Parallel(n_jobs=1)]: Done 157 tasks      | elapsed: 283.4min\n",
      "[Parallel(n_jobs=1)]: Done 158 tasks      | elapsed: 283.7min\n",
      "[Parallel(n_jobs=1)]: Done 159 tasks      | elapsed: 283.9min\n",
      "[Parallel(n_jobs=1)]: Done 160 tasks      | elapsed: 284.2min\n",
      "[Parallel(n_jobs=1)]: Done 161 tasks      | elapsed: 284.5min\n",
      "[Parallel(n_jobs=1)]: Done 162 tasks      | elapsed: 284.7min\n",
      "[Parallel(n_jobs=1)]: Done 163 tasks      | elapsed: 285.0min\n",
      "[Parallel(n_jobs=1)]: Done 164 tasks      | elapsed: 285.3min\n",
      "[Parallel(n_jobs=1)]: Done 165 tasks      | elapsed: 285.6min\n",
      "[Parallel(n_jobs=1)]: Done 166 tasks      | elapsed: 285.8min\n",
      "[Parallel(n_jobs=1)]: Done 167 tasks      | elapsed: 286.1min\n",
      "[Parallel(n_jobs=1)]: Done 168 tasks      | elapsed: 286.4min\n",
      "[Parallel(n_jobs=1)]: Done 169 tasks      | elapsed: 286.6min\n",
      "[Parallel(n_jobs=1)]: Done 170 tasks      | elapsed: 286.9min\n",
      "[Parallel(n_jobs=1)]: Done 171 tasks      | elapsed: 287.2min\n",
      "[Parallel(n_jobs=1)]: Done 172 tasks      | elapsed: 297.8min\n",
      "[Parallel(n_jobs=1)]: Done 173 tasks      | elapsed: 298.1min\n",
      "[Parallel(n_jobs=1)]: Done 174 tasks      | elapsed: 308.8min\n",
      "[Parallel(n_jobs=1)]: Done 175 tasks      | elapsed: 319.5min\n",
      "[Parallel(n_jobs=1)]: Done 175 tasks      | elapsed: 319.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing statistics for ROI: lpfc\n",
      "stat_func returns a tuple. Taking the first element\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jz421\\AppData\\Local\\anaconda3\\envs\\ieeg\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:1081: RuntimeWarning: divide by zero encountered in divide\n",
      "  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n",
      "c:\\Users\\jz421\\AppData\\Local\\anaconda3\\envs\\ieeg\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:1081: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:   17.7s\n",
      "[Parallel(n_jobs=1)]: Done   2 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=1)]: Done   2 tasks      | elapsed:   35.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing statistics for ROI: occ\n",
      "stat_func returns a tuple. Taking the first element\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jz421\\AppData\\Local\\anaconda3\\envs\\ieeg\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:1081: RuntimeWarning: divide by zero encountered in divide\n",
      "  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n",
      "c:\\Users\\jz421\\AppData\\Local\\anaconda3\\envs\\ieeg\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:1081: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing statistics for ROI: lpfc\n",
      "stat_func returns a tuple. Taking the first element\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jz421\\AppData\\Local\\anaconda3\\envs\\ieeg\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:1081: RuntimeWarning: divide by zero encountered in divide\n",
      "  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n",
      "c:\\Users\\jz421\\AppData\\Local\\anaconda3\\envs\\ieeg\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:1081: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n",
      "[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=1)]: Done   2 tasks      | elapsed:   34.2s\n",
      "[Parallel(n_jobs=1)]: Done   3 tasks      | elapsed:   51.2s\n",
      "[Parallel(n_jobs=1)]: Done   4 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=1)]: Done   5 tasks      | elapsed: 11.8min\n",
      "[Parallel(n_jobs=1)]: Done   6 tasks      | elapsed: 22.4min\n",
      "[Parallel(n_jobs=1)]: Done   7 tasks      | elapsed: 33.0min\n",
      "[Parallel(n_jobs=1)]: Done   8 tasks      | elapsed: 43.6min\n",
      "[Parallel(n_jobs=1)]: Done   9 tasks      | elapsed: 43.9min\n",
      "[Parallel(n_jobs=1)]: Done  10 tasks      | elapsed: 44.2min\n",
      "[Parallel(n_jobs=1)]: Done  11 tasks      | elapsed: 44.5min\n",
      "[Parallel(n_jobs=1)]: Done  12 tasks      | elapsed: 44.8min\n",
      "[Parallel(n_jobs=1)]: Done  13 tasks      | elapsed: 45.0min\n",
      "[Parallel(n_jobs=1)]: Done  14 tasks      | elapsed: 45.3min\n",
      "[Parallel(n_jobs=1)]: Done  15 tasks      | elapsed: 45.6min\n",
      "[Parallel(n_jobs=1)]: Done  16 tasks      | elapsed: 45.9min\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed: 46.2min\n",
      "[Parallel(n_jobs=1)]: Done  17 tasks      | elapsed: 46.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing statistics for ROI: occ\n",
      "stat_func returns a tuple. Taking the first element\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jz421\\AppData\\Local\\anaconda3\\envs\\ieeg\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:1081: RuntimeWarning: divide by zero encountered in divide\n",
      "  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n",
      "c:\\Users\\jz421\\AppData\\Local\\anaconda3\\envs\\ieeg\\Lib\\site-packages\\scipy\\stats\\_stats_py.py:1081: RuntimeWarning: invalid value encountered in scalar multiply\n",
      "  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n"
     ]
    }
   ],
   "source": [
    "# THIS TAKES FOREVER DON'T RUN IT, JUST FOR DEBUGGING SO I CAN PLOT THE SIG CLUSTERS\n",
    "\n",
    "# For per-subject analysis (no electrode filtering needed)\n",
    "sig_elec_masks_per_subject, sig_elec_pvals_per_subject = get_sig_tfr_differences_per_subject(subjects_tfr_objects=subjects_tfr_objects, condition_names=condition_names, stat_func=stat_func, p_thresh=p_thresh, n_perm=n_perm, ignore_adjacency=ignore_adjacency, n_jobs=n_jobs, seed=seed, tails=tails)\n",
    "\n",
    "all_elec_masks_per_subject, all_elec_pvals_per_subject = get_sig_tfr_differences_per_subject(subjects_tfr_objects=subjects_tfr_objects, condition_names=condition_names, stat_func=stat_func, p_thresh=p_thresh, n_perm=n_perm, ignore_adjacency=ignore_adjacency, n_jobs=n_jobs, seed=seed, tails=tails)\n",
    "\n",
    "# For per-ROI analysis (with electrode filtering)\n",
    "sig_elec_masks_per_roi, sig_elec_pvals_per_roi = get_sig_tfr_differences_per_roi(subjects_tfr_objects=subjects_tfr_objects, electrodes_per_subject_roi=sig_electrodes_per_subject_roi, condition_names=condition_names, stat_func=stat_func, p_thresh=p_thresh, n_perm=n_perm, ignore_adjacency=ignore_adjacency, n_jobs=n_jobs, seed=seed, tails=tails)\n",
    "\n",
    "all_elec_masks_per_roi, all_elec_pvals_per_roi = get_sig_tfr_differences_per_roi(subjects_tfr_objects=subjects_tfr_objects, electrodes_per_subject_roi=all_electrodes_per_subject_roi, condition_names=condition_names, stat_func=stat_func, p_thresh=p_thresh, n_perm=n_perm, ignore_adjacency=ignore_adjacency, n_jobs=n_jobs, seed=seed, tails=tails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fa06aa",
   "metadata": {},
   "source": [
    "plot these clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a83c8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved figure: D0057_sig_elecs_sig_multitaper_clusters_stimulus_switch_type_conditions_Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False_1_subjects_page_1.png\n",
      "Saved figure: D0057_sig_elecs_sig_multitaper_clusters_stimulus_switch_type_conditions_Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False_1_subjects_page_2.png\n",
      "Saved figure: D0057_sig_elecs_sig_multitaper_clusters_stimulus_switch_type_conditions_Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False_1_subjects_page_3.png\n",
      "Saved figure: D0057_all_elecs_sig_multitaper_clusters_stimulus_switch_type_conditions_Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False_1_subjects_page_1.png\n",
      "Saved figure: D0057_all_elecs_sig_multitaper_clusters_stimulus_switch_type_conditions_Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False_1_subjects_page_2.png\n",
      "Saved figure: D0057_all_elecs_sig_multitaper_clusters_stimulus_switch_type_conditions_Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False_1_subjects_page_3.png\n",
      "Saved figure: D0057_lpfc_sig_elecs_sig_multitaper_clusters_stimulus_switch_type_conditions_Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False_1_subjects_page_1.png\n",
      "Saved figure: D0057_lpfc_all_elecs_sig_multitaper_clusters_stimulus_switch_type_conditions_Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False_1_subjects_page_1.png\n",
      "Saved figure: D0057_occ_sig_elecs_sig_multitaper_clusters_stimulus_switch_type_conditions_Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False_1_subjects_page_1.png\n",
      "Saved figure: D0057_occ_all_elecs_sig_multitaper_clusters_stimulus_switch_type_conditions_Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False_1_subjects_page_1.png\n"
     ]
    }
   ],
   "source": [
    "# TODO: Update the mean differences to be per subject and per roi, currently copied over from wavelet_differences.\n",
    "first_sub = subjects[0]\n",
    "first_condition = list(subjects_tfr_objects[first_sub].keys())[0]\n",
    "ch_names = subjects_tfr_objects[first_sub][first_condition].ch_names\n",
    "times = subjects_tfr_objects[first_sub][first_condition].times\n",
    "freqs = subjects_tfr_objects[first_sub][first_condition].freqs\n",
    "\n",
    "subjects_tfr_objects_save_dir = os.path.join(layout.root, 'derivatives', 'spec', spec_method, 'subjects_tfr_objects')\n",
    "\n",
    "# Now plot the mask pages:\n",
    "for sub in subjects:\n",
    "    sig_elecs_mask = sig_elec_masks_per_subject[sub]\n",
    "    sig_elecs_mask_pages = plot_mask_pages(sig_elecs_mask,\n",
    "                    ch_names,\n",
    "                    times=times,\n",
    "                    freqs=freqs,\n",
    "                    channels_per_page=60,\n",
    "                    grid_shape=(6, 10),\n",
    "                    cmap=parula_map,\n",
    "                    title_prefix=f\"{sub} \",\n",
    "                    log_freq=True,\n",
    "                    show=False)\n",
    "\n",
    "    # Save each page as a separate figure file:\n",
    "    for i, fig in enumerate(sig_elecs_mask_pages):\n",
    "        fig_name = f\"{sub}_sig_elecs_sig_{spec_method}_clusters_{conditions_save_name}_page_{i+1}.png\"\n",
    "        fig_pathname = os.path.join(subjects_tfr_objects_save_dir, fig_name)\n",
    "        fig.savefig(fig_pathname, bbox_inches='tight')\n",
    "        print(\"Saved figure:\", fig_name)\n",
    "\n",
    "    all_elecs_mask = all_elec_masks_per_subject[sub]\n",
    "    all_elecs_mask_pages = plot_mask_pages(all_elecs_mask,\n",
    "                        ch_names,\n",
    "                        times=times,\n",
    "                        freqs=freqs,\n",
    "                        channels_per_page=60,\n",
    "                        grid_shape=(6, 10),\n",
    "                        cmap=parula_map,\n",
    "                        title_prefix=f\"{sub} \",\n",
    "                        log_freq=True,\n",
    "                        show=False)\n",
    "\n",
    "    # Save each page as a separate figure file:\n",
    "    for i, fig in enumerate(all_elecs_mask_pages):\n",
    "        fig_name = f\"{sub}_all_elecs_sig_{spec_method}_clusters_{conditions_save_name}_page_{i+1}.png\"\n",
    "        fig_pathname = os.path.join(subjects_tfr_objects_save_dir, fig_name)\n",
    "        fig.savefig(fig_pathname, bbox_inches='tight')\n",
    "        print(\"Saved figure:\", fig_name)\n",
    "\n",
    "for roi in rois:\n",
    "    sig_elecs_roi_mask = sig_elec_masks_per_roi[roi]\n",
    "    sig_elecs_roi_mask_pages = plot_mask_pages(sig_elecs_roi_mask,\n",
    "                    ch_names,\n",
    "                    times=times,\n",
    "                    freqs=freqs,\n",
    "                    channels_per_page=60,\n",
    "                    grid_shape=(6, 10),\n",
    "                    cmap=parula_map,\n",
    "                    title_prefix=f\"{sub} {roi} \",\n",
    "                    log_freq=True,\n",
    "                    show=False)\n",
    "\n",
    "    # Save each page as a separate figure file:\n",
    "    for i, fig in enumerate(sig_elecs_roi_mask_pages):\n",
    "        fig_name = f\"{sub}_{roi}_sig_elecs_sig_{spec_method}_clusters_{conditions_save_name}_page_{i+1}.png\"\n",
    "        fig_pathname = os.path.join(subjects_tfr_objects_save_dir, fig_name)\n",
    "        fig.savefig(fig_pathname, bbox_inches='tight')\n",
    "        print(\"Saved figure:\", fig_name)\n",
    "\n",
    "    all_elecs_roi_mask = all_elec_masks_per_roi[roi]\n",
    "    all_elecs_roi_mask_pages = plot_mask_pages(all_elecs_roi_mask,\n",
    "                    ch_names,\n",
    "                    times=times,\n",
    "                    freqs=freqs,\n",
    "                    channels_per_page=60,\n",
    "                    grid_shape=(6, 10),\n",
    "                    cmap=parula_map,\n",
    "                    title_prefix=f\"{sub} {roi} \",\n",
    "                    log_freq=True,\n",
    "                    show=False)\n",
    "\n",
    "    # Save each page as a separate figure file:\n",
    "    for i, fig in enumerate(all_elecs_roi_mask_pages):\n",
    "        fig_name = f\"{sub}_{roi}_all_elecs_sig_{spec_method}_clusters_{conditions_save_name}_page_{i+1}.png\"\n",
    "        fig_pathname = os.path.join(subjects_tfr_objects_save_dir, fig_name)\n",
    "        fig.savefig(fig_pathname, bbox_inches='tight')\n",
    "        print(\"Saved figure:\", fig_name)\n",
    "            \n",
    "# # get the mean differences themselves and plot them\n",
    "# mean_diff_inc_vs_con = mean_diff(incongruent_spec._data, congruent_spec._data, axis=0)\n",
    "# mean_diff_switch_vs_repeat = mean_diff(switch_spec._data, repeat_spec._data, axis=0)\n",
    "\n",
    "# # Now, plot the mean differences directly:\n",
    "# congruency_mean_diff_pages = plot_mask_pages(\n",
    "#     mean_diff_inc_vs_con,\n",
    "#     incongruent_spec.ch_names,\n",
    "#     times=incongruent_spec.times,\n",
    "#     freqs=incongruent_spec.freqs,\n",
    "#     grid_shape=(6, 10),\n",
    "#     cmap=parula_map,  # play with color maps\n",
    "#     title_prefix=f\"{sub} Mean Inc-Con Diff: \",\n",
    "#     log_freq=True,\n",
    "#     show=False\n",
    "# )\n",
    "\n",
    "# # Save each page as a separate figure file:\n",
    "# for i, fig in enumerate(congruency_mean_diff_pages):\n",
    "#     if rescaled:\n",
    "#         fig_name = f\"{sub}_inc-con_mean_diff_multitaper_{conditions_and_output_names_and_events['incongruent']['output_name']}-{conditions_and_output_names_and_events['congruent']['output_name']}_rescaled_page_{i+1}.jpg\"\n",
    "#     else:\n",
    "#         fig_name = f\"{sub}_inc-con_mean_diff_multitaper_{conditions_and_output_names_and_events['incongruent']['output_name']}-{conditions_and_output_names_and_events['congruent']['output_name']}_uncorrected_page_{i+1}.jpg\"\n",
    "#     fig_pathname = os.path.join(save_dir, fig_name)\n",
    "#     fig.savefig(fig_pathname, bbox_inches='tight')\n",
    "#     print(\"Saved figure:\", fig_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a15cb79",
   "metadata": {},
   "source": [
    "decoding functions that concatenate the significant cluster mask for each electrode in an roi, made using training data, and test on just the concatenated significant cluster masks in an roi on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22f0b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_roi_data(subjects_tfr_objects, electrodes_per_subject_roi, roi, subjects, condition_names):\n",
    "    \"\"\"\n",
    "    Collect all data for a specific ROI across subjects and conditions.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    all_data : np.array\n",
    "        Shape: (n_trials, n_channels, n_freqs, n_time)\n",
    "    trial_labels : list\n",
    "        Labels for each trial (e.g., \"D0057_r25_trial0\")\n",
    "    condition_labels : list\n",
    "        Condition name for each trial\n",
    "    channel_labels : list\n",
    "        Channel names for this ROI\n",
    "    freq_labels : list\n",
    "        Frequency values\n",
    "    time_labels : list\n",
    "        Time values\n",
    "    \"\"\"\n",
    "    all_data_list = []\n",
    "    trial_labels = []\n",
    "    condition_labels = []\n",
    "    channel_labels = None\n",
    "    freq_labels = None\n",
    "    time_labels = None\n",
    "    \n",
    "    for sub in subjects:\n",
    "        roi_electrodes = electrodes_per_subject_roi.get(roi, {}).get(sub, [])\n",
    "        if not roi_electrodes:\n",
    "            continue\n",
    "            \n",
    "        for cond_name in condition_names:\n",
    "            tfr = subjects_tfr_objects[sub][cond_name]\n",
    "            \n",
    "            # Get indices of ROI electrodes\n",
    "            electrode_indices = []\n",
    "            for elec in roi_electrodes:\n",
    "                if elec in tfr.ch_names:\n",
    "                    electrode_indices.append(tfr.ch_names.index(elec))\n",
    "            \n",
    "            if not electrode_indices:\n",
    "                continue\n",
    "            \n",
    "            # Extract data for these electrodes\n",
    "            data = tfr.data[:, electrode_indices, :, :]  # (trials, channels, freqs, time)\n",
    "            \n",
    "            # Add to lists\n",
    "            for trial_idx in range(len(data)):\n",
    "                all_data_list.append(data[trial_idx])\n",
    "                trial_labels.append(f\"{sub}_{cond_name}_trial{trial_idx}\")\n",
    "                condition_labels.append(cond_name)\n",
    "            \n",
    "            # Set labels if not set\n",
    "            if channel_labels is None:\n",
    "                channel_labels = [tfr.ch_names[i] for i in electrode_indices]\n",
    "            if freq_labels is None:\n",
    "                freq_labels = tfr.freqs.tolist()\n",
    "            if time_labels is None:\n",
    "                time_labels = tfr.times.tolist()\n",
    "    \n",
    "    all_data = np.array(all_data_list) if all_data_list else np.array([])\n",
    "    \n",
    "    return all_data, trial_labels, condition_labels, channel_labels, freq_labels, time_labels\n",
    "\n",
    "\n",
    "def create_train_tfr_subset(subjects_tfr_objects, train_trials, subjects, condition_names):\n",
    "    \"\"\"\n",
    "    Create a subset of TFR objects containing only training trials.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    train_tfrs : dict\n",
    "        Deep copy of TFR objects filtered to only training trials\n",
    "    \"\"\"\n",
    "    train_tfrs = {}\n",
    "    \n",
    "    for sub in subjects:\n",
    "        train_tfrs[sub] = {}\n",
    "        for cond in condition_names:\n",
    "            # Find training trials for this subject/condition\n",
    "            train_trial_indices = []\n",
    "            for trial_label in train_trials:\n",
    "                if trial_label.startswith(f\"{sub}_{cond}_\"):\n",
    "                    trial_num = int(trial_label.split('_trial')[1])\n",
    "                    train_trial_indices.append(trial_num)\n",
    "            \n",
    "            if train_trial_indices:\n",
    "                train_tfrs[sub][cond] = subjects_tfr_objects[sub][cond][train_trial_indices]\n",
    "            else:\n",
    "                # Create empty TFR\n",
    "                train_tfrs[sub][cond] = subjects_tfr_objects[sub][cond][:0]\n",
    "    \n",
    "    return train_tfrs\n",
    "\n",
    "\n",
    "def create_feature_mask(train_masks, electrodes_per_subject_roi, roi, subjects, \n",
    "                       channel_labels, freq_labels, time_labels, tfr):\n",
    "    \"\"\"\n",
    "    Create a feature mask based on significant clusters from training data.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    feature_mask : np.array\n",
    "        Boolean mask of shape (n_channels, n_freqs, n_times)\n",
    "    \"\"\"\n",
    "    feature_mask = np.zeros((len(channel_labels), len(freq_labels), len(time_labels)), dtype=bool)\n",
    "    \n",
    "    for sub in subjects:\n",
    "        if sub not in train_masks:\n",
    "            continue\n",
    "            \n",
    "        roi_electrodes = electrodes_per_subject_roi.get(roi, {}).get(sub, [])\n",
    "        \n",
    "        for elec_name in roi_electrodes:\n",
    "            if elec_name in tfr.ch_names and elec_name in channel_labels:\n",
    "                orig_elec_idx = tfr.ch_names.index(elec_name)\n",
    "                la_elec_idx = channel_labels.index(elec_name)\n",
    "                \n",
    "                if orig_elec_idx < train_masks[sub].shape[0]:\n",
    "                    feature_mask[la_elec_idx, :, :] = train_masks[sub][orig_elec_idx, :, :]\n",
    "    \n",
    "    return feature_mask\n",
    "\n",
    "\n",
    "def extract_and_clean_features(train_data, test_data, feature_mask):\n",
    "    \"\"\"\n",
    "    Extract features using mask and handle NaN values.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train : np.array\n",
    "        Training features with NaNs imputed\n",
    "    X_test : np.array\n",
    "        Test features with NaNs imputed\n",
    "    \"\"\"\n",
    "    # Reshape to (n_trials, n_features_total)\n",
    "    # MODIFICATION: Use .data to get the raw NumPy array\n",
    "    X_train = train_data.data.reshape(len(train_data), -1)\n",
    "    X_test = test_data.data.reshape(len(test_data), -1)\n",
    "    \n",
    "    # Apply feature mask\n",
    "    mask_flat = feature_mask.flatten()\n",
    "    X_train = X_train[:, mask_flat]\n",
    "    X_test = X_test[:, mask_flat]\n",
    "    \n",
    "    # Handle NaN values\n",
    "    col_mean = np.zeros(X_train.shape[1])\n",
    "    if np.any(np.isnan(X_train)):\n",
    "        col_mean = np.nanmean(X_train, axis=0)\n",
    "        nan_mask = np.isnan(X_train)\n",
    "        X_train[nan_mask] = np.take(col_mean, np.where(nan_mask)[1])\n",
    "    \n",
    "    if np.any(np.isnan(X_test)):\n",
    "        nan_mask = np.isnan(X_test)\n",
    "        # Use the mean calculated from the training data to fill NaNs in the test data\n",
    "        X_test[nan_mask] = np.take(col_mean, np.where(nan_mask)[1])\n",
    "    \n",
    "    return X_train, X_test\n",
    "\n",
    "def balance_training_data(X_train, y_train, random_state):\n",
    "    \"\"\"\n",
    "    Balance training data by subsampling to the smallest class.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train_balanced : np.array\n",
    "        Balanced training features\n",
    "    y_train_balanced : np.array\n",
    "        Balanced training labels\n",
    "    \"\"\"\n",
    "    unique_labels, counts = np.unique(y_train, return_counts=True)\n",
    "    min_trials = np.min(counts)\n",
    "    \n",
    "    balanced_indices = []\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    \n",
    "    for label_val in unique_labels:\n",
    "        label_indices = np.where(y_train == label_val)[0]\n",
    "        subsampled = rng.choice(label_indices, min_trials, replace=False)\n",
    "        balanced_indices.extend(subsampled)\n",
    "    \n",
    "    X_train_balanced = X_train[balanced_indices]\n",
    "    y_train_balanced = y_train[balanced_indices]\n",
    "    \n",
    "    return X_train_balanced, y_train_balanced\n",
    "\n",
    "def run_single_cv_fold(fold_data):\n",
    "    \"\"\"\n",
    "    Run a single cross-validation fold using PCA-LDA classifier.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    fold_data : dict\n",
    "        Dictionary containing all necessary data for one fold\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    confusion_matrix : np.array\n",
    "        Confusion matrix for this fold\n",
    "    \"\"\"\n",
    "    # Extract parameters\n",
    "    train_trials = fold_data['train_trials']\n",
    "    test_trials = fold_data['test_trials']\n",
    "    train_data = fold_data['train_data']\n",
    "    test_data = fold_data['test_data']\n",
    "    y_train = fold_data['y_train']\n",
    "    y_test = fold_data['y_test']\n",
    "    subjects_tfr_objects = fold_data['subjects_tfr_objects']\n",
    "    subjects = fold_data['subjects']\n",
    "    condition_names = fold_data['condition_names']\n",
    "    stat_func = fold_data['stat_func']\n",
    "    p_thresh = fold_data['p_thresh']\n",
    "    n_perm = fold_data['n_perm']\n",
    "    ignore_adjacency = fold_data['ignore_adjacency']\n",
    "    random_state = fold_data['random_state']\n",
    "    fold_idx = fold_data['fold_idx']\n",
    "    electrodes_per_subject_roi = fold_data['electrodes_per_subject_roi']\n",
    "    roi = fold_data['roi']\n",
    "    channel_labels = fold_data['channel_labels']\n",
    "    freq_labels = fold_data['freq_labels']\n",
    "    time_labels = fold_data['time_labels']\n",
    "    tfr = fold_data['tfr']\n",
    "    cats = fold_data['cats']\n",
    "    \n",
    "    print(f\"    Processing fold {fold_idx + 1}\")\n",
    "    \n",
    "    # 1. Create training TFR subset\n",
    "    train_tfrs = create_train_tfr_subset(\n",
    "        subjects_tfr_objects, train_trials, subjects, condition_names\n",
    "    )\n",
    "    \n",
    "    # 2. Find significant clusters using only training data\n",
    "    train_masks, _ = get_sig_tfr_differences_per_subject(\n",
    "        subjects_tfr_objects=train_tfrs,\n",
    "        condition_names=condition_names,\n",
    "        stat_func=stat_func,\n",
    "        p_thresh=p_thresh,\n",
    "        n_perm=n_perm,\n",
    "        ignore_adjacency=ignore_adjacency,\n",
    "        seed=random_state,\n",
    "        tails=2\n",
    "    )\n",
    "    \n",
    "    # 3. Create feature mask\n",
    "    feature_mask = create_feature_mask(\n",
    "        train_masks, electrodes_per_subject_roi, roi, subjects,\n",
    "        channel_labels, freq_labels, time_labels, tfr\n",
    "    )\n",
    "    \n",
    "    n_features = np.sum(feature_mask)\n",
    "    print(f\"    Using {n_features} features from significant clusters\")\n",
    "    \n",
    "    # 4. Extract and clean features\n",
    "    X_train, X_test = extract_and_clean_features(train_data, test_data, feature_mask)\n",
    "    \n",
    "    # 5. Balance training data\n",
    "    X_train_balanced, y_train_balanced = balance_training_data(\n",
    "        X_train, y_train, random_state + fold_idx\n",
    "    )\n",
    "    \n",
    "    # 6. Use the Decoder class (PCA-LDA) instead of SVC\n",
    "    # The Decoder class uses PcaLdaClassification internally\n",
    "    # First parameter is the percentage of variance to retain (0.80 = 80%)\n",
    "    decoder = Decoder(cats, 0.80, n_splits=1, n_repeats=1, oversample=False)\n",
    "    \n",
    "    # Fit and predict using the decoder's methods\n",
    "    decoder.fit(X_train_balanced, y_train_balanced)\n",
    "    preds = decoder.predict(X_test)\n",
    "    \n",
    "    return confusion_matrix(y_test, preds, labels=list(cats.values()))\n",
    "\n",
    "def run_cv_cluster_decoding(\n",
    "    subjects_tfr_objects,\n",
    "    electrodes_per_subject_roi,\n",
    "    rois,\n",
    "    subjects,\n",
    "    condition_names,\n",
    "    stat_func,\n",
    "    p_thresh,\n",
    "    n_perm,\n",
    "    ignore_adjacency,\n",
    "    n_splits=5,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Main function that orchestrates the cross-validated cluster-based decoding.\n",
    "    \"\"\"\n",
    "    results_per_roi = {}\n",
    "    cats = {name: i for i, name in enumerate(condition_names)}\n",
    "    \n",
    "    for roi in rois:\n",
    "        print(f\"\\n--- Starting Cross-Validation for ROI: {roi} ---\")\n",
    "        \n",
    "        # 1. Collect all data for this ROI\n",
    "        all_data, trial_labels, condition_labels, channel_labels, freq_labels, time_labels = \\\n",
    "            collect_roi_data(subjects_tfr_objects, electrodes_per_subject_roi, \n",
    "                           roi, subjects, condition_names)\n",
    "        \n",
    "        if len(all_data) == 0:\n",
    "            print(f\"  No data found for ROI: {roi}\")\n",
    "            continue\n",
    "        \n",
    "        # Create LabeledArray\n",
    "        roi_data = LabeledArray(\n",
    "            all_data,\n",
    "            labels=[trial_labels, channel_labels, freq_labels, time_labels]\n",
    "        )\n",
    "        \n",
    "        # Create label array for classification\n",
    "        y_labels = np.array([cats[cond] for cond in condition_labels])\n",
    "        \n",
    "        # 2. Set up cross-validation\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "        fold_cms = []\n",
    "        \n",
    "        # Get a sample TFR for channel name lookups\n",
    "        sample_tfr = subjects_tfr_objects[subjects[0]][condition_names[0]]\n",
    "        \n",
    "        for fold_idx, (train_indices, test_indices) in enumerate(skf.split(np.zeros(len(y_labels)), y_labels)):\n",
    "            print(f\"  --- Running Fold {fold_idx + 1}/{n_splits} ---\")\n",
    "\n",
    "            # Create the list of trial labels for indexing\n",
    "            train_label_list = [trial_labels[i] for i in train_indices]\n",
    "            test_label_list = [trial_labels[i] for i in test_indices]\n",
    "            \n",
    "            # Prepare fold data with corrected, explicit indexing\n",
    "            fold_data = {\n",
    "                'train_trials': train_label_list,\n",
    "                'test_trials': test_label_list,\n",
    "                # Corrected Lines: Wrap the list in a tuple to ensure it's treated as an index for the first dimension.\n",
    "                'train_data': roi_data[(train_label_list,)], \n",
    "                'test_data': roi_data[(test_label_list,)],\n",
    "                'y_train': y_labels[train_indices],\n",
    "                'y_test': y_labels[test_indices],\n",
    "                'subjects_tfr_objects': subjects_tfr_objects,\n",
    "                'subjects': subjects,\n",
    "                'condition_names': condition_names,\n",
    "                'stat_func': stat_func,\n",
    "                'p_thresh': p_thresh,\n",
    "                'n_perm': n_perm,\n",
    "                'ignore_adjacency': ignore_adjacency,\n",
    "                'random_state': random_state,\n",
    "                'fold_idx': fold_idx,\n",
    "                'electrodes_per_subject_roi': electrodes_per_subject_roi,\n",
    "                'roi': roi,\n",
    "                'channel_labels': channel_labels,\n",
    "                'freq_labels': freq_labels,\n",
    "                'time_labels': time_labels,\n",
    "                'tfr': sample_tfr,\n",
    "                'cats': cats\n",
    "            }\n",
    "\n",
    "            # Run single fold\n",
    "            cm = run_single_cv_fold(fold_data)\n",
    "            fold_cms.append(cm)\n",
    "        \n",
    "        # 3. Aggregate results\n",
    "        final_cm = np.sum(fold_cms, axis=0)\n",
    "        accuracy = np.trace(final_cm) / np.sum(final_cm)\n",
    "        \n",
    "        results_per_roi[roi] = {\n",
    "            'confusion_matrix': final_cm,\n",
    "            'accuracy': accuracy,\n",
    "            'cats': cats,\n",
    "            'labeled_array': roi_data,\n",
    "            'fold_cms': fold_cms  # Keep individual fold results for analysis\n",
    "        }\n",
    "        \n",
    "        print(f\"  --> Final Accuracy for {roi}: {accuracy:.3f}\")\n",
    "    \n",
    "    return results_per_roi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3988d0d8",
   "metadata": {},
   "source": [
    "plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d77ad2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cv_confusion_matrices(cv_results, save_dir, conditions_save_name):\n",
    "    \"\"\"\n",
    "    Plot confusion matrices for cross-validated cluster-based decoding results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cv_results : dict\n",
    "        Results from run_cv_cluster_decoding containing confusion matrices per ROI\n",
    "    save_dir : str\n",
    "        Directory to save plots\n",
    "    conditions_save_name : str\n",
    "        Name identifier for the conditions being compared\n",
    "    \"\"\"\n",
    "    for roi, results in cv_results.items():\n",
    "        # Get the final aggregated confusion matrix\n",
    "        cm = results['confusion_matrix']\n",
    "        cats = results['cats']\n",
    "        accuracy = results['accuracy']\n",
    "        \n",
    "        # Also get individual fold confusion matrices\n",
    "        fold_cms = results.get('fold_cms', [])\n",
    "        \n",
    "        # 1. Plot the aggregated confusion matrix\n",
    "        plot_single_confusion_matrix(\n",
    "            cm, cats, accuracy, roi, \n",
    "            f'{roi}_CV_cluster_decoded_final_cm_{conditions_save_name}',\n",
    "            save_dir,\n",
    "            title_suffix='Final Aggregated'\n",
    "        )\n",
    "        \n",
    "        # 2. Plot individual fold confusion matrices in a grid\n",
    "        if fold_cms:\n",
    "            plot_fold_confusion_matrices(\n",
    "                fold_cms, cats, roi,\n",
    "                f'{roi}_CV_cluster_decoded_folds_{conditions_save_name}',\n",
    "                save_dir\n",
    "            )\n",
    "        \n",
    "        # 3. Plot normalized confusion matrix\n",
    "        plot_normalized_confusion_matrix(\n",
    "            cm, cats, accuracy, roi,\n",
    "            f'{roi}_CV_cluster_decoded_normalized_{conditions_save_name}',\n",
    "            save_dir\n",
    "        )\n",
    "\n",
    "\n",
    "def plot_single_confusion_matrix(cm, cats, accuracy, roi, filename, save_dir, \n",
    "                                title_suffix='', normalize=False):\n",
    "    \"\"\"\n",
    "    Plot a single confusion matrix with customizable normalization.\n",
    "    \"\"\"\n",
    "    # Normalize if requested\n",
    "    if normalize:\n",
    "        cm_plot = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm_plot = np.nan_to_num(cm_plot)  # Handle division by zero\n",
    "    else:\n",
    "        cm_plot = cm\n",
    "    \n",
    "    display_labels = list(cats.keys())\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Create the display\n",
    "    if normalize:\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm_plot, display_labels=display_labels)\n",
    "        im = disp.plot(ax=ax, cmap='Blues', values_format='.2f')\n",
    "        im.im_.set_clim(0, 1)\n",
    "    else:\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm_plot, display_labels=display_labels)\n",
    "        im = disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "    \n",
    "    # Add title\n",
    "    title = f'ROI: {roi} - Accuracy: {accuracy:.3f}'\n",
    "    if title_suffix:\n",
    "        title += f' ({title_suffix})'\n",
    "    ax.set_title(title, fontsize=14, pad=20)\n",
    "    \n",
    "    # Enhance the plot\n",
    "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "    ax.set_ylabel('True Label', fontsize=12)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    ax.set_xticks(np.arange(len(display_labels)) - 0.5, minor=True)\n",
    "    ax.set_yticks(np.arange(len(display_labels)) - 0.5, minor=True)\n",
    "    ax.grid(which='minor', color='gray', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'{filename}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_fold_confusion_matrices(fold_cms, cats, roi, filename, save_dir):\n",
    "    \"\"\"\n",
    "    Plot all fold confusion matrices in a grid layout.\n",
    "    \"\"\"\n",
    "    n_folds = len(fold_cms)\n",
    "    n_cols = min(3, n_folds)\n",
    "    n_rows = (n_folds + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "    if n_folds == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    display_labels = list(cats.keys())\n",
    "    \n",
    "    for idx, cm in enumerate(fold_cms):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Normalize the confusion matrix\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm_normalized = np.nan_to_num(cm_normalized)\n",
    "        \n",
    "        # Calculate fold accuracy\n",
    "        fold_accuracy = np.trace(cm) / np.sum(cm)\n",
    "        \n",
    "        # Plot\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, \n",
    "                                    display_labels=display_labels)\n",
    "        im = disp.plot(ax=ax, cmap='Blues', values_format='.2f', colorbar=False)\n",
    "        im.im_.set_clim(0, 1)\n",
    "        \n",
    "        ax.set_title(f'Fold {idx+1} (Acc: {fold_accuracy:.3f})', fontsize=12)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "    \n",
    "    # Remove unused subplots\n",
    "    for idx in range(n_folds, len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    # Add overall title\n",
    "    fig.suptitle(f'ROI: {roi} - Individual Fold Confusion Matrices', fontsize=16)\n",
    "    \n",
    "    # Add shared labels\n",
    "    fig.text(0.5, 0.02, 'Predicted Label', ha='center', fontsize=14)\n",
    "    fig.text(0.02, 0.5, 'True Label', va='center', rotation='vertical', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'{filename}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_normalized_confusion_matrix(cm, cats, accuracy, roi, filename, save_dir):\n",
    "    \"\"\"\n",
    "    Plot a normalized confusion matrix showing percentages.\n",
    "    \"\"\"\n",
    "    # Calculate different normalizations\n",
    "    cm_row_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    cm_row_norm = np.nan_to_num(cm_row_norm)\n",
    "    \n",
    "    display_labels = list(cats.keys())\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Row-normalized (recall)\n",
    "    disp1 = ConfusionMatrixDisplay(confusion_matrix=cm_row_norm, \n",
    "                                  display_labels=display_labels)\n",
    "    im1 = disp1.plot(ax=ax1, cmap='Blues', values_format='.2%')\n",
    "    im1.im_.set_clim(0, 1)\n",
    "    ax1.set_title(f'Row-Normalized (Recall)\\nAccuracy: {accuracy:.3f}', fontsize=14)\n",
    "    \n",
    "    # Plot 2: Raw counts with percentages\n",
    "    total_samples = np.sum(cm)\n",
    "    cm_percentage = cm / total_samples\n",
    "    \n",
    "    # Create custom annotation\n",
    "    im2 = ax2.imshow(cm_percentage, cmap='Blues', aspect='auto')\n",
    "    \n",
    "    # Add text annotations showing both count and percentage\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            count = cm[i, j]\n",
    "            pct = cm_percentage[i, j] * 100\n",
    "            text = f'{int(count)}\\n({pct:.1f}%)'\n",
    "            color = 'white' if cm_percentage[i, j] > 0.5 else 'black'\n",
    "            ax2.text(j, i, text, ha='center', va='center', color=color, fontsize=10)\n",
    "    \n",
    "    ax2.set_xticks(range(len(display_labels)))\n",
    "    ax2.set_yticks(range(len(display_labels)))\n",
    "    ax2.set_xticklabels(display_labels)\n",
    "    ax2.set_yticklabels(display_labels)\n",
    "    ax2.set_xlabel('Predicted Label')\n",
    "    ax2.set_ylabel('True Label')\n",
    "    ax2.set_title(f'Raw Counts with Percentages\\nTotal Samples: {int(total_samples)}', fontsize=14)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im2, ax=ax2)\n",
    "    cbar.set_label('Proportion of Total', rotation=270, labelpad=20)\n",
    "    \n",
    "    fig.suptitle(f'ROI: {roi} - Detailed Confusion Matrix Analysis', fontsize=16)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'{filename}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_accuracy_comparison_across_rois(cv_results, save_dir, conditions_save_name):\n",
    "    \"\"\"\n",
    "    Create a bar plot comparing accuracies across ROIs.\n",
    "    \"\"\"\n",
    "    rois = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for roi, results in cv_results.items():\n",
    "        rois.append(roi)\n",
    "        accuracies.append(results['accuracy'])\n",
    "    \n",
    "    # Create bar plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars = ax.bar(rois, accuracies, color='steelblue', alpha=0.8)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    # Add chance level line (assuming binary classification)\n",
    "    n_classes = len(list(cv_results.values())[0]['cats'])\n",
    "    chance_level = 1.0 / n_classes\n",
    "    ax.axhline(y=chance_level, color='red', linestyle='--', alpha=0.7, \n",
    "               label=f'Chance Level ({chance_level:.3f})')\n",
    "    \n",
    "    ax.set_xlabel('ROI', fontsize=14)\n",
    "    ax.set_ylabel('Accuracy', fontsize=14)\n",
    "    ax.set_title(f'Decoding Accuracy Across ROIs\\n{conditions_save_name}', fontsize=16)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f'accuracy_comparison_across_rois_{conditions_save_name}.png'\n",
    "    plt.savefig(os.path.join(save_dir, filename), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_cv_results_summary(cv_results, save_dir, conditions_save_name):\n",
    "    \"\"\"\n",
    "    Save a text summary of the cross-validation results.\n",
    "    \"\"\"\n",
    "    summary_file = os.path.join(save_dir, f'cv_results_summary_{conditions_save_name}.txt')\n",
    "    \n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(f\"Cross-Validation Cluster-Based Decoding Results\\n\")\n",
    "        f.write(f\"Conditions: {conditions_save_name}\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        \n",
    "        for roi, results in cv_results.items():\n",
    "            f.write(f\"ROI: {roi}\\n\")\n",
    "            f.write(f\"Final Accuracy: {results['accuracy']:.4f}\\n\")\n",
    "            f.write(f\"Number of folds: {len(results.get('fold_cms', []))}\\n\")\n",
    "            \n",
    "            # Calculate per-class metrics\n",
    "            cm = results['confusion_matrix']\n",
    "            cats = results['cats']\n",
    "            \n",
    "            f.write(\"\\nPer-class metrics:\\n\")\n",
    "            for i, (cat_name, cat_idx) in enumerate(cats.items()):\n",
    "                if i < cm.shape[0]:  # Ensure we don't go out of bounds\n",
    "                    tp = cm[i, i]\n",
    "                    fp = np.sum(cm[:, i]) - tp\n",
    "                    fn = np.sum(cm[i, :]) - tp\n",
    "                    \n",
    "                    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "                    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "                    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                    \n",
    "                    f.write(f\"  {cat_name}:\\n\")\n",
    "                    f.write(f\"    Precision: {precision:.4f}\\n\")\n",
    "                    f.write(f\"    Recall: {recall:.4f}\\n\")\n",
    "                    f.write(f\"    F1-score: {f1:.4f}\\n\")\n",
    "            \n",
    "            f.write(\"\\nConfusion Matrix:\\n\")\n",
    "            f.write(str(cm) + \"\\n\")\n",
    "            f.write(\"\\n\" + \"=\"*60 + \"\\n\\n\")\n",
    "\n",
    "\n",
    "# Enhanced main execution with plotting\n",
    "def run_analysis_with_plots(subjects_tfr_objects, all_electrodes_per_subject_roi, \n",
    "                           rois, subjects, condition_names, stat_func, p_thresh, \n",
    "                           n_perm, ignore_adjacency, n_splits, random_state,save_dir, conditions_save_name):\n",
    "    \"\"\"\n",
    "    Run the complete analysis pipeline with enhanced plotting.\n",
    "    \"\"\"\n",
    "    # Ensure save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Run the cross-validated analysis\n",
    "    print(\"Running cross-validated cluster-based decoding...\")\n",
    "    cv_results = run_cv_cluster_decoding(\n",
    "        subjects_tfr_objects=subjects_tfr_objects,\n",
    "        electrodes_per_subject_roi=all_electrodes_per_subject_roi,\n",
    "        rois=rois,\n",
    "        subjects=subjects,\n",
    "        condition_names=condition_names,\n",
    "        stat_func=stat_func,\n",
    "        p_thresh=p_thresh,\n",
    "        n_perm=n_perm,\n",
    "        ignore_adjacency=ignore_adjacency,\n",
    "        n_splits=n_splits,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Generate all plots\n",
    "    print(\"Generating confusion matrix plots...\")\n",
    "    plot_cv_confusion_matrices(cv_results, save_dir, conditions_save_name)\n",
    "    \n",
    "    print(\"Generating accuracy comparison plot...\")\n",
    "    plot_accuracy_comparison_across_rois(cv_results, save_dir, conditions_save_name)\n",
    "    \n",
    "    print(\"Saving results summary...\")\n",
    "    save_cv_results_summary(cv_results, save_dir, conditions_save_name)\n",
    "    \n",
    "    print(\"Analysis complete!\")\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8d3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cross-validated cluster-based decoding...\n",
      "\n",
      "--- Starting Cross-Validation for ROI: lpfc ---\n",
      "  --- Running Fold 1/2 ---\n",
      "    Processing fold 1\n"
     ]
    }
   ],
   "source": [
    "# --- HOW TO RUN THE NEW PIPELINE ---\n",
    "\n",
    "# Make sure your save directory is defined\n",
    "save_dir = os.path.join(layout.root, 'derivatives', 'decoding', 'james_sun_cluster_decoding', conditions_save_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Run the enhanced analysis with all plots\n",
    "cv_results = run_analysis_with_plots(\n",
    "    subjects_tfr_objects=subjects_tfr_objects,\n",
    "    all_electrodes_per_subject_roi=all_electrodes_per_subject_roi,\n",
    "    rois=rois,\n",
    "    subjects=subjects,\n",
    "    condition_names=condition_names,\n",
    "    stat_func=stat_func,\n",
    "    p_thresh=p_thresh,\n",
    "    n_perm=n_perm,\n",
    "    ignore_adjacency=ignore_adjacency,\n",
    "    n_splits=n_splits,\n",
    "    random_state=random_state,\n",
    "    save_dir=save_dir,\n",
    "    conditions_save_name=conditions_save_name\n",
    ")\n",
    "\n",
    "with open(os.path.join(save_dir, f'cv_results_{conditions_save_name}.pkl'), 'wb') as f:\n",
    "    pickle.dump(cv_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f83ddec",
   "metadata": {},
   "source": [
    "#### 3. Train a decoder on just the significant time-frequency clusters, test on the test trials\n",
    "\n",
    "#### 4. Repeat but with new test trials (cross-validate)\n",
    "\n",
    "#### 5. Figure out a way of plotting this. This is univariate approach, make things modular so I can do multivariate later once I figure that out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
