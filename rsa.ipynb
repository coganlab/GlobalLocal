{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "sys.path.append(\"C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/\") #need to do this cuz otherwise ieeg isn't added to path...\n",
    "\n",
    "from ieeg.navigate import channel_outlier_marker, trial_ieeg, crop_empty_data, \\\n",
    "    outliers_to_nan\n",
    "from ieeg.io import raw_from_layout, get_data\n",
    "from ieeg.timefreq.utils import crop_pad\n",
    "from ieeg.timefreq import gamma\n",
    "from ieeg.calc.scaling import rescale\n",
    "import mne\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ieeg.calc.reshape import make_data_same\n",
    "from ieeg.calc.stats import time_perm_cluster\n",
    "\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas import read_csv\n",
    "import scipy.stats as stats\n",
    "import joblib\n",
    "\n",
    "from scipy.ndimage import label\n",
    "\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make model rdms here (just do congruency and switch type for testing for now 5/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conditions\n",
    "conditions = [\"i25s25\", \"i25s75\", \"i75s25\", \"i75s75\", \"i25r25\", \"i25r75\", \"i75r25\", \"i75r75\",\n",
    "              \"c25s25\", \"c25s75\", \"c75s25\", \"c75s75\", \"c25r25\", \"c25r75\", \"c75r25\", \"c75r75\"]\n",
    "\n",
    "# Extract specific features from each condition\n",
    "congruency = [cond[0] for cond in conditions]\n",
    "switchTypes = [cond[-3] for cond in conditions]\n",
    "congruencyProportion = [int(cond[1:3]) for cond in conditions]\n",
    "switchProportion = [int(cond[-2:]) for cond in conditions]\n",
    "\n",
    "# Number of conditions\n",
    "n = len(conditions)\n",
    "\n",
    "# initialize dict to store model RDMs\n",
    "model_rdms = {'congruency': np.ones((n, n)), 'switchType': np.ones((n, n)),\n",
    "              'congruencyProportion': np.ones((n, n)), 'switchProportion': np.ones((n, n)), \n",
    "              'congruency:congruencyProportion': np.ones((n, n)), 'switchType:switchProportion': np.ones((n,n)),\n",
    "              'congruency:switchProportion': np.ones((n, n)), 'switchType:congruencyProportion': np.ones((n,n))}\n",
    "\n",
    "# Populate RDMs based on feature comparisons\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if congruency[i] == congruency[j]:\n",
    "            model_rdms['congruency'][i, j] = 0\n",
    "        if switchTypes[i] == switchTypes[j]:\n",
    "            model_rdms['switchType'][i, j] = 0\n",
    "        if congruencyProportion[i] == congruencyProportion[j]:\n",
    "            model_rdms['congruencyProportion'][i, j] = 0\n",
    "        if switchProportion[i] == switchProportion[j]:\n",
    "            model_rdms['switchProportion'][i, j] = 0\n",
    "        if congruency[i] == congruency[j] and congruencyProportion[i] == congruencyProportion[j]:\n",
    "            model_rdms['congruency:congruencyProportion'][i,j] = 0\n",
    "        if switchTypes[i] == switchTypes[j] and switchProportion[i] == switchProportion[j]:\n",
    "            model_rdms['switchType:switchProportion'][i,j] = 0\n",
    "        if congruency[i] == congruency[j] and switchProportion[i] == switchProportion[j]:\n",
    "            model_rdms['congruency:switchProportion'][i,j] = 0\n",
    "        if switchTypes[i] == switchTypes[j] and congruencyProportion[i] == congruencyProportion[j]:\n",
    "            model_rdms['switchType:congruencyProportion'][i,j] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load in epochs data  \n",
    "(get only acc trials, avg across epochs to make evoked using only acc trials, and then delete epochs from memory) 5/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['D0057','D0059', 'D0063', 'D0065', 'D0069', 'D0071', 'D0077', 'D0090', 'D0094', 'D0100', 'D0102', 'D0103']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make or load subjects electrodes to rois dict (mapping from electrode names to roi labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in subjects electrodes to rois dict. If it doesn't already exist, make it and then load it.\n",
    "filename = 'subjects_electrodestoROIs_dict.json'\n",
    "subjects_electrodestoROIs_dict = utils.make_or_load_subjects_electrodes_to_rois_dict(filename, subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load accuracy arrays so we can filter by only accurate trials  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this makes numpy arrays for each subject that are 0 or 1 for each trial based on accuracy\n",
    "from makeRawBehavioralData import main\n",
    "main()\n",
    "\n",
    "# Directory where your .npy files are saved\n",
    "npy_directory = r'C:\\Users\\jz421\\Box\\CoganLab\\D_Data\\GlobalLocal\\accArrays'  # Replace with your directory path if you're not Jim\n",
    "\n",
    "acc_array = utils.load_acc_arrays(npy_directory, skip_subjects=['D107'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load in behavioral data and convert block types to congruency and switch proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.read_csv(r'C:\\Users\\jz421\\Box\\CoganLab\\D_Data\\GlobalLocal\\combinedData.csv')\n",
    "\n",
    "# Apply the function to each row and create new columns\n",
    "combined_data[['congruencyProportion', 'switchProportion']] = combined_data.apply(utils.map_block_type, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load epochs and evoked for all 16 conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test these functions to just load in HG_ev1_rescaled cuz memory issues 5/6. Replace the ones in utils.py if these work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mne_objects(sub, output_name, task, just_HG_ev1_rescaled=False, LAB_root=None):\n",
    "    \"\"\"\n",
    "    Load MNE objects for a given subject and output name, with an option to load only rescaled high gamma epochs.\n",
    "\n",
    "    Parameters:\n",
    "    - sub (str): Subject identifier.\n",
    "    - output_name (str): Output name used in the file naming.\n",
    "    - task (str): Task identifier.\n",
    "    - just_HG_ev1_rescaled (bool): If True, only the rescaled high gamma epochs are loaded.\n",
    "    - LAB_root (str, optional): Root directory for the lab. If None, it will be determined based on the OS.\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing loaded MNE objects.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine LAB_root based on the operating system\n",
    "    if LAB_root is None:\n",
    "        HOME = os.path.expanduser(\"~\")\n",
    "        LAB_root = os.path.join(HOME, \"Box\", \"CoganLab\") if os.name == 'nt' else os.path.join(HOME, \"Library\", \"CloudStorage\", \"Box-Box\", \"CoganLab\")\n",
    "\n",
    "    # Get data layout\n",
    "    layout = get_data(task, root=LAB_root)\n",
    "    save_dir = os.path.join(layout.root, 'derivatives', 'freqFilt', 'figs', sub)\n",
    "\n",
    "    # Ensure save directory exists\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Initialize the return dictionary\n",
    "    mne_objects = {}\n",
    "\n",
    "    if just_HG_ev1_rescaled:\n",
    "        # Define path and load only the rescaled high gamma epochs\n",
    "        HG_ev1_rescaled_file = f'{save_dir}/{sub}_{output_name}_HG_ev1_rescaled-epo.fif'\n",
    "        HG_ev1_rescaled = mne.read_epochs(HG_ev1_rescaled_file)\n",
    "        mne_objects['HG_ev1_rescaled'] = HG_ev1_rescaled\n",
    "    else:\n",
    "        # Define file paths\n",
    "        HG_ev1_file = f'{save_dir}/{sub}_{output_name}_HG_ev1-epo.fif'\n",
    "        HG_base_file = f'{save_dir}/{sub}_{output_name}_HG_base-epo.fif'\n",
    "        HG_ev1_rescaled_file = f'{save_dir}/{sub}_{output_name}_HG_ev1_rescaled-epo.fif'\n",
    "        \n",
    "        # Load the objects\n",
    "        HG_ev1 = mne.read_epochs(HG_ev1_file)\n",
    "        HG_base = mne.read_epochs(HG_base_file)\n",
    "        HG_ev1_evoke = HG_ev1.average(method=lambda x: np.nanmean(x, axis=0))\n",
    "        HG_ev1_rescaled = mne.read_epochs(HG_ev1_rescaled_file)\n",
    "        HG_ev1_evoke_rescaled = HG_ev1_rescaled.average(method=lambda x: np.nanmean(x, axis=0))\n",
    "\n",
    "        mne_objects['HG_ev1'] = HG_ev1\n",
    "        mne_objects['HG_base'] = HG_base\n",
    "        mne_objects['HG_ev1_evoke'] = HG_ev1_evoke\n",
    "        mne_objects['HG_ev1_rescaled'] = HG_ev1_rescaled\n",
    "        mne_objects['HG_ev1_evoke_rescaled'] = HG_ev1_evoke_rescaled\n",
    "\n",
    "    return mne_objects\n",
    "\n",
    "\n",
    "def create_subjects_mne_objects_dict(subjects, output_names_conditions, task, combined_data, acc_array, just_HG_ev1_rescaled=False, LAB_root=None):\n",
    "    \"\"\"\n",
    "    Adjusted to handle multiple conditions per output name, with multiple condition columns.\n",
    "\n",
    "    Parameters:\n",
    "    - subjects: List of subject IDs.\n",
    "    - output_names_conditions: Dictionary where keys are output names and values are dictionaries\n",
    "        of condition column names and their required values.\n",
    "    - task: Task identifier.\n",
    "    - combined_data: DataFrame with combined behavioral and trial information.\n",
    "    - acc_array: dict of numpy arrays of 0 for incorrect and 1 for correct trials for each subject\n",
    "    - LAB_root: Root directory for data (optional).\n",
    "    \"\"\"\n",
    "    subjects_mne_objects = {}\n",
    "\n",
    "    for sub in subjects:\n",
    "        print(f\"Loading data for subject: {sub}\")\n",
    "        sub_mne_objects = {}\n",
    "        for output_name, conditions in output_names_conditions.items():\n",
    "            print(f\"  Loading output: {output_name} with conditions: {conditions}\")\n",
    "            \n",
    "            # Build the filtering condition\n",
    "            sub_without_zeroes = \"D\" + sub[1:].lstrip('0') \n",
    "            condition_filter = (combined_data['subject_ID'] == sub) # this previously indexed using sub_without_zeroes, but now just uses sub. 3/17.\n",
    "                    \n",
    "            for condition_column, condition_value in conditions.items():\n",
    "                if isinstance(condition_value, list):\n",
    "                    # If the condition needs to match any value in a list\n",
    "                    condition_filter &= combined_data[condition_column].isin(condition_value)\n",
    "                else:\n",
    "                    # If the condition is a single value\n",
    "                    condition_filter &= (combined_data[condition_column] == condition_value)\n",
    "            \n",
    "            # Filter combinedData for the specific subject and conditions\n",
    "            subject_condition_data = combined_data[condition_filter]\n",
    "            \n",
    "            # Load MNE objects and update with accuracy data\n",
    "            mne_objects = load_mne_objects(sub, output_name, task, just_HG_ev1_rescaled=just_HG_ev1_rescaled, LAB_root=None)\n",
    "            \n",
    "            if sub in acc_array:\n",
    "                trial_counts = subject_condition_data['trialCount'].values.astype(int)\n",
    "                accuracy_data = [acc_array[sub][i-1] for i in trial_counts if i-1 < len(acc_array[sub])] # Subtract 1 here for zero-based indexing in acc array.\n",
    "                # Now pass trial_counts along with accuracy_data\n",
    "                mne_objects['HG_ev1_rescaled'] = utils.add_accuracy_to_epochs(mne_objects['HG_ev1_rescaled'], accuracy_data)\n",
    "\n",
    "            sub_mne_objects[output_name] = mne_objects\n",
    "        subjects_mne_objects[sub] = sub_mne_objects\n",
    "\n",
    "    return subjects_mne_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_names = [\"Stimulus_i25s25_fixationCrossBase_1sec_mirror_0to1Test\", \"Stimulus_i25s75_fixationCrossBase_1sec_mirror_0to1Test\", \n",
    "                \"Stimulus_i75s25_fixationCrossBase_1sec_mirror_0to1Test\", \"Stimulus_i75s75_fixationCrossBase_1sec_mirror_0to1Test\", \n",
    "                \"Stimulus_i25r25_fixationCrossBase_1sec_mirror_0to1Test\", \"Stimulus_i25r75_fixationCrossBase_1sec_mirror_0to1Test\", \n",
    "                \"Stimulus_i75r25_fixationCrossBase_1sec_mirror_0to1Test\", \"Stimulus_i75r75_fixationCrossBase_1sec_mirror_0to1Test\", \n",
    "                \"Stimulus_c25s25_fixationCrossBase_1sec_mirror_0to1Test\", \"Stimulus_c25s75_fixationCrossBase_1sec_mirror_0to1Test\", \n",
    "                \"Stimulus_c75s25_fixationCrossBase_1sec_mirror_0to1Test\", \"Stimulus_c75s75_fixationCrossBase_1sec_mirror_0to1Test\",\n",
    "                \"Stimulus_c25r25_fixationCrossBase_1sec_mirror_0to1Test\", \"Stimulus_c25r75_fixationCrossBase_1sec_mirror_0to1Test\",\n",
    "                \"Stimulus_c75r25_fixationCrossBase_1sec_mirror_0to1Test\", \"Stimulus_c75r75_fixationCrossBase_1sec_mirror_0to1Test\",]\n",
    "                \n",
    "output_names_conditions = {\n",
    "    \"Stimulus_i25s25_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus_i25s75_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus_i75s25_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus_i75s75_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus_i25r25_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus_i25r75_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus_i75r25_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus_i75r75_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus_c25s25_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus_c25s75_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus_c75s25_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus_c75s75_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus_c25r25_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus_c25r75_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus_c75r25_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus_c75r75_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },                                                            \n",
    "}\n",
    "\n",
    "task='GlobalLocal'\n",
    "\n",
    "# Assuming 'combined_data' is your DataFrame and 'subjects' is your list of subject IDs\n",
    "subjects_mne_objects = create_subjects_mne_objects_dict(subjects, output_names_conditions, task=\"GlobalLocal\", combined_data=combined_data, acc_array=acc_array, just_HG_ev1_rescaled=True)\n",
    "\n",
    "# Save the subjects_mne_objects dictionary to a file\n",
    "# joblib.dump(subjects_mne_objects, 'subjects_mne_objects.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the subjects_mne_objects from the file. This is not equal to when we create subjects_mne_objects directly...test this later. 5/6.\n",
    "# subjects_mne_objects_test = joblib.load('subjects_mne_objects.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load stimulus significant channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_chans_per_subject = utils.get_sig_chans_per_subject(subjects, task='GlobalLocal', LAB_root=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get electrodes for each roi (definitions based on destrieux atlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois_dict = {\n",
    "    # 'dlpfc': [\"G_front_middle\", \"G_front_sup\", \"S_front_inf\", \"S_front_middle\", \"S_front_sup\"],\n",
    "    # 'acc': [\"G_and_S_cingul-Ant\", \"G_and_S_cingul-Mid-Ant\"],\n",
    "    # 'parietal': [\"G_parietal_sup\", \"S_intrapariet_and_P_trans\", \"G_pariet_inf-Angular\", \"G_pariet_inf-Supramar\"],\n",
    "    'lpfc': [\"G_front_inf-Opercular\", \"G_front_inf-Orbital\", \"G_front_inf-Triangul\", \"G_front_middle\", \"G_front_sup\", \"Lat_Fis-ant-Horizont\", \"Lat_Fis-ant-Vertical\", \"S_circular_insula_ant\", \"S_circular_insula_sup\", \"S_front_inf\", \"S_front_middle\", \"S_front_sup\"],\n",
    "    'v1': [\"G_oc-temp_med-Lingual\", \"S_calcarine\", \"G_cuneus\"]\n",
    "}\n",
    "\n",
    "rois = list(rois_dict.keys())\n",
    "electrodes_per_subject_roi, sig_electrodes_per_subject_roi = utils.make_sig_electrodes_per_subject_and_roi_dict(rois_dict, subjects_electrodestoROIs_dict, sig_chans_per_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_electrodes_per_subject_roi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get electrode counts for each roi (just for fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_electrodes_info = utils.calculate_total_electrodes(sig_electrodes_per_subject_roi, electrodes_per_subject_roi)\n",
    "for roi, counts in total_electrodes_info.items():\n",
    "    print(f\"Total number of significant {roi} electrodes across all subjects:\", counts['total_significant_electrodes'])\n",
    "    print(f\"Total number of {roi} electrodes across all subjects:\", counts['total_electrodes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if any subjects have a different sampling rate than 2048 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sampling_rates(subjects_mne_objects):\n",
    "    # This dictionary will store subjects with different sampling rates\n",
    "    different_sampling_rates = {}\n",
    "    \n",
    "    # Iterate through each subject and their corresponding data\n",
    "    for subject, data in subjects_mne_objects.items():\n",
    "        # Access the specific Epochs object and check its sampling rate\n",
    "        if 'Stimulus_i25s25_fixationCrossBase_1sec_mirror_0to1Test' in data:\n",
    "            epochs = data['Stimulus_i25s25_fixationCrossBase_1sec_mirror_0to1Test']['HG_ev1_rescaled']\n",
    "            sampling_rate = epochs.info['sfreq']\n",
    "            \n",
    "            # Check if the sampling rate is not 2048.0 Hz\n",
    "            if sampling_rate != 2048.0:\n",
    "                different_sampling_rates[subject] = sampling_rate\n",
    "    \n",
    "    return different_sampling_rates\n",
    "\n",
    "# Assuming 'subjects_mne_objects' is your dictionary containing MNE objects for each subject\n",
    "different_rates = check_sampling_rates(subjects_mne_objects)\n",
    "\n",
    "# Print the results\n",
    "if different_rates:\n",
    "    print(\"Subjects with different sampling rates:\")\n",
    "    for subject, rate in different_rates.items():\n",
    "        print(f\"Subject {subject} has a sampling rate of {rate} Hz.\")\n",
    "else:\n",
    "    print(\"All subjects have a sampling rate of 2048.0 Hz.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get trial averaged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time indices just for the function to work. Don't actually use time averaged data.\n",
    "time_indices = {\n",
    "    'firstHalfSecond': (2048, 3072),\n",
    "    'secondHalfSecond': (3072, 4096),\n",
    "    'fullSecond': (2048, 4096)\n",
    "}\n",
    "# Process the data\n",
    "data_trialAvg_lists, data_trialStd_lists, _, overall_electrode_mapping, electrode_mapping_per_roi = utils.process_data_for_roi(\n",
    "    subjects_mne_objects, output_names, rois, subjects, sig_electrodes_per_subject_roi, time_indices)\n",
    "\n",
    "# need to figure out how to use the overall_electrode_mapping to get the electrode names back. Apparently I do this in the ANOVA in roi analysis. 5/5.\n",
    "\n",
    "# Concatenate the data\n",
    "concatenated_trialAvg_data = utils.concatenate_data(data_trialAvg_lists, rois, output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_std(data_dict):\n",
    "    \"\"\"\n",
    "    Computes the mean and standard deviation across electrodes for each time point in each ROI and output name.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dict (dict): A dictionary containing trial-averaged neural data (concatenated_trialAvg_data). It is expected to have a structure where each key is an output name,\n",
    "      and its value is another dictionary. This nested dictionary should have ROIs as keys, and arrays of shape (n_electrodes, n_timepoints)\n",
    "      as values.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing two dictionaries:\n",
    "        - mean_data: A dictionary with the same structure as input, but each array contains the mean across electrodes at each timepoint.\n",
    "        - std_data: A dictionary with the same structure as input, but each array contains the standard deviation across electrodes at each timepoint.\n",
    "\n",
    "    Example:\n",
    "    mean_trialAvg_data, std_trialAvg_data = compute_avg_std(concatenated_trialAvg_data)\n",
    "    print(\"Mean data for LPFC:\", mean_trialAvg_data['Stimulus_i25s25_fixationCrossBase_1sec_mirror_0to1Test']['lpfc'])\n",
    "    print(\"Standard deviation data for LPFC:\", std_trialAvg_data['Stimulus_i25s25_fixationCrossBase_1sec_mirror_0to1Test']['lpfc'])\n",
    "    \"\"\"\n",
    "    mean_data = {}\n",
    "    std_data = {}\n",
    "    for output_name, rois_data in data_dict.items():\n",
    "        mean_data[output_name] = {}\n",
    "        std_data[output_name] = {}\n",
    "        for roi, data in rois_data.items():\n",
    "            # Compute mean and std across the electrodes (axis=0)\n",
    "            mean_data[output_name][roi] = np.mean(data, axis=0)\n",
    "            std_data[output_name][roi] = np.std(data, axis=0)\n",
    "    return mean_data, std_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean and std across electrodes for each time point, in the already trial-averaged data.\n",
    "mean_across_elecs_trialAvg_data, std_across_elecs_trialAvg_data = compute_avg_std(concatenated_trialAvg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try kumar's suggestion of flattening the data to be more data driven 5/7  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_trialAvg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do RSA!  \n",
    "1. Create time windows of 20 ms width, at 5 ms steps through the electrode and trial-averaged data.  \n",
    "2. For each time window and ROI, calculate (1-Spearman correlation coefficient) between each condition. This will make a 16x16 neural RDM for each time window and ROI.  \n",
    "3. For each neural RDM, find its correlation with each model RDM.\n",
    "4. Do time perm cluster stats for significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.ndimage import label\n",
    "\n",
    "def create_time_windows(data, window_size_ms, step_size_ms, sampling_rate):\n",
    "    window_size_samples = int(window_size_ms * sampling_rate / 1000)\n",
    "    step_size_samples = int(step_size_ms * sampling_rate / 1000)\n",
    "    total_samples = data.shape[0]\n",
    "    windows = [(start, start + window_size_samples) for start in range(0, total_samples - window_size_samples + 1, step_size_samples)]\n",
    "    return windows\n",
    "\n",
    "def make_spearman_rdm(data_list):\n",
    "    num_conditions = len(data_list)\n",
    "    rdm = np.zeros((num_conditions, num_conditions))\n",
    "    flattened_data = [data.flatten() if data.ndim > 1 else data for data in data_list]\n",
    "    for i in range(num_conditions):\n",
    "        for j in range(num_conditions):\n",
    "            if i != j:\n",
    "                correlation, _ = stats.spearmanr(flattened_data[i], flattened_data[j])\n",
    "                rdm[i, j] = 1 - correlation\n",
    "            else:\n",
    "                rdm[i, j] = 0\n",
    "    return rdm\n",
    "\n",
    "def make_neural_rdms_whole_roi(data_dict, sampling_rate=2048.0, window_size_ms=100, step_size_ms=20):\n",
    "    example_condition = next(iter(data_dict.values()))\n",
    "    example_roi_array = next(iter(example_condition.values()))\n",
    "    example_electrode_data = example_roi_array[0, :]\n",
    "    sample_windows = create_time_windows(example_electrode_data, window_size_ms, step_size_ms, sampling_rate)\n",
    "\n",
    "    all_rdms = {}\n",
    "    roi_data = {}\n",
    "    for output_name, rois in data_dict.items():\n",
    "        for roi, electrode_time_data in rois.items():\n",
    "            if roi not in roi_data:\n",
    "                roi_data[roi] = []\n",
    "            roi_data[roi].append(electrode_time_data)\n",
    "\n",
    "    for roi, electrode_time_data_list in roi_data.items():\n",
    "        all_rdms[roi] = []\n",
    "        for window in sample_windows:\n",
    "            windowed_data_list = [condition_array[:, window[0]:window[1]] for condition_array in electrode_time_data_list]\n",
    "            rdm = make_spearman_rdm(windowed_data_list)\n",
    "            all_rdms[roi].append((window, rdm))\n",
    "    \n",
    "    return all_rdms\n",
    "\n",
    "def permutation_test_with_clustering_for_rsa(neural_rdms, model_rdm, num_permutations=10000, alpha=0.05):\n",
    "    actual_correlations = []\n",
    "    perm_correlations = np.zeros((num_permutations, len(neural_rdms)))\n",
    "\n",
    "    for idx, (window, neural_rdm) in enumerate(neural_rdms):\n",
    "        neural_rdm_tril = neural_rdm[np.tril_indices_from(neural_rdm, k=-1)]\n",
    "        model_rdm_tril = model_rdm[np.tril_indices_from(model_rdm, k=-1)]\n",
    "        \n",
    "        # Compute correlations for the actual data\n",
    "        correlation, _ = stats.spearmanr(neural_rdm_tril, model_rdm_tril)\n",
    "        actual_correlations.append(correlation)\n",
    "        \n",
    "        for perm in range(num_permutations):\n",
    "            shuffled_model_rdm_tril = np.random.permutation(model_rdm_tril)\n",
    "            perm_correlation, _ = stats.spearmanr(neural_rdm_tril, shuffled_model_rdm_tril)\n",
    "            perm_correlations[perm, idx] = perm_correlation\n",
    "\n",
    "    threshold = np.percentile(perm_correlations, 100 *  (1-alpha))\n",
    "\n",
    "    actual_correlations = np.array(actual_correlations)\n",
    "    \n",
    "    clusters, num_clusters = label(actual_correlations > threshold)\n",
    "    cluster_sizes = np.array([np.sum(actual_correlations[clusters == cluster]) for cluster in range(1, num_clusters + 1)])\n",
    "    \n",
    "    max_permuted_cluster_sizes = np.zeros(num_permutations)\n",
    "    for perm in range(num_permutations):\n",
    "        perm_clusters, num_perm_clusters = label(perm_correlations[perm, :] > threshold)\n",
    "        if num_perm_clusters > 0:\n",
    "            max_permuted_cluster_sizes[perm] = max([np.sum(perm_correlations[perm, perm_clusters == cluster]) for cluster in range(1, num_perm_clusters + 1)])\n",
    "    \n",
    "    p_values = np.ones_like(actual_correlations)  # Default p-value to 1.0 (not significant)\n",
    "    cluster_p_values = np.array([np.sum(max_permuted_cluster_sizes >= size) / num_permutations for size in cluster_sizes])\n",
    "    \n",
    "    for cluster_index in range(1, num_clusters + 1):\n",
    "        p_values[clusters == cluster_index] = cluster_p_values[cluster_index - 1]\n",
    "    \n",
    "    return p_values\n",
    "\n",
    "def calculate_rsa_with_permutation_testing(neural_rdms, model_rdms, rois, num_permutations=10000, alpha=0.05):\n",
    "    rsa_results = {roi: {model_name: [] for model_name in model_rdms} for roi in rois}\n",
    "    \n",
    "    for roi in rois:\n",
    "        for model_name, model_rdm in model_rdms.items():\n",
    "            p_values = permutation_test_with_clustering_for_rsa(neural_rdms[roi], model_rdm, num_permutations, alpha)\n",
    "            for idx, (window, neural_rdm) in enumerate(neural_rdms[roi]):\n",
    "                neural_rdm_tril = neural_rdm[np.tril_indices_from(neural_rdm, k=-1)]\n",
    "                model_rdm_tril = model_rdm[np.tril_indices_from(model_rdm, k=-1)]\n",
    "                correlation, _ = stats.spearmanr(neural_rdm_tril, model_rdm_tril)\n",
    "                rsa_results[roi][model_name].append((window, 1-correlation, p_values[idx] if idx < len(p_values) else np.nan))\n",
    "    \n",
    "    return rsa_results\n",
    "\n",
    "def calculate_rsa_whole_roi(data_dict, model_rdms, rois, sampling_rate=2048.0, window_size_ms=100, step_size_ms=20, num_permutations=10000, alpha=0.05):\n",
    "    neural_rdms = make_neural_rdms_whole_roi(data_dict, sampling_rate, window_size_ms, step_size_ms)\n",
    "    rsa_results = calculate_rsa_with_permutation_testing(neural_rdms, model_rdms, rois, num_permutations, alpha)\n",
    "    return rsa_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# congruency_model_rdm = {}\n",
    "# congruency_model_rdm['congruency'] = model_rdms['congruency']\n",
    "\n",
    "neural_rdms_by_samples_and_roi = make_neural_rdms_whole_roi(concatenated_trialAvg_data, sampling_rate=2048.0, window_size_ms=200, step_size_ms=20)\n",
    "rois = list(neural_rdms_by_samples_and_roi.keys())\n",
    "rsa_results = calculate_rsa_whole_roi(concatenated_trialAvg_data, model_rdms, rois, num_permutations=500, alpha=0.05)\n",
    "\n",
    "# Save rsa results for each electrode to a file\n",
    "rois_str = '_'.join(rois)\n",
    "with open(f'rsa_results_whole_roi_{rois_str}.pkl', 'wb') as file:\n",
    "    pickle.dump(rsa_results, file)\n",
    "\n",
    "# Example: Print the RSA result for a specific ROI and model\n",
    "print(\"RSA results for ROI 'lpfc' and model 'congruency':\", rsa_results['lpfc']['congruency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save rsa results for each electrode to a file\n",
    "rois_str = '_'.join(rois)\n",
    "with open(f'rsa_results_whole_roi_{rois_str}.pkl', 'wb') as file:\n",
    "    pickle.dump(rsa_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsa_results['lpfc']['congruency']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try to do rsa on each electrode individually too 5/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_index_mapping(sig_electrodes_per_subject_roi):\n",
    "    \"\"\"\n",
    "    Constructs a mapping from subject and electrode labels to indices for each ROI.\n",
    "    \"\"\"\n",
    "    index_mapping = {}\n",
    "    for roi, subjects in sig_electrodes_per_subject_roi.items():\n",
    "        index_mapping[roi] = {}\n",
    "        electrode_counter = 0  # Reset counter for each ROI\n",
    "        for subject, electrodes in subjects.items():\n",
    "            for electrode in electrodes:\n",
    "                index_mapping[roi][electrode_counter] = (subject, electrode)\n",
    "                electrode_counter += 1\n",
    "    return index_mapping\n",
    "\n",
    "def make_neural_rdms_per_electrode(data_dict, sig_electrodes_per_subject_roi, sampling_rate, window_size_ms, step_size_ms):\n",
    "    \"\"\"\n",
    "    Makes a neural RDM across all conditions for each subject-specific electrode in each ROI and time window, using ROI-specific indexing.\n",
    "    \"\"\"\n",
    "\n",
    "    example_condition = next(iter(data_dict.values()))  # Get the first condition's data\n",
    "    example_roi = next(iter(example_condition.values()))  # Get the first ROI's data array from that condition\n",
    "    example_electrode_data = example_roi[0, :]  # Get the first electrode (row) time series from the example roi array\n",
    "    sample_windows = create_time_windows(example_electrode_data, window_size_ms, step_size_ms, sampling_rate)\n",
    "\n",
    "    # Prepare the index mapping for accessing subject and electrode names\n",
    "    index_mapping = make_index_mapping(sig_electrodes_per_subject_roi)\n",
    "\n",
    "    # Initialize a dictionary to hold the RDMs\n",
    "    all_rdms = {roi: {} for roi in sig_electrodes_per_subject_roi}\n",
    "    # First organize the data by ROI, aggregating across all conditions\n",
    "    roi_data = {}\n",
    "    for output_name, rois in data_dict.items():\n",
    "        for roi, electrode_time_data in rois.items():\n",
    "            if roi not in roi_data:\n",
    "                roi_data[roi] = []\n",
    "            roi_data[roi].append(electrode_time_data)\n",
    "\n",
    "    # Calculate RDMs for each electrode and time window\n",
    "    for roi, electrode_time_data_list in roi_data.items():\n",
    "        for window in sample_windows:\n",
    "            for electrode_index in range(electrode_time_data_list[0].shape[0]):\n",
    "                windowed_data_list = [condition_array[electrode_index, window[0]:window[1]] for condition_array in electrode_time_data_list]\n",
    "                rdm = make_spearman_rdm(windowed_data_list)\n",
    "                subject, electrode = index_mapping[roi][electrode_index]\n",
    "                if subject not in all_rdms[roi]:\n",
    "                    all_rdms[roi][subject] = {}\n",
    "                if electrode not in all_rdms[roi][subject]:\n",
    "                    all_rdms[roi][subject][electrode] = []\n",
    "                all_rdms[roi][subject][electrode].append((window, rdm))\n",
    "\n",
    "    return all_rdms\n",
    "\n",
    "def calculate_rsa_per_electrode(neural_rdms, model_rdms, rois):\n",
    "    \"\"\"\n",
    "    Calculates the RSA between neural RDMs and model RDMs for each electrode in each ROI, including subject-specific data.\n",
    "    \n",
    "    Parameters:\n",
    "    - neural_rdms: dict\n",
    "        A dictionary of neural RDMs structured by ROI, then subject, then electrode.\n",
    "    - model_rdms: dict\n",
    "        A dictionary of model RDMs structured by ROI.\n",
    "    - rois: list\n",
    "        List of ROIs to be analyzed.\n",
    "    \n",
    "    Returns:\n",
    "    - dict\n",
    "        A structured dictionary containing RSA results mapped by ROI, subject, electrode, and sample window.\n",
    "    \"\"\"\n",
    "    rsa_results = {}\n",
    "\n",
    "    for roi in rois:\n",
    "        rsa_results[roi] = {}\n",
    "\n",
    "        for model_name, model_rdm in model_rdms.items():\n",
    "            rsa_results[roi][model_name] = {}\n",
    "\n",
    "            for subject in neural_rdms[roi]:\n",
    "                rsa_results[roi][model_name][subject] = {}\n",
    "\n",
    "                for electrode in neural_rdms[roi][subject]:\n",
    "                    rsa_results[roi][model_name][subject][electrode] = []\n",
    "\n",
    "                    neural_rdm_windows = neural_rdms[roi][subject][electrode]\n",
    "                    p_values = permutation_test_with_clustering_for_rsa(neural_rdm_windows, model_rdm, num_permutations=500, alpha=0.05)\n",
    "                    \n",
    "                    for idx, (window, neural_rdm) in enumerate(neural_rdm_windows):\n",
    "                        # Extract the lower triangular part of the neural RDM, excluding the diagonal\n",
    "                        tri_indices = np.tril_indices_from(neural_rdm, k=-1)\n",
    "                        neural_rdm_tril = neural_rdm[tri_indices]\n",
    "                        model_rdm_tril = model_rdm[tri_indices]\n",
    "                        correlation, _ = stats.spearmanr(neural_rdm_tril, model_rdm_tril)\n",
    "                        rsa_results[roi][model_name][subject][electrode].append((window, 1-correlation, p_values[idx] if idx < len(p_values) else np.nan))\n",
    "               \n",
    "    return rsa_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_rdms_by_samples_and_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_rdms_by_sub_and_elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_rdms_by_sub_and_elec = make_neural_rdms_per_electrode(concatenated_trialAvg_data, sig_electrodes_per_subject_roi, sampling_rate=2048.0, window_size_ms=100, step_size_ms=20)\n",
    "\n",
    "# Define ROIs based on keys in your neural_rdms_by_samples_and_roi (if they are consistent)\n",
    "rois = list(neural_rdms_by_sub_and_elec.keys())\n",
    "\n",
    "# Calculate RSA\n",
    "rsa_results_by_sub_and_elec = calculate_rsa_per_electrode(neural_rdms_by_sub_and_elec, model_rdms, rois)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsa_results_by_sub_and_elec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save rsa results for each electrode to a file\n",
    "with open('rsa_results_by_sub_and_elec.pkl', 'wb') as file:\n",
    "    pickle.dump(rsa_results_by_sub_and_elec, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the rsa results for each electrode\n",
    "# with open('rsa_results_by_sub_and_elec.pkl', 'rb') as file:\n",
    "#     rsa_results_by_sub_and_elec = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsa_results_by_sub_and_elec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot rsa results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_significance_rsa(ax, sig_windows, y_offset=0.1):\n",
    "    \"\"\"\n",
    "    Plot significance bars for the effects on top of the existing axes, adjusted for time windows.\n",
    "\n",
    "    Parameters:\n",
    "    - ax: The matplotlib Axes object to plot on.\n",
    "    - sig_windows: List of tuples where each tuple is (start_time, end_time, p_value).\n",
    "    - y_offset: The vertical offset for placing significance bars.\n",
    "    \"\"\"\n",
    "    y_pos_base = ax.get_ylim()[1]  # Get the top y-axis limit to place significance bars\n",
    "\n",
    "    # Cluster contiguous significant windows\n",
    "    clusters = []\n",
    "    current_cluster = []\n",
    "\n",
    "    for start_time, end_time, p_value in sig_windows:\n",
    "        if not current_cluster:\n",
    "            current_cluster = [start_time, end_time, p_value]\n",
    "        else:\n",
    "            # Check if the current window is contiguous with the last window in the cluster\n",
    "            if start_time <= current_cluster[1]:\n",
    "                current_cluster[1] = end_time\n",
    "                current_cluster[2] = min(current_cluster[2], p_value)  # Use the most significant p_value\n",
    "            else:\n",
    "                clusters.append(tuple(current_cluster))\n",
    "                current_cluster = [start_time, end_time, p_value]\n",
    "\n",
    "    # Append the last cluster if it exists\n",
    "    if current_cluster:\n",
    "        clusters.append(tuple(current_cluster))\n",
    "\n",
    "    for start_time, end_time, p_value in clusters:\n",
    "        y_pos = y_pos_base + y_offset\n",
    "\n",
    "        # Define the color and number of asterisks based on the p_value\n",
    "        color = 'black'\n",
    "        num_asterisks = '*' if p_value < 0.05 else '**' if p_value < 0.01 else '***' if p_value < 0.001 else ''\n",
    "        \n",
    "        if num_asterisks:  # Only plot bars for significant p-values\n",
    "            # Draw the significance bar\n",
    "            ax.plot([start_time, end_time], [y_pos, y_pos], color=color, lw=4)\n",
    "            ax.text((start_time + end_time) / 2, y_pos, num_asterisks, ha='center', va='bottom', color=color)\n",
    "\n",
    "def plot_rsa_correlation_by_time(rsa_results, model_name, roi, colors, sampling_rate=2048.0, time_shift=1000, significance_level=0.05, save_dir=None):\n",
    "    \"\"\"\n",
    "    Plots RSA correlation over time for specified ROIs and model.\n",
    "\n",
    "    Parameters:\n",
    "    - rsa_results: dict\n",
    "        A dictionary containing RSA results.\n",
    "    - model_name: str\n",
    "        The model name to plot RSA results for.\n",
    "    - roi: string\n",
    "        ROIs to be included in the plot.\n",
    "    - colors: dict\n",
    "        A dictionary mapping ROIs to plot colors.\n",
    "    - sampling_rate: float, optional\n",
    "        The sampling rate of the data in Hz. Default is 2048.0.\n",
    "    - time_shift: int, optional\n",
    "        Time in milliseconds to shift the x-axis. Default is 1000 ms.\n",
    "    - significance_level: float, optional\n",
    "        The significance level to determine significant windows. Default is 0.05.\n",
    "    \"\"\"\n",
    "    # Create the figure and axis objects\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    if roi in rsa_results:\n",
    "        times = []\n",
    "        # correlations = []\n",
    "        correlation_distances = []\n",
    "        sig_windows = []\n",
    "        for window, correlation_distance, p_value in rsa_results[roi][model_name]:\n",
    "            # Calculate the middle of the window\n",
    "            middle_sample = (window[0] + window[1]) / 2\n",
    "            time_ms = (middle_sample * 1000 / sampling_rate) - time_shift  # Subtract the shift to align times\n",
    "            times.append(time_ms)\n",
    "            correlation_distances.append(correlation_distance)\n",
    "            \n",
    "            # Collect significant windows\n",
    "            if p_value < significance_level:\n",
    "                start_time = (window[0] * 1000 / sampling_rate) - time_shift\n",
    "                end_time = (window[1] * 1000 / sampling_rate) - time_shift\n",
    "                sig_windows.append((start_time, end_time, p_value))\n",
    "\n",
    "        # Sort the data by time for plotting lines\n",
    "        sorted_indices = np.argsort(times)\n",
    "        times = np.array(times)[sorted_indices]\n",
    "        correlation_distances = np.array(correlation_distances)[sorted_indices]\n",
    "\n",
    "        # Plot each ROI's data with a specific color and connect points with lines\n",
    "        ax.scatter(times, correlation_distances, color=colors[model_name], label=f'{roi}')\n",
    "        ax.plot(times, correlation_distances, color=colors[model_name])  # This line connects the dots\n",
    "\n",
    "        # Plot significance bars\n",
    "        plot_significance_rsa(ax, sig_windows)\n",
    "\n",
    "    # Adding labels and title\n",
    "    ax.set_xlabel('Time from Stim Onset (ms)')\n",
    "    ax.set_ylabel('1-Ï')\n",
    "    ax.set_title(f'{model_name.capitalize()} RSA Correlation Distance in {roi}')\n",
    "    # ax.legend()\n",
    "\n",
    "    # Set x-axis limits to reflect the new time range\n",
    "    ax.set_xlim(-1000, 1500)\n",
    "    if save_dir:\n",
    "        save_name = f\"{roi}_{model_name}_RSA_whole_roi.png\"\n",
    "        plt.savefig(os.path.join(save_dir, save_name))\n",
    "        plt.close(fig)\n",
    "    # Show the plot\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_rsa_correlation_by_electrode_grid(rsa_results, model_name, roi, colors, sampling_rate=2048.0, time_shift=1000, significance_level=0.05, grid_size=(4, 4), save_dir=None):\n",
    "    \"\"\"\n",
    "    Plots RSA correlation over time for each electrode in a grid layout, for specified ROIs and model.\n",
    "\n",
    "    Parameters:\n",
    "    - rsa_results: dict\n",
    "        A dictionary containing RSA results.\n",
    "    - model_name: str\n",
    "        The model name to plot RSA results for.\n",
    "    - roi: string\n",
    "        ROI to plot.\n",
    "    - colors: dict\n",
    "        A dictionary mapping ROIs to plot colors.\n",
    "    - sampling_rate: float, optional\n",
    "        The sampling rate of the data in Hz. Default is 2048.0.\n",
    "    - time_shift: int, optional\n",
    "        Time in milliseconds to shift the x-axis. Default is 1000 ms.\n",
    "    - significance_level: float, optional\n",
    "        The significance level to determine significant windows. Default is 0.05.\n",
    "    - grid_size: tuple, optional\n",
    "        The grid size (rows, columns) for the electrode plots. Default is (4, 4).\n",
    "    - save_dir: str, optional\n",
    "        Directory to save the plots. If None, plots are not saved. Default is None.\n",
    "\n",
    "    \"\"\"\n",
    "    all_electrodes = []\n",
    "\n",
    "    # Flatten the electrode data for this roi across all subjects\n",
    "    for subject in rsa_results[roi][model_name]:\n",
    "        electrodes = rsa_results[roi][model_name][subject]\n",
    "        for electrode in electrodes:\n",
    "            all_electrodes.append((roi, subject, electrode, electrodes[electrode]))\n",
    "\n",
    "    num_electrodes = len(all_electrodes)\n",
    "    num_plots = grid_size[0] * grid_size[1]\n",
    "    num_grids = (num_electrodes + num_plots - 1) // num_plots\n",
    "\n",
    "    for grid_idx in range(num_grids):\n",
    "        fig, axes = plt.subplots(grid_size[0], grid_size[1], figsize=(20, 12))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for plot_idx in range(num_plots):\n",
    "            electrode_idx = grid_idx * num_plots + plot_idx\n",
    "            if electrode_idx >= num_electrodes:\n",
    "                axes[plot_idx].axis('off')\n",
    "                continue\n",
    "\n",
    "            roi, subject, electrode, electrode_data = all_electrodes[electrode_idx]\n",
    "            times = []\n",
    "            correlations = []\n",
    "            sig_windows = []\n",
    "\n",
    "            for window, correlation, p_value in electrode_data:\n",
    "                middle_sample = (window[0] + window[1]) / 2\n",
    "                time_ms = (middle_sample * 1000 / sampling_rate) - time_shift\n",
    "                times.append(time_ms)\n",
    "                correlations.append(correlation)\n",
    "\n",
    "                if p_value < significance_level:\n",
    "                    start_time = (window[0] * 1000 / sampling_rate) - time_shift\n",
    "                    end_time = (window[1] * 1000 / sampling_rate) - time_shift\n",
    "                    sig_windows.append((start_time, end_time, p_value))\n",
    "\n",
    "            sorted_indices = np.argsort(times)\n",
    "            times = np.array(times)[sorted_indices]\n",
    "            correlations = np.array(correlations)[sorted_indices]\n",
    "\n",
    "            ax = axes[plot_idx]\n",
    "            ax.scatter(times, correlations, color=colors[model_name], label=f'{electrode}')\n",
    "            ax.plot(times, correlations, color=colors[model_name])\n",
    "            plot_significance_rsa(ax, sig_windows)\n",
    "\n",
    "            ax.set_title(f'{roi} - Subject {subject}, Electrode {electrode}')\n",
    "            ax.set_xlabel('Time from Stim Onset (ms)')\n",
    "            ax.set_ylabel('1-Ï')\n",
    "            # ax.axvline(x=0.5, color='k', linestyle='--', linewidth=1)\n",
    "            ax.spines['top'].set_visible(False)\n",
    "            ax.spines['right'].set_visible(False)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if save_dir:\n",
    "            save_name = f\"{roi}_{model_name}_single_electrodes_grid_{grid_idx + 1}.png\"\n",
    "            plt.savefig(os.path.join(save_dir, save_name))\n",
    "            plt.close(fig)\n",
    "        else:\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "individual electrode grid plotting 5/19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_electrodes_per_subject_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rdms.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine LAB_root based on the operating system\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "LAB_root = os.path.join(HOME, \"Box\", \"CoganLab\") if os.name == 'nt' else os.path.join(HOME, \"Library\", \"CloudStorage\", \"Box-Box\", \"CoganLab\")\n",
    "save_dir = os.path.join(LAB_root, 'BIDS-1.1_GlobalLocal', 'BIDS', 'derivatives', 'RSA', 'figs')\n",
    "\n",
    "rois = ['lpfc', 'v1']\n",
    "\n",
    "# Define colors for the model names\n",
    "colors = {\n",
    "    'congruency': 'red',\n",
    "    'switchType': 'blue',\n",
    "    'congruencyProportion': 'pink',\n",
    "    'switchProportion': 'purple',\n",
    "    'congruency:congruencyProportion': (0.5,0,0.5),\n",
    "    'switchType:switchProportion': (0.6,1,0.6)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rsa_results_by_sub_and_elec is the result from calculate_rsa_per_electrode\n",
    "for roi in rois:\n",
    "    plot_rsa_correlation_by_electrode_grid(rsa_results_by_sub_and_elec, model_name='congruency', roi=roi, colors=colors, save_dir=save_dir)\n",
    "    plot_rsa_correlation_by_electrode_grid(rsa_results_by_sub_and_elec, model_name='switchType', roi=roi, colors=colors, save_dir=save_dir)\n",
    "    plot_rsa_correlation_by_electrode_grid(rsa_results_by_sub_and_elec, model_name='congruencyProportion', roi=roi, colors=colors, save_dir=save_dir)\n",
    "    plot_rsa_correlation_by_electrode_grid(rsa_results_by_sub_and_elec, model_name='switchProportion', roi=roi, colors=colors, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Plot the RSA result for the specified ROIs and model\n",
    "for roi in rois:\n",
    "    plot_rsa_correlation_by_time(rsa_results, 'congruency', roi, colors, save_dir=save_dir)\n",
    "    plot_rsa_correlation_by_time(rsa_results, 'switchType', roi, colors, save_dir=save_dir)\n",
    "    plot_rsa_correlation_by_time(rsa_results, 'congruencyProportion', roi, colors, save_dir=save_dir)\n",
    "    plot_rsa_correlation_by_time(rsa_results, 'switchProportion', roi, colors, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stats  \n",
    "1. for each roi, for each neural rdm (time window), compute correlation between a randomly shuffled model rdm and neural rdm. Do this a lot of times (10,000 for now).\n",
    "2. Find proportion of random correlations that are greater than the actual correlation between the model rdm and the neural rdm. That's the p-value.\n",
    "3. Find clusters in time for each permutation where p<0.05. For each cluster get a cluster statistic (sum of correlation values within the cluster). Do this for the real data too.\n",
    "4. Get the cluster statistic for the largest cluster in each of the fake data permutations. Make a distribution of this.\n",
    "5. For each cluster in the real data, find its p-value as the proportion of fake data clusters that have a cluster statistic >= the real data cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def permutation_test_rsa(neural_rdm, model_rdm, num_permutations=10000, threshold=0.05):\n",
    "#     \"\"\"\n",
    "#     Performs a permutation test for RSA with cluster-based correction.\n",
    "#     \"\"\"\n",
    "#     actual_correlation, _ = stats.spearmanr(neural_rdm.flatten(), model_rdm.flatten())\n",
    "    \n",
    "#     permuted_correlations = np.zeros(num_permutations)\n",
    "#     for i in range(num_permutations):\n",
    "#         shuffled_model_rdm = np.random.permutation(model_rdm.flatten())\n",
    "#         permuted_corr, _ = stats.spearmanr(neural_rdm.flatten(), shuffled_model_rdm)\n",
    "#         permuted_correlations[i] = permuted_corr\n",
    "\n",
    "#     clusters, num_clusters = label(actual_correlation > threshold)\n",
    "#     actual_cluster_size = np.sum(clusters > threshold)\n",
    "    \n",
    "#     permuted_cluster_sizes = np.zeros(num_permutations)\n",
    "#     for i in range(num_permutations):\n",
    "#         perm_clusters, num_perm_clusters = label(permuted_correlations[i] > threshold)\n",
    "#         if num_perm_clusters > 0:\n",
    "#             permuted_cluster_sizes[i] = max([np.sum(permuted_correlations[i][perm_clusters == cluster]) for cluster in range(1, num_perm_clusters + 1)])\n",
    "\n",
    "#     p_value = np.sum(permuted_cluster_sizes >= actual_cluster_size) / num_permutations\n",
    "    \n",
    "#     return actual_correlation, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now try to plot the average dissimilarity of condition pairs in the neural rdms over time\n",
    "E.g., congruent-congruent, incongruent-congruent and incongruent-congruent    \n",
    "\n",
    "this is untested and wrong. Need to index correctly. 5/18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is already defined for the model rdms but here it is again as a reminder. Delete once everything works. 5/18.\n",
    "conditions = [\"i25s25\", \"i25s75\", \"i75s25\", \"i75s75\", \"i25r25\", \"i25r75\", \"i75r25\", \"i75r75\",\n",
    "              \"c25s25\", \"c25s75\", \"c75s25\", \"c75s75\", \"c25r25\", \"c25r75\", \"c75r25\", \"c75r75\"]\n",
    "\n",
    "# Extract specific features from each condition\n",
    "congruency = [cond[0] for cond in conditions]\n",
    "switch_types = [cond[-3] for cond in conditions]\n",
    "congruency_proportion = [int(cond[1:3]) for cond in conditions]\n",
    "switch_proportion = [int(cond[-2:]) for cond in conditions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the average correlation for condition comparisons\n",
    "def compute_average_correlations(neural_rdm, condition_indices):\n",
    "    n = len(condition_indices)\n",
    "    avg_corr = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            avg_corr[i, j] = np.mean([neural_rdm[x, y] for x in condition_indices[i] for y in condition_indices[j]])\n",
    "            avg_corr[j, i] = avg_corr[i, j]  # Since RDM is symmetric\n",
    "    return avg_corr\n",
    "\n",
    "# Function to plot the average correlations over time\n",
    "def plot_neural_rdm_averages(rsa_results, model_name, rois, conditions, sampling_rate=2048.0, time_shift=1000):\n",
    "    condition_indices = {\n",
    "        \"congruency\": {\n",
    "            (0, 0): [i for i, c in enumerate(congruency) if c == 'c'],\n",
    "            (1, 1): [i for i, c in enumerate(congruency) if c == 'i'],\n",
    "            (0, 1): [i for i, c in enumerate(congruency) if c == 'c'],\n",
    "        },\n",
    "        \"switchType\": {\n",
    "            (0, 0): [i for i, s in enumerate(switch_types) if s == 'r'],\n",
    "            (1, 1): [i for i, s in enumerate(switch_types) if s == 's'],\n",
    "            (0, 1): [i for i, s in enumerate(switch_types) if s == 'r'],\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for roi in rois:\n",
    "        for subject in rsa_results[roi][model_name]:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            times = []\n",
    "            avg_correlations = {cond: [] for cond in condition_indices[model_name].keys()}\n",
    "            \n",
    "            for electrode in rsa_results[roi][model_name][subject]:\n",
    "                for window, neural_rdm, _ in rsa_results[roi][model_name][subject][electrode]:\n",
    "                    middle_sample = (window[0] + window[1]) / 2\n",
    "                    time_ms = (middle_sample * 1000 / sampling_rate) - time_shift\n",
    "                    if time_ms not in times:\n",
    "                        times.append(time_ms)\n",
    "                    avg_corr = compute_average_correlations(neural_rdm, condition_indices[model_name])\n",
    "                    for cond, indices in condition_indices[model_name].items():\n",
    "                        avg_correlations[cond].append(avg_corr[indices[0], indices[1]])\n",
    "\n",
    "            for cond, avg_corr in avg_correlations.items():\n",
    "                avg_corr = np.mean(avg_corr, axis=0)\n",
    "                ax.plot(times, avg_corr, label=f'{cond} Comparison')\n",
    "\n",
    "            ax.set_xlabel('Time from Stim Onset (ms)')\n",
    "            ax.set_ylabel('Average Spearman Corr. Coeff.')\n",
    "            ax.set_title(f'{model_name.capitalize()} Average Correlation over Time by Condition for ROI {roi} and Subject {subject}')\n",
    "            ax.legend()\n",
    "            ax.set_xlim(-1000, 1500)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
