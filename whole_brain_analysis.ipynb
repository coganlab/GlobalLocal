{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\jz421\\\\Desktop\\\\GlobalLocal', 'C:\\\\Users\\\\jz421\\\\Desktop\\\\GlobalLocal\\\\IEEG_Pipelines', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\python311.zip', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\DLLs', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg', '', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\win32', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jz421\\AppData\\Local\\anaconda3\\envs\\ieeg\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "sys.path.append(\"C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/\") #need to do this cuz otherwise ieeg isn't added to path...\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from ieeg.navigate import channel_outlier_marker, trial_ieeg, crop_empty_data, \\\n",
    "    outliers_to_nan\n",
    "from ieeg.io import raw_from_layout, get_data\n",
    "from ieeg.timefreq.utils import crop_pad\n",
    "from ieeg.timefreq import gamma\n",
    "from ieeg.calc.scaling import rescale\n",
    "import mne\n",
    "import os\n",
    "import numpy as np\n",
    "from ieeg.calc.reshape import make_data_same\n",
    "from ieeg.calc.stats import time_perm_cluster, window_averaged_shuffle\n",
    "from ieeg.viz.mri import gen_labels\n",
    "\n",
    "from misc_functions import calculate_RTs, save_channels_to_file, save_sig_chans, load_sig_chans, channel_names_to_indices, filter_and_average_epochs, permutation_test, perform_permutation_test_across_electrodes, perform_permutation_test_within_electrodes, add_accuracy_to_epochs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from collections import OrderedDict, defaultdict\n",
    "import json\n",
    "# still need to test if the permutation test functions load in properly.\n",
    "import pandas as pd\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MOVE ALL FUNCTIONS TO THE TOP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mne_objects(sub, output_name, task, LAB_root=None):\n",
    "    \"\"\"\n",
    "    Load MNE objects for a given subject and output name.\n",
    "\n",
    "    Parameters:\n",
    "    - sub (str): Subject identifier.\n",
    "    - output_name (str): Output name used in the file naming.\n",
    "    - task (str): Task identifier.\n",
    "    - LAB_root (str, optional): Root directory for the lab. If None, it will be determined based on the OS.\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing loaded MNE objects.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine LAB_root based on the operating system\n",
    "    if LAB_root is None:\n",
    "        HOME = os.path.expanduser(\"~\")\n",
    "        LAB_root = os.path.join(HOME, \"Box\", \"CoganLab\") if os.name == 'nt' else os.path.join(HOME, \"Library\", \"CloudStorage\", \"Box-Box\", \"CoganLab\")\n",
    "\n",
    "    # Get data layout\n",
    "    layout = get_data(task, root=LAB_root)\n",
    "    save_dir = os.path.join(layout.root, 'derivatives', 'freqFilt', 'figs', sub)\n",
    "    \n",
    "    # Ensure save directory exists\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Define file paths\n",
    "    HG_ev1_file = f'{save_dir}/{sub}_{output_name}_HG_ev1-epo.fif'\n",
    "    HG_base_file = f'{save_dir}/{sub}_{output_name}_HG_base-epo.fif'\n",
    "    HG_ev1_rescaled_file = f'{save_dir}/{sub}_{output_name}_HG_ev1_rescaled-epo.fif'\n",
    "\n",
    "    # Load the objects\n",
    "    HG_ev1 = mne.read_epochs(HG_ev1_file)\n",
    "    HG_base = mne.read_epochs(HG_base_file)\n",
    "    HG_ev1_rescaled = mne.read_epochs(HG_ev1_rescaled_file)\n",
    "    HG_ev1_evoke = HG_ev1.average(method=lambda x: np.nanmean(x, axis=0))\n",
    "    HG_ev1_evoke_rescaled = HG_ev1_rescaled.average(method=lambda x: np.nanmean(x, axis=0))\n",
    "\n",
    "    return {\n",
    "        'HG_ev1': HG_ev1,\n",
    "        'HG_base': HG_base,\n",
    "        'HG_ev1_rescaled': HG_ev1_rescaled,\n",
    "        'HG_ev1_evoke': HG_ev1_evoke,\n",
    "        'HG_ev1_evoke_rescaled': HG_ev1_evoke_rescaled\n",
    "    }\n",
    "\n",
    "#delete this once we know the import of this function works\n",
    "def add_accuracy_to_epochs(epochs, accuracy_array):\n",
    "    \"\"\"\n",
    "    Adds accuracy data from accuracy_array to the metadata of epochs.\n",
    "    Assumes the order of trials in accuracy_array matches the order in epochs.\n",
    "    \"\"\"\n",
    "    if epochs.metadata is None:\n",
    "        # Create a new DataFrame if no metadata exists\n",
    "        epochs.metadata = pd.DataFrame(index=range(len(epochs)))\n",
    "    \n",
    "    # Ensure the accuracy_array length matches the number of epochs\n",
    "    print('length of accuracy array:', len(accuracy_array))\n",
    "    print('length of epochs:', len(epochs))\n",
    "    assert len(accuracy_array) == len(epochs), \"Mismatch in number of trials and accuracy data length.\"\n",
    "    \n",
    "    # Add the accuracy array as a new column in the metadata\n",
    "    epochs.metadata['accuracy'] = accuracy_array\n",
    "\n",
    "    # Reset the index to ensure it's sequential starting from 0\n",
    "    epochs.metadata.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return epochs\n",
    "\n",
    "def create_subjects_mne_objects_dict(subjects, output_names_conditions, task, combined_data, acc_array, LAB_root=None):\n",
    "    \"\"\"\n",
    "    Adjusted to handle multiple conditions per output name, with multiple condition columns.\n",
    "\n",
    "    Parameters:\n",
    "    - subjects: List of subject IDs.\n",
    "    - output_names_conditions: Dictionary where keys are output names and values are dictionaries\n",
    "        of condition column names and their required values.\n",
    "    - task: Task identifier.\n",
    "    - combined_data: DataFrame with combined behavioral and trial information.\n",
    "    - acc_array: dict of numpy arrays of 0 for incorrect and 1 for correct trials for each subject\n",
    "    - LAB_root: Root directory for data (optional).\n",
    "    \"\"\"\n",
    "    subjects_mne_objects = {}\n",
    "\n",
    "    for sub in subjects:\n",
    "        print(f\"Loading data for subject: {sub}\")\n",
    "        sub_mne_objects = {}\n",
    "        for output_name, conditions in output_names_conditions.items():\n",
    "            print(f\"  Loading output: {output_name} with conditions: {conditions}\")\n",
    "            \n",
    "            # Build the filtering condition\n",
    "            sub_without_zeroes = \"D\" + sub[1:].lstrip('0') \n",
    "            condition_filter = (combined_data['subject_ID'] == sub) # this previously indexed using sub_without_zeroes, but now just uses sub. 3/17.\n",
    "                    \n",
    "            for condition_column, condition_value in conditions.items():\n",
    "                if isinstance(condition_value, list):\n",
    "                    # If the condition needs to match any value in a list\n",
    "                    condition_filter &= combined_data[condition_column].isin(condition_value)\n",
    "                else:\n",
    "                    # If the condition is a single value\n",
    "                    condition_filter &= (combined_data[condition_column] == condition_value)\n",
    "            \n",
    "            # Filter combinedData for the specific subject and conditions\n",
    "            subject_condition_data = combined_data[condition_filter]\n",
    "            \n",
    "            # Load MNE objects and update with accuracy data\n",
    "            mne_objects = load_mne_objects(sub, output_name, task, LAB_root)\n",
    "            \n",
    "            if sub in acc_array:\n",
    "                trial_counts = subject_condition_data['trialCount'].values.astype(int)\n",
    "                accuracy_data = [acc_array[sub][i-1] for i in trial_counts if i-1 < len(acc_array[sub])] # Subtract 1 here for zero-based indexing in acc array.\n",
    "                # Now pass trial_counts along with accuracy_data\n",
    "                mne_objects['HG_ev1_rescaled'] = add_accuracy_to_epochs(mne_objects['HG_ev1_rescaled'], accuracy_data)\n",
    "\n",
    "            sub_mne_objects[output_name] = mne_objects\n",
    "        subjects_mne_objects[sub] = sub_mne_objects\n",
    "\n",
    "    return subjects_mne_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the outer dictionary.\n",
    "subjects_electrodestoROIs_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make subjects rois to electrodes dict. Don't need to run this more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['D0057','D0059', 'D0063', 'D0065', 'D0069', 'D0071', 'D0077', 'D0090', 'D0094', 'D0100', 'D0102', 'D0103']\n",
    "# subjects = ['D0057','D0059', 'D0063', 'D0065', 'D0071', 'D0077', 'D0090', 'D0100', 'D0102', 'D0103']\n",
    "# subjects = ['D0103'] #testing cuz d0065 being weird\n",
    "\n",
    "for sub in subjects:\n",
    "    # sub = 'D0059'\n",
    "    task = 'GlobalLocal'\n",
    "    output_name = \"Response_fixationCrossBase_1sec_mirror\"\n",
    "    events = [\"Response\"]\n",
    "    times = (-1,1.5)\n",
    "    base_times = [-1,0]\n",
    "    LAB_root = None\n",
    "    channels = None\n",
    "    full_trial_base = False\n",
    "\n",
    "\n",
    "    if LAB_root is None:\n",
    "        HOME = os.path.expanduser(\"~\")\n",
    "        if os.name == 'nt':  # windows\n",
    "            LAB_root = os.path.join(HOME, \"Box\", \"CoganLab\")\n",
    "        else:  # mac\n",
    "            LAB_root = os.path.join(HOME, \"Library\", \"CloudStorage\", \"Box-Box\",\n",
    "                                    \"CoganLab\")\n",
    "\n",
    "    layout = get_data(task, root=LAB_root)\n",
    "    filt = raw_from_layout(layout.derivatives['derivatives/clean'], subject=sub,\n",
    "                        extension='.edf', desc='clean', preload=False)\n",
    "    save_dir = os.path.join(layout.root, 'derivatives', 'freqFilt', 'figs', sub)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    good = crop_empty_data(filt)\n",
    "    # %%\n",
    "\n",
    "    print(f\"good channels before dropping bads: {len(good.ch_names)}\")\n",
    "    print(f\"filt channels before dropping bads: {len(filt.ch_names)}\")\n",
    "\n",
    "    good.info['bads'] = channel_outlier_marker(good, 3, 2)\n",
    "    print(\"Bad channels in 'good':\", good.info['bads'])\n",
    "\n",
    "    filt.drop_channels(good.info['bads'])  # this has to come first cuz if you drop from good first, then good.info['bads'] is just empty\n",
    "    good.drop_channels(good.info['bads'])\n",
    "\n",
    "    print(\"Bad channels in 'good' after dropping once:\", good.info['bads'])\n",
    "\n",
    "    print(f\"good channels after dropping bads: {len(good.ch_names)}\")\n",
    "    print(f\"filt channels after dropping bads: {len(filt.ch_names)}\")\n",
    "\n",
    "    good.load_data()\n",
    "\n",
    "    # If channels is None, use all channels\n",
    "    if channels is None:\n",
    "        channels = good.ch_names\n",
    "    else:\n",
    "        # Validate the provided channels\n",
    "        invalid_channels = [ch for ch in channels if ch not in good.ch_names]\n",
    "        if invalid_channels:\n",
    "            raise ValueError(\n",
    "                f\"The following channels are not valid: {invalid_channels}\")\n",
    "\n",
    "        # Use only the specified channels\n",
    "        good.pick_channels(channels)\n",
    "\n",
    "    ch_type = filt.get_channel_types(only_data_chs=True)[0]\n",
    "    good.set_eeg_reference(ref_channels=\"average\", ch_type=ch_type)\n",
    "\n",
    "    default_dict = gen_labels(good.info)\n",
    "    \n",
    "    # Create rawROI_dict for the subject\n",
    "    rawROI_dict = defaultdict(list)\n",
    "    for key, value in default_dict.items():\n",
    "        rawROI_dict[value].append(key)\n",
    "    rawROI_dict = dict(rawROI_dict)\n",
    "\n",
    "    # Filter out keys containing \"White-Matter\"\n",
    "    filtROI_dict = {key: value for key, value in rawROI_dict.items() if \"White-Matter\" not in key}\n",
    "\n",
    "    # Store the dictionaries in the subjects dictionary\n",
    "    subjects_electrodestoROIs_dict[sub] = {\n",
    "        'default_dict': dict(default_dict),\n",
    "        'rawROI_dict': dict(rawROI_dict),\n",
    "        'filtROI_dict': dict(filtROI_dict)\n",
    "    }\n",
    "\n",
    "\n",
    "# # Save to a JSON file. Uncomment when actually running.\n",
    "filename = 'subjects_electrodestoROIs_dict.json'\n",
    "with open(filename, 'w') as file:\n",
    "    json.dump(subjects_electrodestoROIs_dict, file, indent=4)\n",
    "\n",
    "print(f\"Saved subjects_dict to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load subjects electrodes to rois dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data from subjects_electrodestoROIs_dict.json\n"
     ]
    }
   ],
   "source": [
    "# Load from a JSON file\n",
    "filename = 'subjects_electrodestoROIs_dict.json'\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    subjects_electrodestoROIs_dict = json.load(file)\n",
    "\n",
    "print(f\"Loaded data from {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load high gamma data so we can do roi analysis on it\n",
    "once we have more subjects, turn this into a function and loop over all subjects.  \n",
    "this code is a crime against humanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_c25_fixationCrossBase_1sec_mirror_HG_ev1-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "168 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_c25_fixationCrossBase_1sec_mirror_HG_base-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...       0.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_c25_fixationCrossBase_1sec_mirror_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "168 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_c75_fixationCrossBase_1sec_mirror_HG_ev1-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "56 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_c75_fixationCrossBase_1sec_mirror_HG_base-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...       0.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_c75_fixationCrossBase_1sec_mirror_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "56 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "# sub = 'D0057'\n",
    "# output_name = \"Stimulus_i25and75_fixationCrossBase_1sec_mirror\"\n",
    "# task = 'GlobalLocal'\n",
    "loaded_objects_D0057_i = load_mne_objects('D0057', \"Stimulus_c25_fixationCrossBase_1sec_mirror\", 'GlobalLocal')\n",
    "loaded_objects_D0057_c = load_mne_objects('D0057', \"Stimulus_c75_fixationCrossBase_1sec_mirror\", 'GlobalLocal')\n",
    "\n",
    "# Access the objects\n",
    "HG_ev1_D0057_i = loaded_objects_D0057_i['HG_ev1']\n",
    "HG_base_D0057_i = loaded_objects_D0057_i['HG_base']\n",
    "HG_ev1_rescaled_D0057_i = loaded_objects_D0057_i['HG_ev1_rescaled']\n",
    "HG_ev1_evoke_D0057_i = loaded_objects_D0057_i['HG_ev1_evoke']\n",
    "HG_ev1_evoke_rescaled_D0057_i = loaded_objects_D0057_i['HG_ev1_evoke_rescaled']\n",
    "\n",
    "HG_ev1_D0057_c = loaded_objects_D0057_c['HG_ev1']\n",
    "HG_base_D0057_c = loaded_objects_D0057_c['HG_base']\n",
    "HG_ev1_rescaled_D0057_c = loaded_objects_D0057_c['HG_ev1_rescaled']\n",
    "HG_ev1_evoke_D0057_c = loaded_objects_D0057_c['HG_ev1_evoke']\n",
    "HG_ev1_evoke_rescaled_D0057_c = loaded_objects_D0057_c['HG_ev1_evoke_rescaled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HG_ev1_rescaled_D0057_c.get_data().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load accuracy arrays so we can filter by only accurate trials  \n",
    "combine this code into add_accuracy_to_epochs later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where your .npy files are saved\n",
    "npy_directory = r'C:\\Users\\jz421\\Box\\CoganLab\\D_Data\\GlobalLocal\\accArrays'  # Replace with your directory path\n",
    "\n",
    "# Dictionary to hold the data\n",
    "acc_array = {}\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for file in os.listdir(npy_directory):\n",
    "    if file.endswith('.npy'):\n",
    "        subject_id = file.split('_')[0]  # Extract subject ID from the file name\n",
    "        if subject_id != 'D0107':  # Check if the subject ID is not D0107. Skip D0107 for now because it's not preprocessed yet 3/17.\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(npy_directory, file)\n",
    "            # Load the numpy array from the file\n",
    "            acc_array[subject_id] = np.load(file_path)\n",
    "\n",
    "# Now you have a dictionary where each key is the subject ID\n",
    "# and the value is the numpy array of accuracies for that subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.read_csv(r'C:\\Users\\jz421\\Box\\CoganLab\\D_Data\\GlobalLocal\\combinedData.csv')\n",
    "\n",
    "# Skip D0107 for now cuz it's not preprocessed yet 3/17.\n",
    "combined_data = combined_data[combined_data['subject_ID'] != 'D0107']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to map blockType to congruencyProportion and switchProportion\n",
    "def map_block_type(row):\n",
    "    if row['blockType'] == 'A':\n",
    "        return pd.Series(['25%', '25%'])\n",
    "    elif row['blockType'] == 'B':\n",
    "        return pd.Series(['25%', '75%'])\n",
    "    elif row['blockType'] == 'C':\n",
    "        return pd.Series(['75%', '25%'])\n",
    "    elif row['blockType'] == 'D':\n",
    "        return pd.Series(['75%', '75%'])\n",
    "    else:\n",
    "        return pd.Series([None, None])\n",
    "\n",
    "# Apply the function to each row and create new columns\n",
    "combined_data[['congruencyProportion', 'switchProportion']] = combined_data.apply(map_block_type, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load evoked and stuff for all subjects in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for subject: D0057\n",
      "  Loading output: Stimulus_ir_fixationCrossBase_1sec_mirror with conditions: {'congruency': 'i', 'switchType': 'r'}\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_ir_fixationCrossBase_1sec_mirror_HG_ev1-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "103 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_ir_fixationCrossBase_1sec_mirror_HG_base-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...       0.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_ir_fixationCrossBase_1sec_mirror_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "103 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 0 columns\n",
      "length of accuracy array: 103\n",
      "length of epochs: 103\n",
      "  Loading output: Stimulus_is_fixationCrossBase_1sec_mirror with conditions: {'congruency': 'i', 'switchType': 's'}\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_is_fixationCrossBase_1sec_mirror_HG_ev1-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "118 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_is_fixationCrossBase_1sec_mirror_HG_base-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...       0.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_is_fixationCrossBase_1sec_mirror_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "118 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 0 columns\n",
      "length of accuracy array: 118\n",
      "length of epochs: 118\n",
      "  Loading output: Stimulus_cr_fixationCrossBase_1sec_mirror with conditions: {'congruency': 'c', 'switchType': 'r'}\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_cr_fixationCrossBase_1sec_mirror_HG_ev1-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "121 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_cr_fixationCrossBase_1sec_mirror_HG_base-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...       0.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_cr_fixationCrossBase_1sec_mirror_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "121 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 0 columns\n",
      "length of accuracy array: 121\n",
      "length of epochs: 121\n",
      "  Loading output: Stimulus_cs_fixationCrossBase_1sec_mirror with conditions: {'congruency': 'c', 'switchType': 's'}\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_cs_fixationCrossBase_1sec_mirror_HG_ev1-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "102 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_cs_fixationCrossBase_1sec_mirror_HG_base-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...       0.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_cs_fixationCrossBase_1sec_mirror_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "102 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 0 columns\n",
      "length of accuracy array: 102\n",
      "length of epochs: 102\n",
      "Loading data for subject: D0059\n",
      "  Loading output: Stimulus_ir_fixationCrossBase_1sec_mirror with conditions: {'congruency': 'i', 'switchType': 'r'}\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0059\\D0059_Stimulus_ir_fixationCrossBase_1sec_mirror_HG_ev1-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "114 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0059\\D0059_Stimulus_ir_fixationCrossBase_1sec_mirror_HG_base-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...       0.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0059\\D0059_Stimulus_ir_fixationCrossBase_1sec_mirror_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "114 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 0 columns\n",
      "length of accuracy array: 114\n",
      "length of epochs: 114\n",
      "  Loading output: Stimulus_is_fixationCrossBase_1sec_mirror with conditions: {'congruency': 'i', 'switchType': 's'}\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0059\\D0059_Stimulus_is_fixationCrossBase_1sec_mirror_HG_ev1-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "107 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0059\\D0059_Stimulus_is_fixationCrossBase_1sec_mirror_HG_base-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...       0.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0059\\D0059_Stimulus_is_fixationCrossBase_1sec_mirror_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "107 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 0 columns\n",
      "length of accuracy array: 107\n",
      "length of epochs: 107\n",
      "  Loading output: Stimulus_cr_fixationCrossBase_1sec_mirror with conditions: {'congruency': 'c', 'switchType': 'r'}\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0059\\D0059_Stimulus_cr_fixationCrossBase_1sec_mirror_HG_ev1-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "110 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0059\\D0059_Stimulus_cr_fixationCrossBase_1sec_mirror_HG_base-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...       0.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0059\\D0059_Stimulus_cr_fixationCrossBase_1sec_mirror_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "110 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 0 columns\n",
      "length of accuracy array: 110\n",
      "length of epochs: 110\n",
      "  Loading output: Stimulus_cs_fixationCrossBase_1sec_mirror with conditions: {'congruency': 'c', 'switchType': 's'}\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0059\\D0059_Stimulus_cs_fixationCrossBase_1sec_mirror_HG_ev1-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "113 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0059\\D0059_Stimulus_cs_fixationCrossBase_1sec_mirror_HG_base-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...       0.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0059\\D0059_Stimulus_cs_fixationCrossBase_1sec_mirror_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "113 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 0 columns\n",
      "length of accuracy array: 113\n",
      "length of epochs: 113\n",
      "Loading data for subject: D0063\n",
      "  Loading output: Stimulus_ir_fixationCrossBase_1sec_mirror with conditions: {'congruency': 'i', 'switchType': 'r'}\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0063\\D0063_Stimulus_ir_fixationCrossBase_1sec_mirror_HG_ev1-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "106 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0063\\D0063_Stimulus_ir_fixationCrossBase_1sec_mirror_HG_base-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...       0.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0063\\D0063_Stimulus_ir_fixationCrossBase_1sec_mirror_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "106 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 0 columns\n",
      "length of accuracy array: 106\n",
      "length of epochs: 106\n",
      "  Loading output: Stimulus_is_fixationCrossBase_1sec_mirror with conditions: {'congruency': 'i', 'switchType': 's'}\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0063\\D0063_Stimulus_is_fixationCrossBase_1sec_mirror_HG_ev1-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "116 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0063\\D0063_Stimulus_is_fixationCrossBase_1sec_mirror_HG_base-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...       0.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0063\\D0063_Stimulus_is_fixationCrossBase_1sec_mirror_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "116 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 0 columns\n",
      "length of accuracy array: 116\n",
      "length of epochs: 116\n",
      "  Loading output: Stimulus_cr_fixationCrossBase_1sec_mirror with conditions: {'congruency': 'c', 'switchType': 'r'}\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0063\\D0063_Stimulus_cr_fixationCrossBase_1sec_mirror_HG_ev1-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "118 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0063\\D0063_Stimulus_cr_fixationCrossBase_1sec_mirror_HG_base-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...       0.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0063\\D0063_Stimulus_cr_fixationCrossBase_1sec_mirror_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "118 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 0 columns\n",
      "length of accuracy array: 118\n",
      "length of epochs: 118\n",
      "  Loading output: Stimulus_cs_fixationCrossBase_1sec_mirror with conditions: {'congruency': 'c', 'switchType': 's'}\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0063\\D0063_Stimulus_cs_fixationCrossBase_1sec_mirror_HG_ev1-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "104 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0063\\D0063_Stimulus_cs_fixationCrossBase_1sec_mirror_HG_base-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...       0.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0063\\D0063_Stimulus_cs_fixationCrossBase_1sec_mirror_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "104 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 0 columns\n",
      "length of accuracy array: 104\n",
      "length of epochs: 104\n",
      "Loading data for subject: D0065\n",
      "  Loading output: Stimulus_ir_fixationCrossBase_1sec_mirror with conditions: {'congruency': 'i', 'switchType': 'r'}\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0065\\D0065_Stimulus_ir_fixationCrossBase_1sec_mirror_HG_ev1-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "119 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0065\\D0065_Stimulus_ir_fixationCrossBase_1sec_mirror_HG_base-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...       0.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0065\\D0065_Stimulus_ir_fixationCrossBase_1sec_mirror_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "119 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 0 columns\n",
      "length of accuracy array: 119\n",
      "length of epochs: 119\n",
      "  Loading output: Stimulus_is_fixationCrossBase_1sec_mirror with conditions: {'congruency': 'i', 'switchType': 's'}\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0065\\D0065_Stimulus_is_fixationCrossBase_1sec_mirror_HG_ev1-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "103 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0065\\D0065_Stimulus_is_fixationCrossBase_1sec_mirror_HG_base-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...       0.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0065\\D0065_Stimulus_is_fixationCrossBase_1sec_mirror_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "103 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 0 columns\n",
      "length of accuracy array: 103\n",
      "length of epochs: 103\n",
      "  Loading output: Stimulus_cr_fixationCrossBase_1sec_mirror with conditions: {'congruency': 'c', 'switchType': 'r'}\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0065\\D0065_Stimulus_cr_fixationCrossBase_1sec_mirror_HG_ev1-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "105 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0065\\D0065_Stimulus_cr_fixationCrossBase_1sec_mirror_HG_base-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...       0.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0065\\D0065_Stimulus_cr_fixationCrossBase_1sec_mirror_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "105 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 0 columns\n",
      "length of accuracy array: 105\n",
      "length of epochs: 105\n",
      "  Loading output: Stimulus_cs_fixationCrossBase_1sec_mirror with conditions: {'congruency': 'c', 'switchType': 's'}\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0065\\D0065_Stimulus_cs_fixationCrossBase_1sec_mirror_HG_ev1-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "117 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0065\\D0065_Stimulus_cs_fixationCrossBase_1sec_mirror_HG_base-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...       0.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "448 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0065\\D0065_Stimulus_cs_fixationCrossBase_1sec_mirror_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "117 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 0 columns\n",
      "length of accuracy array: 117\n",
      "length of epochs: 117\n",
      "Loading data for subject: D0069\n",
      "  Loading output: Stimulus_ir_fixationCrossBase_1sec_mirror with conditions: {'congruency': 'i', 'switchType': 'r'}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File does not exist: \"C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0069\\D0069_Stimulus_ir_fixationCrossBase_1sec_mirror_HG_ev1-epo.fif\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 160\u001b[0m\n\u001b[0;32m    157\u001b[0m task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGlobalLocal\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# Assuming 'combined_data' is your DataFrame and 'subjects' is your list of subject IDs\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m subjects_mne_objects \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_subjects_mne_objects_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubjects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_names_conditions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGlobalLocal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43macc_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macc_array\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 108\u001b[0m, in \u001b[0;36mcreate_subjects_mne_objects_dict\u001b[1;34m(subjects, output_names_conditions, task, combined_data, acc_array, LAB_root)\u001b[0m\n\u001b[0;32m    105\u001b[0m subject_condition_data \u001b[38;5;241m=\u001b[39m combined_data[condition_filter]\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Load MNE objects and update with accuracy data\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m mne_objects \u001b[38;5;241m=\u001b[39m \u001b[43mload_mne_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43msub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLAB_root\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sub \u001b[38;5;129;01min\u001b[39;00m acc_array:\n\u001b[0;32m    111\u001b[0m     trial_counts \u001b[38;5;241m=\u001b[39m subject_condition_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrialCount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 34\u001b[0m, in \u001b[0;36mload_mne_objects\u001b[1;34m(sub, output_name, task, LAB_root)\u001b[0m\n\u001b[0;32m     31\u001b[0m HG_ev1_rescaled_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msub\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_HG_ev1_rescaled-epo.fif\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Load the objects\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m HG_ev1 \u001b[38;5;241m=\u001b[39m \u001b[43mmne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_epochs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHG_ev1_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m HG_base \u001b[38;5;241m=\u001b[39m mne\u001b[38;5;241m.\u001b[39mread_epochs(HG_base_file)\n\u001b[0;32m     36\u001b[0m HG_ev1_rescaled \u001b[38;5;241m=\u001b[39m mne\u001b[38;5;241m.\u001b[39mread_epochs(HG_ev1_rescaled_file)\n",
      "File \u001b[1;32m<decorator-gen-206>:12\u001b[0m, in \u001b[0;36mread_epochs\u001b[1;34m(fname, proj, preload, verbose)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jz421\\AppData\\Local\\anaconda3\\envs\\ieeg\\Lib\\site-packages\\mne\\epochs.py:3870\u001b[0m, in \u001b[0;36mread_epochs\u001b[1;34m(fname, proj, preload, verbose)\u001b[0m\n\u001b[0;32m   3852\u001b[0m \u001b[38;5;129m@verbose\u001b[39m\n\u001b[0;32m   3853\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_epochs\u001b[39m(fname, proj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, preload\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   3854\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read epochs from a fif file.\u001b[39;00m\n\u001b[0;32m   3855\u001b[0m \n\u001b[0;32m   3856\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3868\u001b[0m \u001b[38;5;124;03m        The epochs.\u001b[39;00m\n\u001b[0;32m   3869\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mEpochsFIF\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<decorator-gen-207>:12\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, fname, proj, preload, verbose)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jz421\\AppData\\Local\\anaconda3\\envs\\ieeg\\Lib\\site-packages\\mne\\epochs.py:3921\u001b[0m, in \u001b[0;36mEpochsFIF.__init__\u001b[1;34m(self, fname, proj, preload, verbose)\u001b[0m\n\u001b[0;32m   3915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _path_like(fname):\n\u001b[0;32m   3916\u001b[0m     check_fname(\n\u001b[0;32m   3917\u001b[0m         fname\u001b[38;5;241m=\u001b[39mfname,\n\u001b[0;32m   3918\u001b[0m         filetype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3919\u001b[0m         endings\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-epo.fif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-epo.fif.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_epo.fif\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_epo.fif.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   3920\u001b[0m     )\n\u001b[1;32m-> 3921\u001b[0m     fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43m_check_fname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmust_exist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mread\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   3922\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m preload:\n\u001b[0;32m   3923\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreload must be used with file-like objects\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m<decorator-gen-0>:12\u001b[0m, in \u001b[0;36m_check_fname\u001b[1;34m(fname, overwrite, must_exist, name, need_dir, verbose)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\jz421\\AppData\\Local\\anaconda3\\envs\\ieeg\\Lib\\site-packages\\mne\\utils\\check.py:263\u001b[0m, in \u001b[0;36m_check_fname\u001b[1;34m(fname, overwrite, must_exist, name, need_dir, verbose)\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mPermissionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not have read permissions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m must_exist:\n\u001b[1;32m--> 263\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fname\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File does not exist: \"C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0069\\D0069_Stimulus_ir_fixationCrossBase_1sec_mirror_HG_ev1-epo.fif\""
     ]
    }
   ],
   "source": [
    "# # example of how to use this with multiple conditions, even matching any value in a list. Although I only ever have two conditions of a type so not super necessary.\n",
    "# # make sure to use the correct column names and values that match with what combinedData uses.\n",
    "# output_names_conditions = {\n",
    "#     \"Stimulus_c25and75_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"congruency\": \"c\",\n",
    "#         \"switchType\": [\"s1\", \"s2\"]  # Example where switchType needs to match any value in the list\n",
    "#     },\n",
    "#     \"Stimulus_i25and75_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"congruency\": \"i\",\n",
    "#         \"switchType\": \"s\"\n",
    "#     }\n",
    "# }\n",
    "subjects = ['D0057', 'D0059', 'D0063', 'D0065', 'D0069', 'D0071', 'D0077', 'D0090', 'D0094', 'D0100', 'D0102', 'D0103']\n",
    "\n",
    "# congruency\n",
    "# output_names = [\"Stimulus_c25and75_fixationCrossBase_1sec_mirror\", \"Stimulus_i25and75_fixationCrossBase_1sec_mirror\"]\n",
    "# output_names_conditions = {\n",
    "#     \"Stimulus_c25and75_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"congruency\": \"c\",\n",
    "#     },\n",
    "#     \"Stimulus_i25and75_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"congruency\": \"i\",\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# switch\n",
    "# output_names = [\"Stimulus_r25and75_fixationCrossBase_1sec_mirror\", \"Stimulus_s25and75_fixationCrossBase_1sec_mirror\"]\n",
    "# output_names_conditions = {\n",
    "#     \"Stimulus_r25and75_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"switchType\": \"r\",\n",
    "#     },\n",
    "#     \"Stimulus_s25and75_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"switchType\": \"s\",\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# #  ir vs is\n",
    "# output_names = [\"Stimulus_ir_fixationCrossBase_1sec_mirror\", \"Stimulus_is_fixationCrossBase_1sec_mirror\"]\n",
    "# output_names_conditions = {\n",
    "#     \"Stimulus_ir_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"congruency\": \"i\",\n",
    "#         \"switchType\": \"r\"\n",
    "#     },\n",
    "#     \"Stimulus_is_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"congruency\": \"i\",\n",
    "#         \"switchType\": \"s\"\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# #  cr vs cs\n",
    "# output_names = [\"Stimulus_cr_fixationCrossBase_1sec_mirror\", \"Stimulus_cs_fixationCrossBase_1sec_mirror\"]\n",
    "# output_names_conditions = {\n",
    "#     \"Stimulus_cr_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"congruency\": \"c\",\n",
    "#         \"switchType\": \"r\"\n",
    "#     },\n",
    "#     \"Stimulus_cs_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"congruency\": \"c\",\n",
    "#         \"switchType\": \"s\"\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# #  is vs cs\n",
    "# output_names = [\"Stimulus_cs_fixationCrossBase_1sec_mirror\", \"Stimulus_is_fixationCrossBase_1sec_mirror\"]\n",
    "# output_names_conditions = {\n",
    "#     \"Stimulus_cs_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"congruency\": \"c\",\n",
    "#         \"switchType\": \"s\"\n",
    "#     },\n",
    "#     \"Stimulus_is_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"congruency\": \"i\",\n",
    "#         \"switchType\": \"s\"\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# #  ir vs cr\n",
    "# output_names = [\"Stimulus_cr_fixationCrossBase_1sec_mirror\", \"Stimulus_ir_fixationCrossBase_1sec_mirror\"]\n",
    "# output_names_conditions = {\n",
    "#     \"Stimulus_cr_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"congruency\": \"c\",\n",
    "#         \"switchType\": \"r\"\n",
    "#     },\n",
    "#     \"Stimulus_ir_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"congruency\": \"i\",\n",
    "#         \"switchType\": \"r\"\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # all interaction effects (run this with the anova code. Ugh make everything more modular later.)\n",
    "output_names = [\"Stimulus_ir_fixationCrossBase_1sec_mirror\", \"Stimulus_is_fixationCrossBase_1sec_mirror\", \"Stimulus_cr_fixationCrossBase_1sec_mirror\", \"Stimulus_cs_fixationCrossBase_1sec_mirror\"]\n",
    "\n",
    "output_names_conditions = {\n",
    "    \"Stimulus_ir_fixationCrossBase_1sec_mirror\": {\n",
    "        \"congruency\": \"i\",\n",
    "        \"switchType\": \"r\"\n",
    "    },\n",
    "    \"Stimulus_is_fixationCrossBase_1sec_mirror\": {\n",
    "        \"congruency\": \"i\",\n",
    "        \"switchType\": \"s\"\n",
    "    },\n",
    "    \"Stimulus_cr_fixationCrossBase_1sec_mirror\": {\n",
    "        \"congruency\": \"c\",\n",
    "        \"switchType\": \"r\"\n",
    "    },\n",
    "    \"Stimulus_cs_fixationCrossBase_1sec_mirror\": {\n",
    "        \"congruency\": \"c\",\n",
    "        \"switchType\": \"s\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# # block interaction contrasts for lwpc\n",
    "# output_names = [\"Stimulus_c25_fixationCrossBase_1sec_mirror\", \"Stimulus_c75_fixationCrossBase_1sec_mirror\",  \\\n",
    "#                 \"Stimulus_i25_fixationCrossBase_1sec_mirror\", \"Stimulus_i75_fixationCrossBase_1sec_mirror\"]\n",
    "\n",
    "# output_names_conditions = {\n",
    "#     \"Stimulus_c25_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"congruency\": \"c\",\n",
    "#         \"congruencyProportion\": \"75%\" #this is flipped because the BIDS events are saved in terms of incongruency proportion\n",
    "#     },\n",
    "#     \"Stimulus_c75_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"congruency\": \"c\",\n",
    "#         \"congruencyProportion\": \"25%\"\n",
    "#     },\n",
    "#     \"Stimulus_i25_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"congruency\": \"i\",\n",
    "#         \"congruencyProportion\": \"75%\"\n",
    "#     },\n",
    "#     \"Stimulus_i75_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"congruency\": \"i\",\n",
    "#         \"congruencyProportion\": \"25%\"\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# # block interaction contrasts for lwps\n",
    "# output_names = [\"Stimulus_s25_fixationCrossBase_1sec_mirror\", \"Stimulus_s75_fixationCrossBase_1sec_mirror\",  \\\n",
    "#                 \"Stimulus_r25_fixationCrossBase_1sec_mirror\", \"Stimulus_r75_fixationCrossBase_1sec_mirror\"]\n",
    "\n",
    "# output_names_conditions = {\n",
    "#     \"Stimulus_s25_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"switchType\": \"s\",\n",
    "#         \"switchProportion\": \"25%\"\n",
    "#     },\n",
    "#     \"Stimulus_s75_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"switchType\": \"s\",\n",
    "#         \"switchProportion\": \"75%\"\n",
    "#     },\n",
    "#     \"Stimulus_r25_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"switchType\": \"r\",\n",
    "#         \"switchProportion\": \"25%\"\n",
    "#     },\n",
    "#     \"Stimulus_r75_fixationCrossBase_1sec_mirror\": {\n",
    "#         \"switchType\": \"r\",\n",
    "#         \"switchProportion\": \"75%\"\n",
    "#     },\n",
    "# }\n",
    "\n",
    "task='GlobalLocal'\n",
    "\n",
    "# Assuming 'combined_data' is your DataFrame and 'subjects' is your list of subject IDs\n",
    "subjects_mne_objects = create_subjects_mne_objects_dict(subjects, output_names_conditions, task=\"GlobalLocal\", combined_data=combined_data, acc_array=acc_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load stimulus significant channels. Compare ROI electrodes in next cell to these to see if they're included.\n",
    "\n",
    "maybe do response significant channels too/instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded significant channels for subject D0057\n",
      "Loaded significant channels for subject D0059\n",
      "Loaded significant channels for subject D0063\n",
      "Loaded significant channels for subject D0065\n",
      "Loaded significant channels for subject D0069\n",
      "Loaded significant channels for subject D0071\n",
      "Loaded significant channels for subject D0077\n",
      "Loaded significant channels for subject D0090\n",
      "Loaded significant channels for subject D0094\n",
      "Loaded significant channels for subject D0100\n",
      "Loaded significant channels for subject D0102\n",
      "Loaded significant channels for subject D0103\n"
     ]
    }
   ],
   "source": [
    "def get_sig_chans(sub, task, LAB_root=None):\n",
    "    # Determine LAB_root based on the operating system\n",
    "    if LAB_root is None:\n",
    "        HOME = os.path.expanduser(\"~\")\n",
    "        LAB_root = os.path.join(HOME, \"Box\", \"CoganLab\") if os.name == 'nt' else os.path.join(HOME, \"Library\", \"CloudStorage\", \"Box-Box\", \"CoganLab\")\n",
    "\n",
    "    # Get data layout\n",
    "    layout = get_data(task, root=LAB_root)\n",
    "    save_dir = os.path.join(layout.root, 'derivatives', 'freqFilt', 'figs', sub)\n",
    "\n",
    "    stim_filename = f'{save_dir}\\\\sig_chans_{sub}_Stimulus_fixationCrossBase_1sec_mirror.json'\n",
    "    stim_sig_chans = load_sig_chans(stim_filename)\n",
    "    return stim_sig_chans\n",
    "\n",
    "\n",
    "# Initialize an empty dictionary to store significant channels per subject\n",
    "sig_chans_per_subject = {}\n",
    "\n",
    "# Populate the dictionary using get_sig_chans for each subject\n",
    "for sub in subjects:\n",
    "    sig_chans_per_subject[sub] = get_sig_chans(sub, 'GlobalLocal')\n",
    "\n",
    "# Now sig_chans_per_subject dictionary is populated with significant channels for each subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do time perm cluster stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to hold mappings\n",
    "overall_electrode_mapping = []\n",
    "\n",
    "# Initialize lists for storing data\n",
    "output_0_data_trialAvg_list = []\n",
    "output_1_data_trialAvg_list = []\n",
    "output_0_data_timeAvg_firstHalfSecond_list = []\n",
    "output_1_data_timeAvg_firstHalfSecond_list = []\n",
    "output_0_data_timeAvg_secondHalfSecond_list = []\n",
    "output_1_data_timeAvg_secondHalfSecond_list = []\n",
    "output_0_data_timeAvg_fullSecond_list = []\n",
    "output_1_data_timeAvg_fullSecond_list = []\n",
    "\n",
    "# Time windows\n",
    "start_idx_firstHalfSecond, end_idx_firstHalfSecond = 2048, 3072\n",
    "start_idx_secondHalfSecond, end_idx_secondHalfSecond = 3072, 4096\n",
    "start_idx_fullSecond, end_idx_fullSecond = 2048, 4096\n",
    "\n",
    "\n",
    "for sub in subjects:\n",
    "    sig_electrodes = sig_chans_per_subject.get(sub, [])\n",
    "\n",
    "    for electrode in sig_electrodes:\n",
    "        # For each significant electrode, append a tuple to the mapping list\n",
    "        # Tuple format: (Subject ID, Electrode Name, Index in List)\n",
    "        # The index can be the current length of the list before appending\n",
    "        index = len(overall_electrode_mapping)\n",
    "        overall_electrode_mapping.append((sub, electrode, index))  \n",
    "        \n",
    "    # Load trial-level data for the current condition and pick significant electrodes\n",
    "    output_0_epochs = subjects_mne_objects[sub][output_names[0]]['HG_ev1_rescaled'].copy().pick_channels(sig_electrodes)\n",
    "    output_1_epochs = subjects_mne_objects[sub][output_names[1]]['HG_ev1_rescaled'].copy().pick_channels(sig_electrodes)\n",
    "\n",
    "    # Calculate averages for each time window\n",
    "    trial_avg_0, trial_std_0, time_avg_0_firstHalfSecond = filter_and_average_epochs(output_0_epochs, start_idx_firstHalfSecond, end_idx_firstHalfSecond)\n",
    "    trial_avg_1, trial_std_1, time_avg_1_firstHalfSecond = filter_and_average_epochs(output_1_epochs, start_idx_firstHalfSecond, end_idx_firstHalfSecond)\n",
    "    _, _, time_avg_0_secondHalfSecond = filter_and_average_epochs(output_0_epochs, start_idx_secondHalfSecond, end_idx_secondHalfSecond)\n",
    "    _, _, time_avg_1_secondHalfSecond = filter_and_average_epochs(output_1_epochs, start_idx_secondHalfSecond, end_idx_secondHalfSecond)\n",
    "    _, _, time_avg_0_fullSecond = filter_and_average_epochs(output_0_epochs, start_idx_fullSecond, end_idx_fullSecond)\n",
    "    _, _, time_avg_1_fullSecond = filter_and_average_epochs(output_1_epochs, start_idx_fullSecond, end_idx_fullSecond)\n",
    "\n",
    "    # Append the results to their respective lists\n",
    "    output_0_data_trialAvg_list.append(trial_avg_0)\n",
    "    output_1_data_trialAvg_list.append(trial_avg_1)\n",
    "    output_0_data_timeAvg_firstHalfSecond_list.append(time_avg_0_firstHalfSecond)\n",
    "    output_1_data_timeAvg_firstHalfSecond_list.append(time_avg_1_firstHalfSecond)\n",
    "    output_0_data_timeAvg_secondHalfSecond_list.append(time_avg_0_secondHalfSecond)\n",
    "    output_1_data_timeAvg_secondHalfSecond_list.append(time_avg_1_secondHalfSecond)\n",
    "    output_0_data_timeAvg_fullSecond_list.append(time_avg_0_fullSecond)\n",
    "    output_1_data_timeAvg_fullSecond_list.append(time_avg_1_fullSecond)\n",
    "\n",
    "# After collecting all data, concatenate across subjects for each condition\n",
    "concatenated_trialAvg_data = {}\n",
    "concatenated_timeAvg_firstHalfSecond_data = {}\n",
    "concatenated_timeAvg_secondHalfSecond_data = {}\n",
    "concatenated_timeAvg_fullSecond_data = {}\n",
    "\n",
    "\n",
    "concatenated_trialAvg_data = {\n",
    "    'output_0': np.concatenate(output_0_data_trialAvg_list, axis=0),\n",
    "    'output_1': np.concatenate(output_1_data_trialAvg_list, axis=0)\n",
    "}\n",
    "\n",
    "# Calculate mean and SEM across electrodes for all time windows and rois\n",
    "overall_averages = {}\n",
    "overall_sems = {}\n",
    "\n",
    "for output in ['output_0', 'output_1']:\n",
    "    trialAvg_data = concatenated_trialAvg_data[output]\n",
    "    overall_averages[output] = np.nanmean(trialAvg_data, axis=0)\n",
    "    overall_sems[output] = np.std(trialAvg_data, axis=0, ddof=1) / np.sqrt(trialAvg_data.shape[0])\n",
    "\n",
    "time_perm_cluster_results = time_perm_cluster(\n",
    "    concatenated_trialAvg_data['output_0'],\n",
    "    concatenated_trialAvg_data['output_1'], 0.05, n_jobs=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do window stats  \n",
    "use the time avg outputs from previous cell  \n",
    "use fdr correction after comparing output 0 and output 1 for each electrode to get a p-values list  \n",
    "\n",
    "DO A SHUFFLE INSTEAD OF PAIRED T-TEST AS OF 2/7/24\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shuffle test (perm test). This basically time perm cluster but avg across time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do perm testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the functions perform_permutation_test_within_electrodes and perform_permutation_test_across_electrodes return lists of p-values (and get loaded in properly)\n",
    "p_values = {}\n",
    "# rois = ['dlpfc', 'acc', 'parietal']\n",
    "# for roi in rois:\n",
    "# Initialize p_values[roi] as a dictionary. Initialize dicts for all time windows.\n",
    "p_values = {}\n",
    "p_values['firstHalfSecond'] = {}\n",
    "p_values['secondHalfSecond'] = {}\n",
    "p_values['fullSecond'] = {}\n",
    "\n",
    "# Perform the tests and store results\n",
    "p_values['firstHalfSecond']['within'] = perform_permutation_test_within_electrodes(output_0_data_timeAvg_firstHalfSecond_list, output_1_data_timeAvg_firstHalfSecond_list, n_permutations=10000)\n",
    "p_values['firstHalfSecond']['across'] = perform_permutation_test_across_electrodes(output_0_data_timeAvg_firstHalfSecond_list, output_1_data_timeAvg_firstHalfSecond_list, n_permutations=10000)\n",
    "\n",
    "p_values['secondHalfSecond']['within'] = perform_permutation_test_within_electrodes(output_0_data_timeAvg_secondHalfSecond_list, output_1_data_timeAvg_secondHalfSecond_list, n_permutations=10000)\n",
    "p_values['secondHalfSecond']['across'] = perform_permutation_test_across_electrodes(output_0_data_timeAvg_secondHalfSecond_list, output_1_data_timeAvg_secondHalfSecond_list, n_permutations=10000)\n",
    "\n",
    "p_values['fullSecond']['within'] = perform_permutation_test_within_electrodes(output_0_data_timeAvg_fullSecond_list, output_1_data_timeAvg_fullSecond_list, n_permutations=10000)\n",
    "p_values['fullSecond']['across'] = perform_permutation_test_across_electrodes(output_0_data_timeAvg_fullSecond_list, output_1_data_timeAvg_fullSecond_list, n_permutations=10000)\n",
    "\n",
    "all_p_values = {}\n",
    "all_p_values['firstHalfSecond'] = []\n",
    "all_p_values['secondHalfSecond'] = []\n",
    "all_p_values['fullSecond'] = []\n",
    "\n",
    "# the very last p-value is the across electrodes p-value, all other p-values are within electrode\n",
    "for test_type in p_values['firstHalfSecond']:\n",
    "    p = p_values['firstHalfSecond'][test_type]\n",
    "    if isinstance(p, list):\n",
    "        all_p_values['firstHalfSecond'].extend(p)\n",
    "    else:  # Assume it's a single float value\n",
    "        all_p_values['firstHalfSecond'].append(p)\n",
    "\n",
    "for test_type in p_values['secondHalfSecond']:\n",
    "    p = p_values['secondHalfSecond'][test_type]\n",
    "    if isinstance(p, list):\n",
    "        all_p_values['secondHalfSecond'].extend(p)\n",
    "    else:  # Assume it's a single float value\n",
    "        all_p_values['secondHalfSecond'].append(p)\n",
    "\n",
    "for test_type in p_values[roi]['fullSecond']:\n",
    "    p = p_values['fullSecond'][test_type]\n",
    "    if isinstance(p, list):\n",
    "        all_p_values['fullSecond'].extend(p)\n",
    "    else:  # Assume it's a single float value\n",
    "        all_p_values['fullSecond'].append(p)\n",
    "\n",
    "# Apply FDR correction\n",
    "_, adjusted_p_values_firstHalfSecond = multipletests(all_p_values['firstHalfSecond'], alpha=0.05, method='fdr_bh')[:2]\n",
    "_, adjusted_p_values_secondHalfSecond = multipletestsload(all_p_values['secondHalfSecond'], alpha=0.05, method='fdr_bh')[:2]\n",
    "_, adjusted_p_values_fullSecond = multipletests(all_p_values['fullSecond'], alpha=0.05, method='fdr_bh')[:2]\n",
    "\n",
    "# Incorporating adjusted p-values back into the structure is a bit more complex and depends on how you want to use them next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values['parietal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values['acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values['parietal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "integrate adjusted p values back in the p values dict  \n",
    "oh i think the index map fixes the fact that the across just gets added to the end of the within p values in the all p values list. But that's still confusing. 3/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Build an index map while aggregating p-values\n",
    "index_map = {'firstHalfSecond': [], 'secondHalfSecond': [], 'fullSecond': []}\n",
    "\n",
    "# Step 1: Adjusted - Ensure all p-values are treated as lists\n",
    "for time_window in ['firstHalfSecond', 'secondHalfSecond', 'fullSecond']:\n",
    "    for test_type in ['within', 'across']:\n",
    "        p_value_list = p_values[time_window][test_type]\n",
    "        # Ensure p_value_list is actually a list\n",
    "        if not isinstance(p_value_list, list):\n",
    "            p_value_list = [p_value_list]\n",
    "        for p_value in p_value_list:\n",
    "            all_p_values[time_window].append(p_value)\n",
    "            index_map[time_window].append((test_type))\n",
    "\n",
    "\n",
    "# Step 3: Reintegrate adjusted p-values back into the p_values structure\n",
    "# Using firstHalfSecond as an example\n",
    "# Adjusted reintegration example for firstHalfSecond\n",
    "for time_window in ['firstHalfSecond', 'secondHalfSecond', 'fullSecond']:\n",
    "    adjusted_ps = locals()[f\"adjusted_p_values_{time_window}\"]  # Retrieve adjusted p-values using dynamic variable names\n",
    "    for i, adjusted_p in enumerate(adjusted_ps):\n",
    "        test_type = index_map[time_window][i]\n",
    "        # Ensure the adjusted key and test_type key exist\n",
    "        if 'adjusted' not in p_values[time_window]:\n",
    "            p_values[time_window]['adjusted'] = {}\n",
    "        if test_type not in p_values[time_window]['adjusted']:\n",
    "            p_values[time_window]['adjusted'][test_type] = []\n",
    "        p_values[time_window]['adjusted'][test_type].append(adjusted_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values['acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmm figure out if need to run this always and how its different than the other one.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### do 2x2 anova for interaction effects \n",
    "this requires reloading in all four conditions (four this time cuz interaction contrasts).  \n",
    "ONLY RUN THIS WHEN LOADING IN THE FOUR INTERACTION CONTRASTS RIGHT NOW.  \n",
    "Integrate with other stats and plotting and stuff later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jz421\\Desktop\\GlobalLocal\\misc_functions.py:113: RuntimeWarning: Mean of empty slice\n",
      "  time_avg_data = np.nanmean(all_epochs_data[:, :, start_idx:end_idx], axis=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to hold mappings for all electrodes across all subjects\n",
    "overall_electrode_mapping = []\n",
    "\n",
    "# Initialize lists for storing data for each output condition and time window\n",
    "output_data_trialAvg_lists = {output_name: [] for output_name in output_names}\n",
    "output_data_trialStd_lists = {output_name: [] for output_name in output_names}\n",
    "output_data_timeAvg_firstHalfSecond_lists = {output_name: [] for output_name in output_names}\n",
    "output_data_timeAvg_secondHalfSecond_lists = {output_name: [] for output_name in output_names}\n",
    "output_data_timeAvg_fullSecond_lists = {output_name: [] for output_name in output_names}\n",
    "\n",
    "# Time windows\n",
    "start_idx_firstHalfSecond, end_idx_firstHalfSecond = 2048, 3072\n",
    "start_idx_secondHalfSecond, end_idx_secondHalfSecond = 3072, 4096\n",
    "start_idx_fullSecond, end_idx_fullSecond = 2048, 4096\n",
    "\n",
    "# Iterate over each subject\n",
    "for sub in subjects:\n",
    "    sig_electrodes = sig_chans_per_subject.get(sub, [])  # Get significant electrodes for the current subject\n",
    "\n",
    "    # Map significant electrodes\n",
    "    for electrode in sig_electrodes:\n",
    "        # Append a tuple to the mapping list (Subject ID, Electrode Name, Index in List)\n",
    "        index = len(overall_electrode_mapping)\n",
    "        overall_electrode_mapping.append((sub, electrode, index))\n",
    "\n",
    "    # Process data for each output condition\n",
    "    for output_idx, output_name in enumerate(output_names):\n",
    "        # Load trial-level data for the current condition and pick significant electrodes\n",
    "        epochs = subjects_mne_objects[sub][output_name]['HG_ev1_rescaled'].copy().pick_channels(sig_electrodes)\n",
    "\n",
    "        # Calculate averages for each time window\n",
    "        trial_avg, trial_std, time_avg_firstHalfSecond = filter_and_average_epochs(epochs, start_idx_firstHalfSecond, end_idx_firstHalfSecond)\n",
    "        _, _, time_avg_secondHalfSecond = filter_and_average_epochs(epochs, start_idx_secondHalfSecond, end_idx_secondHalfSecond)\n",
    "        _, _, time_avg_fullSecond = filter_and_average_epochs(epochs, start_idx_fullSecond, end_idx_fullSecond)\n",
    "\n",
    "        # Store the results\n",
    "        output_data_trialAvg_lists[output_name].append(trial_avg)\n",
    "        output_data_trialStd_lists[output_name].append(trial_std)\n",
    "        output_data_timeAvg_firstHalfSecond_lists[output_name].append(time_avg_firstHalfSecond)\n",
    "        output_data_timeAvg_secondHalfSecond_lists[output_name].append(time_avg_secondHalfSecond)\n",
    "        output_data_timeAvg_fullSecond_lists[output_name].append(time_avg_fullSecond)\n",
    "\n",
    "# After collecting all data, concatenate across subjects for each roi and condition\n",
    "concatenated_trialAvg_data = {}\n",
    "concatenated_trialStd_data = {}\n",
    "\n",
    "for output_name in output_names:\n",
    "    concatenated_trialAvg_data[output_name] = np.concatenate(output_data_trialAvg_lists[output_name], axis=0)\n",
    "    concatenated_trialStd_data[output_name] = np.concatenate(output_data_trialStd_lists[output_name], axis=0)\n",
    "\n",
    "# Calculate mean and SEM across electrodes for all time windows and rois\n",
    "overall_averages = {}\n",
    "overall_sems = {}\n",
    "\n",
    "for output_name in output_names:\n",
    "    trialAvg_data = concatenated_trialAvg_data[output_name]\n",
    "    overall_averages[output_name] = np.nanmean(trialAvg_data, axis=0)\n",
    "    overall_sems[output_name] = np.std(trialAvg_data, axis=0, ddof=1) / np.sqrt(trialAvg_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D0057\n",
      "D0059\n",
      "D0063\n",
      "D0065\n",
      "D0069\n",
      "D0071\n",
      "D0077\n",
      "D0090\n",
      "D0094\n",
      "D0100\n",
      "D0102\n",
      "D0103\n",
      "D0057\n",
      "D0059\n",
      "D0063\n",
      "D0065\n",
      "D0069\n",
      "D0071\n",
      "D0077\n",
      "D0090\n",
      "D0094\n",
      "D0100\n",
      "D0102\n",
      "D0103\n",
      "D0057\n",
      "D0059\n",
      "D0063\n",
      "D0065\n",
      "D0069\n",
      "D0071\n",
      "D0077\n",
      "D0090\n",
      "D0094\n",
      "D0100\n",
      "D0102\n",
      "D0103\n",
      "D0057\n",
      "D0059\n",
      "D0063\n",
      "D0065\n",
      "D0069\n",
      "D0071\n",
      "D0077\n",
      "D0090\n",
      "D0094\n",
      "D0100\n",
      "D0102\n",
      "D0103\n",
      "D0057\n",
      "D0059\n",
      "D0063\n",
      "D0065\n",
      "D0069\n",
      "D0071\n",
      "D0077\n",
      "D0090\n",
      "D0094\n",
      "D0100\n",
      "D0102\n",
      "D0103\n",
      "D0057\n",
      "D0059\n",
      "D0063\n",
      "D0065\n",
      "D0069\n",
      "D0071\n",
      "D0077\n",
      "D0090\n",
      "D0094\n",
      "D0100\n",
      "D0102\n",
      "D0103\n",
      "D0057\n",
      "D0059\n",
      "D0063\n",
      "D0065\n",
      "D0069\n",
      "D0071\n",
      "D0077\n",
      "D0090\n",
      "D0094\n",
      "D0100\n",
      "D0102\n",
      "D0103\n",
      "D0057\n",
      "D0059\n",
      "D0063\n",
      "D0065\n",
      "D0069\n",
      "D0071\n",
      "D0077\n",
      "D0090\n",
      "D0094\n",
      "D0100\n",
      "D0102\n",
      "D0103\n",
      "D0057\n",
      "D0059\n",
      "D0063\n",
      "D0065\n",
      "D0069\n",
      "D0071\n",
      "D0077\n",
      "D0090\n",
      "D0094\n",
      "D0100\n",
      "D0102\n",
      "D0103\n",
      "D0057\n",
      "D0059\n",
      "D0063\n",
      "D0065\n",
      "D0069\n",
      "D0071\n",
      "D0077\n",
      "D0090\n",
      "D0094\n",
      "D0100\n",
      "D0102\n",
      "D0103\n",
      "D0057\n",
      "D0059\n",
      "D0063\n",
      "D0065\n",
      "D0069\n",
      "D0071\n",
      "D0077\n",
      "D0090\n",
      "D0094\n",
      "D0100\n",
      "D0102\n",
      "D0103\n",
      "D0057\n",
      "D0059\n",
      "D0063\n",
      "D0065\n",
      "D0069\n",
      "D0071\n",
      "D0077\n",
      "D0090\n",
      "D0094\n",
      "D0100\n",
      "D0102\n",
      "D0103\n"
     ]
    }
   ],
   "source": [
    "LAB_root = None\n",
    "# Determine LAB_root based on the operating system\n",
    "if LAB_root is None:\n",
    "    HOME = os.path.expanduser(\"~\")\n",
    "    LAB_root = os.path.join(HOME, \"Box\", \"CoganLab\") if os.name == 'nt' else os.path.join(HOME, \"Library\", \"CloudStorage\", \"Box-Box\", \"CoganLab\")\n",
    "\n",
    "# Get data layout\n",
    "layout = get_data(task, root=LAB_root)\n",
    "save_dir = os.path.join(layout.root, 'derivatives', 'freqFilt', 'figs')\n",
    "\n",
    "# Example structure for organizing data for ANOVA with four conditions\n",
    "data_for_anova = []\n",
    "\n",
    "# Function to process and append data for ANOVA from time-averaged lists\n",
    "# Adapted function to include Congruency and SwitchType\n",
    "def process_and_append_data_for_anova_whole_brain_analysis(time_averaged_lists, time_window):\n",
    "    for output_name in output_names:\n",
    "        # Dynamically get condition types and their values for the current output_name\n",
    "        conditions = output_names_conditions[output_name]\n",
    "        \n",
    "        for subject_index, subject_data in enumerate(time_averaged_lists[output_name]):\n",
    "            subject_id = subjects[subject_index]\n",
    "            print(subject_id)\n",
    "\n",
    "            # Skip this subject if there are no significant electrodes for them in this ROI\n",
    "            if subject_id not in sig_chans_per_subject or not sig_chans_per_subject[subject_id]:\n",
    "                continue\n",
    "\n",
    "            # Calculate the mean across trials for each electrode\n",
    "            mean_activity_per_electrode = np.nanmean(subject_data, axis=0)\n",
    "            # untested making this more modular 2/27\n",
    "            for electrode_index, mean_activity in enumerate(mean_activity_per_electrode):\n",
    "                electrode_name = sig_chans_per_subject[subject_id][electrode_index]\n",
    "\n",
    "                # Prepare data dictionary, starting with fixed attributes\n",
    "                data_dict = {\n",
    "                    'SubjectID': subject_id,\n",
    "                    'Electrode': electrode_name,\n",
    "                    'TimeWindow': time_window,\n",
    "                    'MeanActivity': mean_activity\n",
    "                }\n",
    "\n",
    "                # Dynamically add condition types and their values\n",
    "                data_dict.update(conditions)\n",
    "\n",
    "                # Append the organized data to the list\n",
    "                data_for_anova.append(data_dict)\n",
    "\n",
    "# Invoke the function for each time-averaged list\n",
    "process_and_append_data_for_anova_whole_brain_analysis(output_data_timeAvg_firstHalfSecond_lists, \"FirstHalfSecond\")\n",
    "process_and_append_data_for_anova_whole_brain_analysis(output_data_timeAvg_secondHalfSecond_lists, \"SecondHalfSecond\")\n",
    "process_and_append_data_for_anova_whole_brain_analysis(output_data_timeAvg_fullSecond_lists, \"FullSecond\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_for_anova = pd.DataFrame(data_for_anova)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now actually run anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataframe_to_serializable_format(df):\n",
    "    \"\"\"\n",
    "    Convert a pandas DataFrame to a serializable format that can be used with json.dump.\n",
    "    \"\"\"\n",
    "    return df.to_dict(orient='records')\n",
    "\n",
    "# move this to misc functions 3/20!\n",
    "def perform_modular_anova(df, time_window, save_dir, save_name):\n",
    "    # Filter for a specific time window (I should probably make this not have a time_window input and just loop over all time windows like the within electrode code does)\n",
    "    df_filtered = df[df['TimeWindow'] == time_window]\n",
    "\n",
    "    # Dynamically construct the model formula based on condition keys\n",
    "    condition_keys = [key for key in output_names_conditions[next(iter(output_names_conditions))].keys()]\n",
    "    formula_terms = ' + '.join([f'C({key})' for key in condition_keys])\n",
    "    interaction_terms = ' * '.join([f'C({key})' for key in condition_keys])\n",
    "    formula = f'MeanActivity ~ {formula_terms} + {interaction_terms}'\n",
    "\n",
    "    # Define the model\n",
    "    model = ols(formula, data=df_filtered).fit()\n",
    "\n",
    "    # Perform the ANOVA\n",
    "    anova_results = anova_lm(model, typ=2)\n",
    "\n",
    "    # Define the full path for the results file\n",
    "    results_file_path = os.path.join(save_dir, save_name)\n",
    "\n",
    "    # Save the ANOVA results to a text file\n",
    "    with open(results_file_path, 'w') as file:\n",
    "        file.write(anova_results.__str__())\n",
    "\n",
    "    # Optionally, print the path to the saved file and/or return it\n",
    "    print(f\"ANOVA results saved to: {results_file_path}\")\n",
    "\n",
    "    # Print the results\n",
    "    print(anova_results)\n",
    "\n",
    "    return anova_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "congruency as function of congruency proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA results saved to: C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\firstHalfSecond_congruency_congruencyProportion_ANOVAacrossElectrodes_wholeBrainAnalysis.txt\n",
      "                                           sum_sq      df         F    PR(>F)\n",
      "C(congruency)                            0.453277     1.0  8.976509  0.002755\n",
      "C(congruencyProportion)                  0.229585     1.0  4.546612  0.033056\n",
      "C(congruency):C(congruencyProportion)    0.087015     1.0  1.723201  0.189371\n",
      "Residual                               169.262335  3352.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "# Assuming output_names is a list of strings that includes the names of the outputs you're working with\n",
    "if 'Stimulus_c25_fixationCrossBase_1sec_mirror' in output_names:\n",
    "    perform_modular_anova(df_for_anova, \"FirstHalfSecond\", save_dir, 'firstHalfSecond_congruency_congruencyProportion_ANOVAacrossElectrodes_wholeBrainAnalysis.txt')\n",
    "else:\n",
    "    print(\"The required output name 'Stimulus_c25_fixationCrossBase_1sec_mirror' is not in output_names.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA results saved to: C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\secondHalfSecond_congruency_congruencyProportion_ANOVAacrossElectrodes_wholeBrainAnalysis.txt\n",
      "                                           sum_sq      df         F    PR(>F)\n",
      "C(congruency)                            0.015082     1.0  0.261866  0.608875\n",
      "C(congruencyProportion)                  0.260262     1.0  4.518999  0.033593\n",
      "C(congruency):C(congruencyProportion)    0.000724     1.0  0.012579  0.910706\n",
      "Residual                               193.051523  3352.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "if 'Stimulus_c25_fixationCrossBase_1sec_mirror' in output_names:\n",
    "    perform_modular_anova(df_for_anova, \"SecondHalfSecond\", save_dir, 'secondHalfSecond_congruency_congruencyProportion_ANOVAacrossElectrodes_wholeBrainAnalysis.txt')\n",
    "else:\n",
    "    print(\"The required output name 'Stimulus_c25_fixationCrossBase_1sec_mirror' is not in output_names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANOVA results saved to: C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\fullSecond_congruency_congruencyProportion_ANOVAacrossElectrodes_wholeBrainAnalysis.txt\n",
      "                                           sum_sq      df         F    PR(>F)\n",
      "C(congruency)                            0.158430     1.0  3.639486  0.056510\n",
      "C(congruencyProportion)                  0.244684     1.0  5.620910  0.017804\n",
      "C(congruency):C(congruencyProportion)    0.017965     1.0  0.412693  0.520650\n",
      "Residual                               145.915708  3352.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "if 'Stimulus_c25_fixationCrossBase_1sec_mirror' in output_names:\n",
    "    perform_modular_anova(df_for_anova, \"FullSecond\", save_dir, 'fullSecond_congruency_congruencyProportion_ANOVAacrossElectrodes_wholeBrainAnalysis.txt')\n",
    "else:\n",
    "    print(\"The required output name 'Stimulus_c25_fixationCrossBase_1sec_mirror' is not in output_names.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "switch type and switch proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Stimulus_s25_fixationCrossBase_1sec_mirror' in output_names:\n",
    "    perform_modular_anova(df_for_anova, \"FirstHalfSecond\", save_dir, 'firstHalfSecond_switchType_switchProportion_ANOVAacrossElectrodes_wholeBrainAnalysis.txt')\n",
    "else:\n",
    "    print(\"The required output name 'Stimulus_s25_fixationCrossBase_1sec_mirror' is not in output_names.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "switch type as function of switch proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Stimulus_s25_fixationCrossBase_1sec_mirror' in output_names:\n",
    "    perform_modular_anova(df_for_anova, \"SecondHalfSecond\", save_dir, 'secondHalfSecond_switchType_switchProportion_ANOVAacrossElectrodes_wholeBrainAnalysis.txt')\n",
    "else:\n",
    "    print(\"The required output name 'Stimulus_s25_fixationCrossBase_1sec_mirror' is not in output_names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Stimulus_s25_fixationCrossBase_1sec_mirror' in output_names:\n",
    "    perform_modular_anova(df_for_anova, \"FullSecond\", save_dir, 'fullSecond_switchType_switchProportion_ANOVAacrossElectrodes_wholeBrainAnalysis.txt')\n",
    "else:\n",
    "    print(\"The required output name 'Stimulus_s25_fixationCrossBase_1sec_mirror' is not in output_names.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "congruency as function of switch type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Stimulus_cr_fixationCrossBase_1sec_mirror' in output_names:\n",
    "    perform_modular_anova(df_for_anova, \"FirstHalfSecond\", save_dir, 'firstHalfSecond_congruency_congruencyProportion_ANOVAacrossElectrodes_wholeBrainAnalysis.txt')\n",
    "else:\n",
    "    print(\"The required output name 'Stimulus_cr_fixationCrossBase_1sec_mirror' is not in output_names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Stimulus_cr_fixationCrossBase_1sec_mirror' in output_names:\n",
    "    perform_modular_anova(df_for_anova, \"SecondHalfSecond\", save_dir, 'secondHalfSecond_congruency_congruencyProportion_ANOVAacrossElectrodes_wholeBrainAnalysis.txt')\n",
    "else:\n",
    "    print(\"The required output name 'Stimulus_cr_fixationCrossBase_1sec_mirror' is not in output_names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Stimulus_cr_fixationCrossBase_1sec_mirror' in output_names:\n",
    "    perform_modular_anova(df_for_anova, \"FullSecond\", save_dir, 'fullSecond_congruency_congruencyProportion_ANOVAacrossElectrodes_wholeBrainAnalysis.txt')\n",
    "else:\n",
    "    print(\"The required output name 'Stimulus_cr_fixationCrossBase_1sec_mirror' is not in output_names.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay now do within-electrode anova too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the new modular code works then apply it here too (change data_for_anova basically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_anova = []\n",
    "\n",
    "def process_and_append_trial_data_for_anova_whole_brain_analysis(time_averaged_lists, time_window, output_names_conditions):\n",
    "    for output_name, conditions in output_names_conditions.items():\n",
    "        for subject_index, subject_data in enumerate(time_averaged_lists[output_name]):\n",
    "            subject_id = subjects[subject_index]\n",
    "            \n",
    "            if subject_id not in sig_chans_per_subject or not sig_chans_per_subject[subject_id]:\n",
    "                continue\n",
    "\n",
    "            for trial_index, trial_data in enumerate(subject_data):\n",
    "                if np.any(np.isnan(trial_data)) or len(trial_data) != len(sig_chans_per_subject[subject_id]):\n",
    "                    continue\n",
    "\n",
    "                for electrode_index, electrode_name in enumerate(sig_chans_per_subject[subject_id]):\n",
    "                    activity = trial_data[electrode_index] if electrode_index < len(trial_data) else np.nan\n",
    "                    \n",
    "                    # Prepare the data dictionary\n",
    "                    data_dict = {\n",
    "                        'SubjectID': subject_id,\n",
    "                        'Electrode': electrode_name,\n",
    "                        'TimeWindow': time_window,\n",
    "                        'Trial': trial_index + 1,\n",
    "                        'Activity': activity\n",
    "                    }\n",
    "                    \n",
    "                    # Dynamically add condition types and their values\n",
    "                    data_dict.update(conditions)\n",
    "\n",
    "                    data_for_anova.append(data_dict)\n",
    "\n",
    "# Invoke the function for each time-averaged list\n",
    "process_and_append_trial_data_for_anova_whole_brain_analysis(output_data_timeAvg_firstHalfSecond_lists, \"FirstHalfSecond\", output_names_conditions)\n",
    "process_and_append_trial_data_for_anova_whole_brain_analysis(output_data_timeAvg_secondHalfSecond_lists, \"SecondHalfSecond\", output_names_conditions)\n",
    "process_and_append_trial_data_for_anova_whole_brain_analysis(output_data_timeAvg_fullSecond_lists, \"FullSecond\", output_names_conditions)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_for_trial_level_anova = pd.DataFrame(data_for_anova)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_trial_level_anova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_significant_effects(anova_table):\n",
    "    \"\"\"\n",
    "    Extract significant effects and their p-values from the ANOVA results table,\n",
    "    removing 'C(...)' from effect names and formatting them neatly.\n",
    "    \"\"\"\n",
    "    significant_effects = []\n",
    "    for effect in anova_table.index:\n",
    "        p_value = anova_table.loc[effect, 'PR(>F)']\n",
    "        if p_value < 0.05:\n",
    "            # Remove 'C(' and ')' from the effect names\n",
    "            formatted_effect = effect.replace('C(', '').replace(')', '')\n",
    "            significant_effects.append((formatted_effect, p_value))\n",
    "    return significant_effects\n",
    "\n",
    "# Assuming df_for_trial_level_anova is your DataFrame and it includes a 'SubjectID' column\n",
    "def perform_modular_within_electrode_anova_whole_brain_analysis(df, save_dir, save_name):\n",
    "    import json\n",
    "    results = []\n",
    "    significant_effects_structure = {}\n",
    "\n",
    "    for subject_id in df['SubjectID'].unique():\n",
    "        for electrode in df['Electrode'].unique(): #this is wrong cuz then it only does dlpfc, fix this 3/6\n",
    "            for time_window in df['TimeWindow'].unique():\n",
    "                df_filtered = df[(df['SubjectID'] == subject_id) & \n",
    "                                 (df['Electrode'] == electrode) & \n",
    "                                 (df['TimeWindow'] == time_window)]\n",
    "                \n",
    "                if df_filtered.empty:\n",
    "                    continue\n",
    "                \n",
    "                # Dynamically construct the formula based on condition keys present in the DataFrame\n",
    "                condition_keys = [key for key in output_names_conditions[next(iter(output_names_conditions))].keys()]\n",
    "                formula_terms = ' + '.join([f'C({key})' for key in condition_keys])\n",
    "                interaction_terms = ' * '.join([f'C({key})' for key in condition_keys])\n",
    "                formula = f'Activity ~ {formula_terms} + {interaction_terms}'\n",
    "\n",
    "                # Perform the ANOVA\n",
    "                model = ols(formula, data=df_filtered).fit()\n",
    "                anova_results = anova_lm(model, typ=2)\n",
    "                \n",
    "                # Append the results\n",
    "                results.append({\n",
    "                    'SubjectID': subject_id,\n",
    "                    'Electrode': electrode,\n",
    "                    'TimeWindow': time_window,\n",
    "                    'ANOVA_Results': anova_results\n",
    "                })\n",
    "    \n",
    "    # Add the suffix '_onlySigElectrodes' to the base filename\n",
    "    allElectrodesFilename = f\"{save_name}_allElectrodes_wholeBrainAnalysis.txt\"\n",
    "    onlySigElectrodesFilename = f\"{save_name}_onlySigElectrodes_wholeBrainAnalysis.txt\"\n",
    "    significantEffectsStructureFilename = f\"{save_name}_significantEffectsStructure_wholeBrainAnalysis.txt\"\n",
    "\n",
    "    # Define the full path for the results file\n",
    "    results_file_path = os.path.join(save_dir, allElectrodesFilename)\n",
    "\n",
    "    # Save the ANOVA results to a text file\n",
    "    with open(results_file_path, 'w') as file:\n",
    "        file.write(results.__str__())\n",
    "\n",
    "    # Optionally, print the path to the saved file and/or return it\n",
    "    print(f\"results saved to: {results_file_path}\")\n",
    "\n",
    "    # Now process the significant results, including the subject ID in the output\n",
    "    significant_results = []\n",
    "\n",
    "    for result in results:\n",
    "        anova_table = result['ANOVA_Results']\n",
    "        subject_id = result['SubjectID']\n",
    "        electrode = result['Electrode']\n",
    "        time_window = result['TimeWindow']\n",
    "        \n",
    "        significant_effects = anova_table[anova_table['PR(>F)'] < 0.05]\n",
    "        \n",
    "        if not significant_effects.empty:\n",
    "            print(f\"Significant effects found for Subject: {subject_id}, Electrode: {electrode}, Time Window: {time_window}\")\n",
    "            print(significant_effects)\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            significant_results.append({\n",
    "                'SubjectID': subject_id,\n",
    "                'Electrode': electrode,\n",
    "                'TimeWindow': time_window,\n",
    "                'SignificantEffects': significant_effects\n",
    "            })\n",
    "        \n",
    "\n",
    "        # Extract significant effects for the current result. Basically just get the p-value. 3/19.\n",
    "        sig_effects_just_p_values = extract_significant_effects(anova_table)\n",
    "        \n",
    "        if sig_effects_just_p_values:\n",
    "            # Ensure subject_id and electrode keys exist\n",
    "            if subject_id not in significant_effects_structure:\n",
    "                significant_effects_structure[subject_id] = {}\n",
    "            if electrode not in significant_effects_structure[subject_id]:\n",
    "                significant_effects_structure[subject_id][electrode] = {}\n",
    "            \n",
    "            # Assign the significant effects and their p-values to the correct structure\n",
    "            significant_effects_structure[subject_id][electrode][time_window] = sig_effects_just_p_values    \n",
    "\n",
    "    # Define the full path for the results file\n",
    "    significant_results_file_path = os.path.join(save_dir, onlySigElectrodesFilename)\n",
    "\n",
    "    # Save the ANOVA results to a text file\n",
    "    with open(significant_results_file_path, 'w') as file:\n",
    "        file.write(significant_results.__str__())\n",
    "\n",
    "    # Optionally, print the path to the saved file and/or return it\n",
    "    print(f\"significant_results saved to: {significant_results_file_path}\")\n",
    "\n",
    "    significant_effects_structure_file_path = os.path.join(save_dir, significantEffectsStructureFilename)\n",
    "    # Save the ANOVA results to a json file (if this works, change the others to json files too)\n",
    "    with open(significant_effects_structure_file_path, 'w') as file:\n",
    "        json.dump(significant_effects_structure, file, indent=4)\n",
    "\n",
    "    # Optionally, print the path to the saved file and/or return it\n",
    "    print(f\"significant_effects_structure saved to: {significant_effects_structure_file_path}\")\n",
    "\n",
    "    return results, significant_results, significant_effects_structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For loading json 3/19. Incorporate into plotting code later.\n",
    "# with open(significant_effects_structure_file_path, 'r') as file:\n",
    "#     loaded_structure = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "congruency as function of congruency proportion  \n",
    "maybe make the save_name based on the conditions..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Stimulus_c25_fixationCrossBase_1sec_mirror' in output_names:\n",
    "    results, significant_results, significant_effects_structure = perform_modular_within_electrode_anova_whole_brain_analysis(df_for_trial_level_anova, save_dir, 'congruency_congruencyProportion_ANOVAwithinElectrodes')\n",
    "else:\n",
    "    print(\"The required output name 'Stimulus_c25_fixationCrossBase_1sec_mirror' is not in output_names.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "switch type as function of switch proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Stimulus_s25_fixationCrossBase_1sec_mirror' in output_names:\n",
    "    results, significant_results, significant_effects_structure = perform_modular_within_electrode_anova_whole_brain_analysis(df_for_trial_level_anova, save_dir, 'switchType_switchProportion_ANOVAwithinElectrodes')\n",
    "else:\n",
    "    print(\"The required output name 'Stimulus_s25_fixationCrossBase_1sec_mirror' is not in output_names.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "congruency as function of switch type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Stimulus_cr_fixationCrossBase_1sec_mirror' in output_names:\n",
    "    results, significant_results, significant_effects_structure = perform_modular_within_electrode_anova_whole_brain_analysis(df_for_trial_level_anova, save_dir, 'congruency_switchType_ANOVAwithinElectrodes')\n",
    "else:\n",
    "    print(\"The required output name 'Stimulus_c25_fixationCrossBase_1sec_mirror' is not in output_names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_effects_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot and QC stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot time perm cluster stats (don't run this immediately below cell if didn't do time perm cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plotting\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(time_perm_cluster_results['dlpfc'])\n",
    "# plt.xlabel('Timepoints')\n",
    "# plt.ylabel('Significance (0 or 1)')\n",
    "# plt.title('Permutation Test Significance Over Time')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot interaction effects (only do this when load in all four of them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://matplotlib.org/stable/gallery/color/named_colors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the other conditions and give them condition names and colors too\n",
    "plotting_parameters = {\n",
    "    'Stimulus_r25and75_fixationCrossBase_1sec_mirror': {\n",
    "        'condition_name': 'repeat',\n",
    "        'color': 'red',\n",
    "        \"line_style\": \"-\"\n",
    "    },\n",
    "    'Stimulus_s25and75_fixationCrossBase_1sec_mirror': {\n",
    "        'condition_name': 'switch',\n",
    "        'color': 'green',\n",
    "        \"line_style\": \"-\"\n",
    "    },\n",
    "    'Stimulus_c25and75_fixationCrossBase_1sec_mirror': {\n",
    "        'condition_name': 'congruent',\n",
    "        'color': 'blue',\n",
    "        \"line_style\": \"-\"\n",
    "    },\n",
    "    'Stimulus_i25and75_fixationCrossBase_1sec_mirror': {\n",
    "        'condition_name': 'incongruent',\n",
    "        'color': 'orange',\n",
    "        \"line_style\": \"-\"\n",
    "    },\n",
    "    \"Stimulus_ir_fixationCrossBase_1sec_mirror\": {\n",
    "        \"condition_name\": \"IR\",\n",
    "        \"color\": \"blue\",\n",
    "        \"line_style\": \"-\"\n",
    "    },\n",
    "    \"Stimulus_is_fixationCrossBase_1sec_mirror\": {\n",
    "        \"condition_name\": \"IS\",\n",
    "        \"color\": \"blue\",\n",
    "        \"line_style\": \"--\"\n",
    "    },\n",
    "    \"Stimulus_cr_fixationCrossBase_1sec_mirror\": {\n",
    "        \"condition_name\": \"CR\",\n",
    "        \"color\": \"red\",\n",
    "        \"line_style\": \"-\"\n",
    "    },\n",
    "    \"Stimulus_cs_fixationCrossBase_1sec_mirror\": {\n",
    "        \"condition_name\": \"CS\",\n",
    "        \"color\": \"red\",\n",
    "        \"line_style\": \"--\"\n",
    "    },\n",
    "    \"Stimulus_c25_fixationCrossBase_1sec_mirror\": {\n",
    "        \"condition_name\": \"c25\",\n",
    "        \"color\": \"red\",\n",
    "        \"line_style\": \"--\"\n",
    "    },\n",
    "    \"Stimulus_c75_fixationCrossBase_1sec_mirror\": {\n",
    "        \"condition_name\": \"c75\",\n",
    "        \"color\": \"red\",\n",
    "        \"line_style\": \"-\"\n",
    "    },\n",
    "    \"Stimulus_i25_fixationCrossBase_1sec_mirror\": {\n",
    "        \"condition_name\": \"i25\",\n",
    "        \"color\": \"blue\",\n",
    "        \"line_style\": \"--\"\n",
    "    },\n",
    "    \"Stimulus_i75_fixationCrossBase_1sec_mirror\": {\n",
    "        \"condition_name\": \"i75\",\n",
    "        \"color\": \"blue\",\n",
    "        \"line_style\": \"-\"\n",
    "    },\n",
    "    \"Stimulus_s25_fixationCrossBase_1sec_mirror\": {\n",
    "        \"condition_name\": \"s25\",\n",
    "        \"color\": \"green\",\n",
    "        \"line_style\": \"--\"\n",
    "    },\n",
    "    \"Stimulus_s75_fixationCrossBase_1sec_mirror\": {\n",
    "        \"condition_name\": \"s75\",\n",
    "        \"color\": \"green\",\n",
    "        \"line_style\": \"-\"\n",
    "    },\n",
    "    \"Stimulus_r25_fixationCrossBase_1sec_mirror\": {\n",
    "        \"condition_name\": \"r25\",\n",
    "        \"color\": \"pink\",\n",
    "        \"line_style\": \"--\"\n",
    "    },\n",
    "    \"Stimulus_r75_fixationCrossBase_1sec_mirror\": {\n",
    "        \"condition_name\": \"r75\",\n",
    "        \"color\": \"pink\",\n",
    "        \"line_style\": \"-\"\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAB_root = None\n",
    "# Determine LAB_root based on the operating system\n",
    "if LAB_root is None:\n",
    "    HOME = os.path.expanduser(\"~\")\n",
    "    LAB_root = os.path.join(HOME, \"Box\", \"CoganLab\") if os.name == 'nt' else os.path.join(HOME, \"Library\", \"CloudStorage\", \"Box-Box\", \"CoganLab\")\n",
    "\n",
    "# Get data layout\n",
    "layout = get_data(task, root=LAB_root)\n",
    "save_dir = os.path.join(layout.root, 'derivatives', 'freqFilt', 'figs')\n",
    "\n",
    "def plot_interact_effects_modular_whole_brain_analysis(save_dir, save_name, output_names, plotting_parameters):\n",
    "    # Base setup for directories and file paths\n",
    "    save_path = os.path.join(save_dir, f'avg_{save_name}_interactEffects_zscore_wholeBrainAnalysis.png')\n",
    "\n",
    "    # Initialize plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Dynamically select the first subject and use it to extract times\n",
    "    first_subject_id = next(iter(subjects_mne_objects))\n",
    "    example_output_name = next(iter(subjects_mne_objects[first_subject_id]))\n",
    "    times = subjects_mne_objects[first_subject_id][example_output_name]['HG_ev1_evoke_rescaled'].times\n",
    "\n",
    "    overall_averages_for_plotting = {}\n",
    "    overall_sem_for_plotting = {}\n",
    "    # Initialize variables to store the global min and max values\n",
    "    global_min_val = float('inf')  # Set to infinity initially\n",
    "    global_max_val = float('-inf')  # Set to negative infinity initially\n",
    "    \n",
    "    # Generate labels and plot each condition\n",
    "    for index, output_name in enumerate(output_names):\n",
    "        # label = output_name.split(\"_\")[1]  # OR extract label from output name instead of plotting parameters dict. Up to you.\n",
    "        overall_averages_for_plotting[output_name] = overall_averages[output_name]\n",
    "        overall_sem_for_plotting[output_name] = overall_sems[output_name]\n",
    "\n",
    "        # Calculate the minimum value for this condition, including SEM\n",
    "        current_min_val = min(overall_averages_for_plotting[output_name] - overall_sem_for_plotting[output_name])\n",
    "        # Calculate the maximum value for this condition, including SEM\n",
    "        current_max_val = max(overall_averages_for_plotting[output_name] + overall_sem_for_plotting[output_name])\n",
    "\n",
    "        # Update the global min and max values if necessary\n",
    "        global_min_val = min(global_min_val, current_min_val)\n",
    "        global_max_val = max(global_max_val, current_max_val)\n",
    "\n",
    "        # Optionally, add a small margin to the range\n",
    "        margin = (global_max_val - global_min_val) * 0.05  # 5% of the range as margin\n",
    "        global_min_val -= margin\n",
    "        global_max_val += margin\n",
    "\n",
    "        label = plotting_parameters[output_name]['condition_name'] # extract label from plotting parameters dict\n",
    "        color = plotting_parameters[output_name]['color']\n",
    "        line_style = plotting_parameters[output_name]['line_style']\n",
    "\n",
    "        plt.plot(times, overall_averages_for_plotting[output_name], linestyle=line_style, color=color, label=f'Average {label}')\n",
    "        plt.fill_between(times, overall_averages_for_plotting[output_name] - overall_sem_for_plotting[output_name], overall_averages_for_plotting[output_name] + overall_sem_for_plotting[output_name], alpha=0.3, color=color)\n",
    "\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Z-score')\n",
    "    plt.title(f'Average Signal with Standard Error for {save_name}')\n",
    "    plt.legend()\n",
    "    # Adjust the y-axis limits\n",
    "    plt.ylim([global_min_val, global_max_val])\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is just for congruent vs congruency proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Stimulus_c25_fixationCrossBase_1sec_mirror' in output_names:\n",
    "    plot_interact_effects_modular_whole_brain_analysis(save_dir, 'congruency_congruencyProportion', output_names, plotting_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this just for switch vs switch proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Stimulus_s25_fixationCrossBase_1sec_mirror' in output_names:\n",
    "    plot_interact_effects_modular_whole_brain_analysis(save_dir, 'switchType_switchProportion', output_names, plotting_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this just for congruency vs switch type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Stimulus_cr_fixationCrossBase_1sec_mirror' in output_names:\n",
    "    plot_interact_effects_modular_whole_brain_analysis(save_dir, 'congruency_switchType', output_names, plotting_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot individual electrodes for interaction effects\n",
    "i think this will just work regardless of the output names 3/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 839 is out of bounds for axis 0 with size 839",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 130\u001b[0m\n\u001b[0;32m    128\u001b[0m electrode_data \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output_name \u001b[38;5;129;01min\u001b[39;00m output_names:\n\u001b[1;32m--> 130\u001b[0m     electrode_data[output_name] \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenated_trialAvg_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43moutput_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43melectrode_counter\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    132\u001b[0m electrodes_data\u001b[38;5;241m.\u001b[39mappend((electrode_data, sub, electrode))\n\u001b[0;32m    133\u001b[0m electrode_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 839 is out of bounds for axis 0 with size 839"
     ]
    }
   ],
   "source": [
    "LAB_root = None\n",
    "channels = None\n",
    "full_trial_base = False\n",
    "\n",
    "if LAB_root is None:\n",
    "    HOME = os.path.expanduser(\"~\")\n",
    "    if os.name == 'nt':  # windows\n",
    "        LAB_root = os.path.join(HOME, \"Box\", \"CoganLab\")\n",
    "    else:  # mac\n",
    "        LAB_root = os.path.join(HOME, \"Library\", \"CloudStorage\", \"Box-Box\",\n",
    "                                \"CoganLab\")\n",
    "\n",
    "layout = get_data(task, root=LAB_root)\n",
    "save_dir = os.path.join(layout.root, 'derivatives', 'freqFilt', 'figs')\n",
    "\n",
    "# Dynamically select the first subject and use it to extract times\n",
    "first_subject_id = next(iter(subjects_mne_objects))\n",
    "example_output_name = next(iter(subjects_mne_objects[first_subject_id]))\n",
    "times = subjects_mne_objects[first_subject_id][example_output_name]['HG_ev1_evoke_rescaled'].times\n",
    "\n",
    "# # Use the times from your evoked data (assuming these are representative for all subjects)\n",
    "# times = HG_ev1_evoke_rescaled_D0057_c.times  # Modify as needed to match your data\n",
    "\n",
    "def plot_significance(ax, times, sig_effects, y_offset=0.1):\n",
    "    \"\"\"\n",
    "    Plot significance bars for the effects on top of the existing axes, adjusted for time windows.\n",
    "\n",
    "    Parameters:\n",
    "    - ax: The matplotlib Axes object to plot on.\n",
    "    - times: Array of time points for the x-axis.\n",
    "    - sig_effects: Dictionary with time windows as keys and lists of tuples (effect, p-value) as values.\n",
    "    - y_offset: The vertical offset between different time window significance bars.\n",
    "    \"\"\"\n",
    "    y_pos_base = ax.get_ylim()[1]  # Get the top y-axis limit to place significance bars\n",
    "    \n",
    "    time_windows = {\n",
    "        'FirstHalfSecond': (0, 0.5),\n",
    "        'SecondHalfSecond': (0.5, 1),\n",
    "        'FullSecond': (0, 1)\n",
    "    }\n",
    "\n",
    "    # Sort time windows to ensure 'FullSecond' bars are plotted last (on top)\n",
    "    for time_window, effects in sorted(sig_effects.items(), key=lambda x: x[0] == 'FullSecond'):\n",
    "        y_pos = y_pos_base + y_offset * list(time_windows).index(time_window)  # Adjust y_pos based on time window\n",
    "        for effect, p_value in effects:\n",
    "            start_time, end_time = time_windows[time_window]\n",
    "            # untested new colors 3/20\n",
    "            # Determine the color based on the effect name\n",
    "            if 'congruency' in effect:\n",
    "                color = 'red'\n",
    "            elif 'congruencyProportion' in effect:\n",
    "                color = 'blue'\n",
    "            elif 'switchType' in effect:\n",
    "                color = 'green'\n",
    "            elif 'switchProportion' in effect:\n",
    "                color = 'yellow'\n",
    "            \n",
    "            else:\n",
    "                color = 'black'  # Default color\n",
    "\n",
    "            # Assign colors for interaction effects\n",
    "            if 'congruency:congruencyProportion' in effect:\n",
    "                color = 'purple'\n",
    "            elif 'switchType:switchProportion' in effect:\n",
    "                color = 'yellowgreen'\n",
    "            elif 'congruency:switchType' in effect:\n",
    "                color = 'brown'\n",
    "\n",
    "            num_asterisks = '*' * (1 if p_value < 0.05 else 2 if p_value < 0.01 else 3)\n",
    "            # actually plot\n",
    "            ax.plot([start_time, end_time], [y_pos, y_pos], color=color, lw=4)\n",
    "            ax.text((start_time + end_time) / 2, y_pos, num_asterisks, ha='center', va='bottom', color=color)\n",
    "        \n",
    "\n",
    "# once these two function are fully working, can move them to misc_functions and then use them in roi analysis too. And can rename to plot_electrodes_grid instead of plot_electrodes_grid_whole_brain_analysis.\n",
    "def plot_electrodes_grid_whole_brain_analysis(electrodes_data, significant_effects_structure, grid_num, output_names, times, save_dir, save_name, plotting_parameters):\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(20, 12))  # Adjust figure size as needed\n",
    "    axes = axes.flatten()  # Flatten the axes array for easy indexing\n",
    "\n",
    "    for i, (data, sub, electrode) in enumerate(electrodes_data):\n",
    "        ax = axes[i]\n",
    "        for output_name in output_names:\n",
    "            color = plotting_parameters[output_name]['color']\n",
    "            line_style = plotting_parameters[output_name]['line_style']\n",
    "            ax.plot(times, data[output_name], label=f'{output_name}', color=color, linestyle=line_style)\n",
    "\n",
    "            # i think this is wrong..? It's getting the standard deviation of the trial averaged data, but I want the stdev of the trials themselves for each timepoint.\n",
    "            # So maybe this is a constant stdev across time? Idk. Try using the trial_std 3/17. \n",
    "            ax.fill_between(times, \n",
    "                            data[output_name] - np.std(data[output_name], ddof=1) / np.sqrt(len(data[output_name])),\n",
    "                            data[output_name] + np.std(data[output_name], ddof=1) / np.sqrt(len(data[output_name])), alpha=0.3)\n",
    "\n",
    "        ax.set_title(f'Subject {sub}, Electrode {electrode}')\n",
    "        ax.set_xlabel('Time (s)')\n",
    "        ax.set_ylabel('Z-score')\n",
    "\n",
    "        # Retrieve significant effects for the current subject and electrode\n",
    "        sig_effects = significant_effects_structure.get(sub, {}).get(electrode, {})\n",
    "        if sig_effects:\n",
    "            # Adjust y_offset based on plotting needs. This used to not be assigned to a variable. 3/20.\n",
    "            plot_significance(ax, times, sig_effects, y_offset=0.1)\n",
    "\n",
    "    # Create the legend at the top center of the figure\n",
    "    handles, labels = ax.get_legend_handles_labels()  # Get handles and labels from the last subplot\n",
    "    fig.legend(handles, labels, loc='lower center', ncol=2)\n",
    "\n",
    "    plt.tight_layout()  # Adjust the layout to make room for the legend\n",
    "    plt.savefig(os.path.join(save_dir, f'wholeBrainAnalysis_{save_name}_electrodes_plot_grid_{grid_num+1}.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Example Usage\n",
    "electrodes_data = []\n",
    "electrode_counter = 0\n",
    "grid_size = 16  # Number of electrodes per grid\n",
    "grid_num = 0\n",
    "if 'Stimulus_c25_fixationCrossBase_1sec_mirror' in output_names:\n",
    "    save_name = 'congruency_congruencyProportion' # i think this will be congruency x con prop if i load in c25?\n",
    "elif 'Stimulus_s25_fixationCrossBase_1sec_mirror' in output_names:\n",
    "    save_name = 'switchType_switchProportion' # i think if there's no c25, but there is s25, then i am doing switch x switch prop? 3/17\n",
    "elif 'Stimulus_cr_fixationCrossBase_1sec_mirror' in output_names:\n",
    "    save_name = 'congruency_switchType'\n",
    "    \n",
    "significant_effects_structure_file_path = os.path.join(save_dir, f'{save_name}_ANOVAwithinElectrodes_significantEffectsStructure_wholeBrainAnalysis.txt')\n",
    "with open(significant_effects_structure_file_path, 'r') as file:\n",
    "    significant_effects_structure = json.load(file)\n",
    "    \n",
    "for sub in subjects:\n",
    "    for electrode in sig_chans_per_subject[sub]:\n",
    "        \n",
    "        electrode_data = {}\n",
    "        for output_name in output_names:\n",
    "            electrode_data[output_name] = concatenated_trialAvg_data[output_name][electrode_counter]\n",
    "\n",
    "        electrodes_data.append((electrode_data, sub, electrode))\n",
    "        electrode_counter += 1\n",
    "        if len(electrodes_data) == grid_size:\n",
    "\n",
    "            plot_electrodes_grid_whole_brain_analysis(electrodes_data, significant_effects_structure, grid_num, output_names, times, save_dir, save_name, plotting_parameters)\n",
    "            electrodes_data = []  # Reset for the next grid\n",
    "            grid_num += 1\n",
    "\n",
    "# Plot remaining electrodes in the last grid\n",
    "if electrodes_data:\n",
    "    plot_electrodes_grid_whole_brain_analysis(electrodes_data, significant_effects_structure, grid_num, output_names, times, save_dir, save_name, plotting_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_effects_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make a legend to put on the powerpoint slide for significance bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def create_significance_legend(effect_color_mapping, save_dir, filename=\"significance_legend.png\"):\n",
    "    \"\"\"\n",
    "    Create and save a separate legend for significance colors.\n",
    "\n",
    "    Parameters:\n",
    "    - effect_color_mapping: Dictionary mapping effect names to colors.\n",
    "    - save_dir: Directory where the legend PNG will be saved.\n",
    "    - filename: Name of the PNG file for the legend.\n",
    "    \"\"\"\n",
    "    # Create a figure and a dummy subplot (not displayed) to host the legend\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.axis('off')  # Turn off the axis\n",
    "\n",
    "    # Create legend handles\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color=color, lw=4, label=effect)\n",
    "        for effect, color in effect_color_mapping.items()\n",
    "    ]\n",
    "    \n",
    "    # Create the legend\n",
    "    legend = ax.legend(handles=legend_elements, loc='center', frameon=False)\n",
    "\n",
    "    # Save the legend as a separate figure\n",
    "    fig.savefig(os.path.join(save_dir, filename), bbox_inches='tight')\n",
    "    plt.close(fig)  # Close the figure window\n",
    "\n",
    "# Example usage\n",
    "effect_color_mapping = {\n",
    "    'congruency': 'red',\n",
    "    'congruencyProportion': 'blue',\n",
    "    'switchType': 'green',\n",
    "    'switchProportion': 'yellow',\n",
    "    'congruency:congruencyProportion': 'purple',\n",
    "    'switchType:switchProportion': 'yellowgreen',\n",
    "    'congruency:switchType': 'brown'\n",
    "}\n",
    "\n",
    "# Assuming you have a save directory defined as save_dir\n",
    "create_significance_legend(effect_color_mapping, save_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are 839 electrodes in concatenated_trialAvg_data. BUT there are 824 electrodes in overall electrode mapping, and also 842 electrodes in sig chans per subject. What is going on here..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the size of each array in concatenated_trialAvg_data\n",
    "sizes = {key: data.shape for key, data in concatenated_trialAvg_data.items()}\n",
    "\n",
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_electrodes = sum(len(electrodes) for electrodes in sig_chans_per_subject.values())\n",
    "total_electrodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
