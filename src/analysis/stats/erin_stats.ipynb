{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/hpc/home/etb28/miniconda3/envs/ieeg/lib/python311.zip', '/hpc/home/etb28/miniconda3/envs/ieeg/lib/python3.11', '/hpc/home/etb28/miniconda3/envs/ieeg/lib/python3.11/lib-dynload', '', '/hpc/home/etb28/miniconda3/envs/ieeg/lib/python3.11/site-packages']\n",
      "Qt5Agg backend not available, using default backend\n",
      "Qt5Agg backend not available, using default backend\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(sys.path)\n",
    "sys.path.append(\"C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/\") #need to do this cuz otherwise ieeg isn't added to path...\n",
    "\n",
    "# Get the absolute path to the directory containing the current script\n",
    "# For GlobalLocal/src/analysis/preproc/make_epoched_data.py, this is GlobalLocal/src/analysis/preproc\n",
    "try:\n",
    "    # This will work if running as a .py script\n",
    "    current_file_path = os.path.abspath(__file__)\n",
    "    current_script_dir = os.path.dirname(current_file_path)\n",
    "except NameError:\n",
    "    # This will be executed if __file__ is not defined (e.g., in a Jupyter Notebook)\n",
    "    # os.getcwd() often gives the directory of the notebook,\n",
    "    # or the directory from which the Jupyter server was started.\n",
    "    current_script_dir = os.getcwd()\n",
    "\n",
    "# Navigate up three levels to get to the 'GlobalLocal' directory\n",
    "project_root = os.path.abspath(os.path.join(current_script_dir, '..', '..', '..'))\n",
    "\n",
    "# Add the 'GlobalLocal' directory to sys.path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root) # insert at the beginning to prioritize it\n",
    "\n",
    "from ieeg.navigate import channel_outlier_marker, trial_ieeg, crop_empty_data, \\\n",
    "    outliers_to_nan\n",
    "from ieeg.io import raw_from_layout, get_data\n",
    "from ieeg.timefreq.utils import crop_pad\n",
    "from ieeg.timefreq import gamma\n",
    "from ieeg.calc.scaling import rescale\n",
    "import mne\n",
    "import numpy as np\n",
    "from ieeg.calc.stats import time_perm_cluster, window_averaged_shuffle\n",
    "from ieeg.viz.mri import gen_labels\n",
    "\n",
    "# from utils import make_or_load_subjects_electrodes_to_ROIs_dict, load_acc_arrays, calculate_RTs, save_channels_to_file, save_sig_chans, \\\n",
    "#       load_sig_chans, channel_names_to_indices, filter_and_average_epochs, permutation_test, perform_permutation_test_across_electrodes, perform_permutation_test_within_electrodes, \\\n",
    "#       add_accuracy_to_epochs, load_mne_objects, create_subjects_mne_objects_dict, extract_significant_effects, convert_dataframe_to_serializable_format, \\\n",
    "#       perform_modular_anova, make_plotting_parameters, plot_significance\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict, defaultdict\n",
    "import json\n",
    "# still need to test if the permutation test functions load in properly.\n",
    "import pandas as pd\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "from statsmodels.formula.api import gee\n",
    "from statsmodels.genmod import families\n",
    "\n",
    "# from src.analysis.power.roi_analysis import blah_blah\n",
    "from src.analysis.config import experiment_conditions\n",
    "from src.analysis.config.plotting_parameters import plotting_parameters as plot_params\n",
    "import src.analysis.utils.general_utils as utils # import utils functions one by one by name\n",
    "from src.analysis.power.power_traces import make_multi_channel_evokeds_for_all_conditions_and_rois, plot_power_traces_for_all_rois, create_subtracted_evokeds_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting path\n"
     ]
    }
   ],
   "source": [
    "## set lab LAB_root\n",
    "\n",
    "LAB_root = None\n",
    "task='GlobalLocal'\n",
    "# Determine LAB_root based on the operating system\n",
    "if LAB_root is None:\n",
    "    HOME = os.path.expanduser(\"~\")\n",
    "    LAB_root = os.path.join(HOME, \"Box\", \"CoganLab\") if os.name == 'nt' else os.path.join(HOME, \"Library\", \"CloudStorage\", \"Box-Box\", \"CoganLab\")\n",
    "\n",
    "LAB_root = '/cwork/etb28/'\n",
    "\n",
    "\n",
    "# Get data layout\n",
    "layout = get_data(task, root=LAB_root)\n",
    "save_dir_root = os.path.join(layout.root, 'derivatives', 'freqFilt', 'figs')\n",
    "\n",
    "print(\"setting path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['D0057','D0059', 'D0063', 'D0065', 'D0069', 'D0071', 'D0077', 'D0090', 'D0094', 'D0100', 'D0102', 'D0103', 'D0107A', 'D0110', 'D0116', 'D0117', 'D0121']\n",
    "# subjects = ['D0057']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load the subjects' electrodes-to-ROIs dictionary...\n",
      "Loaded data from /hpc/group/coganlab/etb28/GlobalLocal/src/analysis/config/subjects_electrodestoROIs_dict.json\n",
      "Dictionary loaded successfully. Ready to proceed!\n",
      "subjects electrodes to rois dict\n"
     ]
    }
   ],
   "source": [
    "# load in subjects electrodes to rois dict. If it doesn't already exist, make it and then load it.\n",
    "config_dir = os.path.join(project_root, 'src', 'analysis', 'config')\n",
    "\n",
    "subjects_electrodestoROIs_dict = utils.make_or_load_subjects_electrodes_to_ROIs_dict(subjects, task='GlobalLocal', LAB_root=None, save_dir=config_dir, \n",
    "                                        filename='subjects_electrodestoROIs_dict.json')\n",
    "\n",
    "print(\"subjects electrodes to rois dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total elecs: 3283\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of electrodes in the 'default_dict' across subjects\n",
    "total_electrodes = utils.count_electrodes_across_subjects(subjects_electrodestoROIs_dict, subjects)\n",
    "print('total elecs:', total_electrodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load behavioral\n"
     ]
    }
   ],
   "source": [
    "combined_data = pd.read_csv(os.path.join(LAB_root, 'D_Data', 'GlobalLocal', 'combinedData.csv'))\n",
    "combined_data[['switchProportion', 'congruencyProportion']] = combined_data.apply(utils.map_block_type, axis=1)\n",
    "\n",
    "print(\"load behavioral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load epochs\n"
     ]
    }
   ],
   "source": [
    "task='GlobalLocal'\n",
    "conditions = experiment_conditions.stimulus_err_corr_conditions # set this to whichever conditions you're running\n",
    "stimulus_locked = True  #toggle\n",
    "response_locked = not stimulus_locked\n",
    "\n",
    "if stimulus_locked:\n",
    "    # epochs_root_file = \"Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_4.0-8.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False\"\n",
    "    # epochs_root_file = \"Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10\"\n",
    "    epochs_root_file = \"Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False\"\n",
    "    # epochs_root_file = \"Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_0.0-30.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False\"\n",
    "    # epochs_root_file = \"Stimulus_100sec_within1-101sec_experimentStartBase_decFactor_8_outliers_10_passband_70-150_padLength_0.5s_stat_func_ttest_ind_equal_var_False\"\n",
    "\n",
    "elif response_locked:\n",
    "    # epochs_root_file = \"Response_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_4.0-8.0_padLength_0.5s_stat_func_ttest_ind\"\n",
    "    epochs_root_file = \"Response_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind\"\n",
    "\n",
    "condition_names = [condition for condition in conditions.keys()]\n",
    "\n",
    "print(\"load epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for subject: D0057\n",
      "Reading /cwork/etb28/BIDS-1.1_GlobalLocal/BIDS/derivatives/freqFilt/figs/D0057/D0057_Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "449 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /cwork/etb28/BIDS-1.1_GlobalLocal/BIDS/derivatives/freqFilt/figs/D0057/D0057_Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False_HG_ev1_power_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "449 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "  Loading condition: Stimulus_err\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/coganlab/etb28/GlobalLocal/src/analysis/utils/general_utils.py:442: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  event_epochs = mne.concatenate_epochs(combined_epochs_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "32 matching events found\n",
      "No baseline correction applied\n",
      "    Stimulus_err: 32 valid trials out of 32\n",
      "  Loading condition: Stimulus_corr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/coganlab/etb28/GlobalLocal/src/analysis/utils/general_utils.py:442: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  event_epochs = mne.concatenate_epochs(combined_epochs_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "405 matching events found\n",
      "No baseline correction applied\n",
      "    Stimulus_corr: 405 valid trials out of 405\n",
      "  Loading condition: Stimulus_err\n",
      "Not setting metadata\n",
      "32 matching events found\n",
      "No baseline correction applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/coganlab/etb28/GlobalLocal/src/analysis/utils/general_utils.py:442: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  event_epochs = mne.concatenate_epochs(combined_epochs_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Stimulus_err: 32 valid trials out of 32\n",
      "  Loading condition: Stimulus_corr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/group/coganlab/etb28/GlobalLocal/src/analysis/utils/general_utils.py:442: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  event_epochs = mne.concatenate_epochs(combined_epochs_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "405 matching events found\n",
      "No baseline correction applied\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if conditions == experiment_conditions.stimulus_conditions:\n",
    "    conditions_save_name = 'stimulus_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_experiment_conditions:\n",
    "    conditions_save_name = 'stimulus_experiment_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_main_effect_conditions:\n",
    "    conditions_save_name = 'stimulus_main_effect_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_lwpc_conditions:\n",
    "    conditions_save_name = 'stimulus_lwpc_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_lwps_conditions:\n",
    "    conditions_save_name = 'stimulus_lwps_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_big_letter_conditions:\n",
    "    conditions_save_name = 'stimulus_big_letter_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_small_letter_conditions:\n",
    "    conditions_save_name = 'stimulus_small_letter_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_task_conditions:\n",
    "    conditions_save_name = 'stimulus_task_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_congruency_conditions:\n",
    "    conditions_save_name = 'stimulus_congruency_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_switch_type_conditions:\n",
    "    conditions_save_name = 'stimulus_switch_type_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_err_corr_conditions:\n",
    "    conditions_save_name = 'stimulus_err_corr_conditions_sem_green' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_err_congruency_conditions:\n",
    "    conditions_save_name = 'stimulus_err_congruency_conditions_sem' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "\n",
    "elif conditions == experiment_conditions.response_conditions:\n",
    "    conditions_save_name = 'response_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.response_experiment_conditions:\n",
    "    conditions_save_name = 'response_experiment_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.response_big_letter_conditions:\n",
    "    conditions_save_name = 'response_big_letter_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.response_small_letter_conditions:\n",
    "    conditions_save_name = 'response_small_letter_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.response_task_conditions:\n",
    "    conditions_save_name = 'response_task_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.response_congruency_conditions:\n",
    "    conditions_save_name = 'response_congruency_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.response_switch_type_conditions:\n",
    "    conditions_save_name = 'response_switch_type_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.response_err_corr_conditions:\n",
    "    conditions_save_name = 'response_err_corr_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.response_err_congruency_conditions:\n",
    "    conditions_save_name = 'response_err_congruency_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "    \n",
    "# Assuming 'combined_data' is your DataFrame and 'subjects' is your list of subject IDs\n",
    "#subjects_mne_objects = utils.create_subjects_mne_objects_dict(subjects=subjects, epochs_root_file=epochs_root_file, conditions=conditions, task=\"GlobalLocal\", just_HG_ev1_rescaled=True, acc_trials_only=True, error_trials_only=False)\n",
    "#one without selecting only correct trials\n",
    "subjects_mne_objects = utils.create_subjects_mne_objects_dict(subjects=subjects, epochs_root_file=epochs_root_file, conditions=conditions, task=\"GlobalLocal\", just_HG_ev1_rescaled=True, acc_trials_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond1_data_list = []\n",
    "cond2_data_list = []\n",
    "\n",
    "condition_names_list = list(conditions.keys())\n",
    "condition1_name = condition_names_list[0]\n",
    "condition2_name = condition_names_list[1]\n",
    "\n",
    "mne_object_key = 'HG_ev1_rescaled'\n",
    "\n",
    "for sub in subjects:\n",
    "    try:\n",
    "        evoked1 = subjects_mne_objects[sub][condition1_name][mne_object_key + \"_avg\"]\n",
    "        cond1_data_list.append(evoked1.data)\n",
    "\n",
    "        evoked2 = subjects_mne_objects[sub][condition2_name][mne_object_key + \"_avg\"]\n",
    "        cond2_data_list.append(evoked2.data)\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Skipping subject {sub}: Missing data. Error: {e}\")\n",
    "        continue\n",
    "\n",
    "if not cond1_data_list or not cond2_data_list:\n",
    "    raise ValueError(\"Error - no data\")\n",
    "\n",
    "array_cond1 = np.stack(cond1_data_list, axis=0)\n",
    "array_cond2 = np.stack(cond2_data_list, axis=0)\n",
    "\n",
    "mask, _ = stats.time_perm_cluster(array_cond1, array_cond2,\n",
    "                                  p_thresh=0.05,\n",
    "                                  axis=0,        \n",
    "                                  n_perm=1000,  \n",
    "                                  n_jobs=-1,     \n",
    "                                  ignore_adjacency=1)\n",
    "\n",
    "# response data points are length 201, while baseline is 200. We need to\n",
    "# trim the response data to match the baseline with [..., :-1]\n",
    "mask2 = stats.window_averaged_shuffle(array_cond1[..., :-1], array_cond2,\n",
    "                                      n_perm=1000,\n",
    "                                      stat_func=mean_diff)\n",
    "fig2 = plt.imshow(0.05 >= mask2[:, None])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
