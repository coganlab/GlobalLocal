{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b3a4ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\jz421\\\\Desktop\\\\GlobalLocal\\\\IEEG_Pipelines', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\python311.zip', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\DLLs', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg', '', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\win32', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: Keras package is not installed. You will be unable to useall neural net decoders\n",
      "['c:\\\\Users\\\\jz421\\\\Desktop\\\\GlobalLocal', 'C:\\\\Users\\\\jz421\\\\Desktop\\\\GlobalLocal\\\\IEEG_Pipelines', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\python311.zip', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\DLLs', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg', '', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\win32', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages\\\\Pythonwin', 'C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/']\n",
      "['c:\\\\Users\\\\jz421\\\\Desktop\\\\GlobalLocal', 'C:\\\\Users\\\\jz421\\\\Desktop\\\\GlobalLocal\\\\IEEG_Pipelines', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\python311.zip', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\DLLs', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg', '', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\win32', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages\\\\Pythonwin', 'C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/', 'C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/', 'C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print(sys.path)\n",
    "sys.path.append(\"C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/\") #need to do this cuz otherwise ieeg isn't added to path...\n",
    "\n",
    "# Get the absolute path to the directory containing the current script\n",
    "# For GlobalLocal/src/analysis/preproc/make_epoched_data.py, this is GlobalLocal/src/analysis/preproc\n",
    "try:\n",
    "    # This will work if running as a .py script\n",
    "    current_file_path = os.path.abspath(__file__)\n",
    "    current_script_dir = os.path.dirname(current_file_path)\n",
    "except NameError:\n",
    "    # This will be executed if __file__ is not defined (e.g., in a Jupyter Notebook)\n",
    "    # os.getcwd() often gives the directory of the notebook,\n",
    "    # or the directory from which the Jupyter server was started.\n",
    "    current_script_dir = os.getcwd()\n",
    "\n",
    "# Navigate up three levels to get to the 'GlobalLocal' directory\n",
    "project_root = os.path.abspath(os.path.join(current_script_dir, '..', '..', '..'))\n",
    "\n",
    "# Add the 'GlobalLocal' directory to sys.path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root) # insert at the beginning to prioritize it\n",
    "    \n",
    "from functools import partial\n",
    "from ieeg.navigate import channel_outlier_marker, trial_ieeg, crop_empty_data, \\\n",
    "    outliers_to_nan\n",
    "from ieeg.io import raw_from_layout, get_data\n",
    "from ieeg.timefreq.utils import crop_pad\n",
    "from ieeg.timefreq import gamma\n",
    "from ieeg.calc.scaling import rescale\n",
    "import mne\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ieeg.calc.reshape import make_data_same\n",
    "from ieeg.calc.stats import time_perm_cluster\n",
    "from ieeg.calc.mat import LabeledArray, combine\n",
    "from ieeg.viz.parula import parula_map\n",
    "\n",
    "# TODO: hmm fix these utils imports, import the funcs individually. 6/1/25.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas import read_csv\n",
    "import scipy.stats as stats\n",
    "import joblib\n",
    "\n",
    "from scipy.ndimage import label\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# rsa toolbox imports\n",
    "from rsatoolbox.io.mne import read_epochs\n",
    "from rsatoolbox.data.ops import merge_datasets\n",
    "from rsatoolbox.rdm import calc_rdm_movie\n",
    "from rsatoolbox.rdm.calc import _parse_input\n",
    "from rsatoolbox.util.build_rdm import _build_rdms\n",
    "from rsatoolbox.rdm import compare\n",
    "from rsatoolbox.vis import show_rdm\n",
    "from rsatoolbox.vis.timecourse import plot_timecourse\n",
    "\n",
    "from os.path import join, expanduser, basename\n",
    "import glob, json\n",
    "import numpy, tqdm, mne, pandas\n",
    "import rsatoolbox\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import copy\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from ieeg.decoding.decoders import PcaLdaClassification\n",
    "from ieeg.calc.oversample import MinimumNaNSplit\n",
    "from ieeg.calc.fast import mixup\n",
    "\n",
    "from src.analysis.config import experiment_conditions\n",
    "\n",
    "from src.analysis.utils.labeled_array_utils import (\n",
    "    put_data_in_labeled_array_per_roi_subject,\n",
    "    remove_nans_from_labeled_array,\n",
    "    remove_nans_from_all_roi_labeled_arrays,\n",
    "    concatenate_conditions_by_string,\n",
    "    get_data_in_time_range\n",
    ")\n",
    "\n",
    "from src.analysis.decoding.decoding import (\n",
    "    concatenate_and_balance_data_for_decoding, \n",
    "    get_and_plot_confusion_matrix_for_rois_jim,\n",
    "    Decoder, \n",
    "    windower,\n",
    "    get_confusion_matrices_for_rois_time_window_decoding_jim,\n",
    "    compute_accuracies,\n",
    "    perform_time_perm_cluster_test_for_accuracies,\n",
    "    plot_accuracies\n",
    ")\n",
    "\n",
    "from src.analysis.spec.wavelet_functions import get_uncorrected_wavelets, get_uncorrected_multitaper, get_sig_tfr_differences, plot_mask_pages\n",
    "from src.analysis.spec.subjects_tfr_objects_functions import load_or_make_subjects_tfr_objects, get_sig_tfr_differences_per_subject, get_sig_tfr_differences_per_roi\n",
    "\n",
    "from src.analysis.utils.general_utils import (\n",
    "    make_or_load_subjects_electrodes_to_ROIs_dict,\n",
    "    get_good_data,\n",
    "    get_sig_chans_per_subject,\n",
    "    make_sig_electrodes_per_subject_and_roi_dict,\n",
    "    calculate_total_electrodes,\n",
    "    check_sampling_rates\n",
    ")\n",
    "\n",
    "import mne.time_frequency\n",
    "from ieeg.calc.scaling import rescale\n",
    "from ieeg.timefreq.utils import wavelet_scaleogram, crop_pad\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from typing import Union, List, Sequence, Dict\n",
    "import doctest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d07116",
   "metadata": {},
   "source": [
    "#### 0. Load data.   \n",
    "Need a way to load in the frequency information too, not just trials x channels x timepoints. Because I'm going to use frequency as a decoding feature too. For each electrode, for each training set, I think I can just mask the multitaper with the significant clusters and use that as the decoding feature, and then concatenate across electrodes to build the full training matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c04940ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load the subjects' electrodes-to-ROIs dictionary...\n",
      "Loaded data from C:\\Users\\jz421\\Desktop\\GlobalLocal\\src\\analysis\\config\\subjects_electrodestoROIs_dict.json\n",
      "Dictionary loaded successfully. Ready to proceed!\n"
     ]
    }
   ],
   "source": [
    "# subjects = ['D0057','D0059', 'D0063', 'D0065', 'D0069', 'D0071', 'D0077', 'D0090', 'D0094', 'D0100', 'D0102', 'D0103', 'D0107A', 'D0110', 'D0116', 'D0117', 'D0121']\n",
    "\n",
    "# params - these will become input variables once i functionalize this stuff\n",
    "subjects = ['D0103']\n",
    "signal_times = [-1.0, 1.5]\n",
    "acc_trials_only = False\n",
    "error_trials_only = False\n",
    "stat_func = partial(ttest_ind, equal_var=False, nan_policy='omit')\n",
    "p_thresh = 0.2\n",
    "ignore_adjacency = 1 # ignore the channels dimension for clusters, just find clusters over frequency and time\n",
    "n_perm = 10\n",
    "n_jobs = 1\n",
    "freqs = np.arange(2, 200., 4.)\n",
    "n_cycles = freqs / 2\n",
    "return_itc = False\n",
    "time_bandwidth=10 \n",
    "spec_method = 'multitaper'\n",
    "average=False\n",
    "seed=None\n",
    "tails=2\n",
    "n_splits=2\n",
    "n_repeats=1\n",
    "random_state=42\n",
    "\n",
    "# load in subjects electrodes to rois dict. If it doesn't already exist, make it and then load it.\n",
    "config_dir = r'C:\\Users\\jz421\\Desktop\\GlobalLocal\\src\\analysis\\config'\n",
    "subjects_electrodestoROIs_dict = make_or_load_subjects_electrodes_to_ROIs_dict(subjects, task='GlobalLocal', LAB_root=None, save_dir=config_dir, \n",
    "                                                filename='subjects_electrodestoROIs_dict.json')\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "\n",
    "USER = os.path.basename(HOME)\n",
    "\n",
    "if os.name == 'nt':  # Windows\n",
    "    LAB_root = os.path.join(HOME, \"Box\", \"CoganLab\")\n",
    "elif sys.platform == 'darwin':  # macOS\n",
    "    LAB_root = os.path.join(HOME, \"Library\", \"CloudStorage\", \"Box-Box\", \"CoganLab\")\n",
    "else:  # Linux (cluster)\n",
    "    # Check if we're on the cluster by looking for /cwork directory\n",
    "    if os.path.exists(f\"/cwork/{USER}\"):\n",
    "        LAB_root = f\"/cwork/{USER}\"\n",
    "    else:\n",
    "        # Fallback for other Linux systems\n",
    "        LAB_root = os.path.join(HOME, \"CoganLab\")\n",
    "\n",
    "layout = get_data(\"GlobalLocal\", root=LAB_root)\n",
    "\n",
    "task='GlobalLocal'\n",
    "conditions = experiment_conditions.stimulus_big_letter_conditions # set this to whichever conditions you're running\n",
    "\n",
    "stimulus_locked = True  #toggle\n",
    "response_locked = not stimulus_locked\n",
    "\n",
    "if stimulus_locked:\n",
    "    # epochs_root_file = \"Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_4.0-8.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False\"\n",
    "    epochs_root_file = \"Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False\"\n",
    "    # epochs_root_file = \"Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_0.0-30.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False\"\n",
    "\n",
    "elif response_locked:\n",
    "    # epochs_root_file = \"Response_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_4.0-8.0_padLength_0.5s_stat_func_ttest_ind\"\n",
    "    epochs_root_file = \"Response_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind\"\n",
    "\n",
    "condition_names = list(conditions.keys()) # get the condition names as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "531435b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if conditions == experiment_conditions.stimulus_conditions:\n",
    "    conditions_save_name = 'stimulus_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_experiment_conditions:\n",
    "    conditions_save_name = 'stimulus_experiment_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_main_effect_conditions:\n",
    "    conditions_save_name = 'stimulus_main_effect_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_lwpc_conditions:\n",
    "    conditions_save_name = 'stimulus_lwpc_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_lwps_conditions:\n",
    "    conditions_save_name = 'stimulus_lwps_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_big_letter_conditions:\n",
    "    conditions_save_name = 'stimulus_big_letter_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_small_letter_conditions:\n",
    "    conditions_save_name = 'stimulus_small_letter_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_task_conditions:\n",
    "    conditions_save_name = 'stimulus_task_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_congruency_conditions:\n",
    "    conditions_save_name = 'stimulus_congruency_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.stimulus_switch_type_conditions:\n",
    "    conditions_save_name = 'stimulus_switch_type_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "\n",
    "elif conditions == experiment_conditions.response_conditions:\n",
    "    conditions_save_name = 'response_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.response_experiment_conditions:\n",
    "    conditions_save_name = 'response_experiment_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.response_big_letter_conditions:\n",
    "    conditions_save_name = 'response_big_letter_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.response_small_letter_conditions:\n",
    "    conditions_save_name = 'response_small_letter_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.response_task_conditions:\n",
    "    conditions_save_name = 'response_task_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.response_congruency_conditions:\n",
    "    conditions_save_name = 'response_congruency_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'\n",
    "elif conditions == experiment_conditions.response_switch_type_conditions:\n",
    "    conditions_save_name = 'response_switch_type_conditions' + '_' + epochs_root_file + '_' + str(len(subjects)) + '_' + 'subjects'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54dfc3",
   "metadata": {},
   "source": [
    "load stimulus significant channels. Compare ROI electrodes in next cell to these to see if they're included.\n",
    "\n",
    "maybe do response significant channels too/instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce4b49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded significant channels for subject D0103\n"
     ]
    }
   ],
   "source": [
    "sig_chans_per_subject = get_sig_chans_per_subject(subjects, epochs_root_file, task='GlobalLocal', LAB_root=None)\n",
    "\n",
    "# Now sig_chans_per_subject dictionary is populated with significant channels for each subject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c70bea6",
   "metadata": {},
   "source": [
    "get the significant electrodes across subjects for each ROI of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43db4297",
   "metadata": {},
   "source": [
    "dlPFC based on Yamagishi et al 2016 definition is G_front_middle, G_front_sup, S_front_inf, S_front_middle, S_front_sup\n",
    "ACC based on Destrieux et al 2010 definition is G_and_S_cingul-Ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48af7055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For subject D0057, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['RAI6', 'RAI12', 'RAI13', 'RAI14', 'RAI15', 'RAI16', 'RPI15', 'RPI14', 'RAMF10', 'RAMF11', 'RAMF12', 'RAMF13', 'RAMF14', 'RAIF11', 'RAIF12', 'RAIF13', 'RAIF14']\n",
      "For subject D0059, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['LMMF9', 'LMMF11', 'LMMF10', 'LMMF12', 'LPSF16']\n",
      "For subject D0063, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['LASF10', 'LASF11', 'LASF14', 'LASF15', 'LASF16', 'LMSF5', 'LMSF6', 'LMSF12', 'LPSF10', 'LPSF12', 'ROF16', 'RAI16', 'RAMF11', 'RAMF12', 'RAMF13', 'RAMF14', 'RMMF13', 'RMMF14', 'RAI4', 'RAI6', 'RAI5', 'RAI10', 'RAI11', 'RASF15', 'RASF16', 'RMSF8', 'RMSF9', 'RMSF10', 'RMSF11', 'RMSF12', 'RMSF7', 'RAMF8', 'RAMF9', 'RAMF10', 'RMMF9', 'RMMF10']\n",
      "For subject D0065, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['RASF13', 'RASF14', 'RASF15', 'RASF16', 'RMSF11', 'RMSF12', 'RMSF13', 'RMSF14', 'RI7']\n",
      "For subject D0069, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['LOF8']\n",
      "For subject D0071, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['RFO14', 'RFO16', 'RIA4', 'RIP6', 'RIA5', 'RIA11', 'RIA12', 'RIA13', 'RIA14', 'RIA16', 'RIP14', 'RIP15', 'RIP16']\n",
      "For subject D0077, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: []\n",
      "For subject D0090, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['RFO15', 'RIA6', 'RIA11', 'RIA12', 'RIA14', 'RIA15', 'RIA16', 'RIP7']\n",
      "For subject D0094, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['LFO12', 'LFO13', 'LFO15', 'LFO16', 'LFAM8', 'LFAM9', 'LFAM10', 'LFAM13', 'LFAM14', 'LFPM10', 'LFPM11', 'LFPM12', 'LPAS1', 'LIA16', 'LFAI3', 'LFAI4', 'LFAI5', 'LFPI10', 'LPAI9', 'LPAI10', 'LIA4', 'LIA5', 'LFAI9', 'LFAI10', 'LIA11', 'LIA12', 'LIA13', 'LIA14']\n",
      "For subject D0100, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: []\n",
      "For subject D0102, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['RFO13', 'RFO14', 'RFAM15', 'RFAI2', 'RFAI3']\n",
      "For subject D0103, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['LFAM8', 'LFAM9', 'LAI13', 'LAI14', 'LFAM15', 'LAI4', 'LAI7', 'LAI8', 'LAI18', 'LFO15', 'LFAI4']\n",
      "For subject D0107A, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['RFOA16', 'RFOA17', 'RFOA18', 'RFAI3', 'RIA5', 'RIA6', 'RFAM7', 'RFAM9', 'RFAM10', 'RFAM11', 'RFAM12', 'RFMM9', 'RFMM11', 'RFMM12', 'RFMM13', 'RFMM14', 'RFMM15', 'RFMM8', 'RIA12', 'RIA13', 'RIA14', 'RIA15', 'RIA16', 'RIA17', 'RIA18']\n",
      "For subject D0110, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['LASF10', 'LOF13', 'LOF15', 'LINS4', 'LINS5']\n",
      "For subject D0116, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['LFOA14', 'LFOA17', 'LFOA18', 'LFOP15', 'LFAM10', 'LFAM11', 'LFMM14', 'LAI12', 'LAI16', 'LFAM12', 'LAI7', 'LAI8', 'LFMI3', 'LFMI5', 'LFMI6', 'RFOA14', 'RFOA15', 'RFOA16', 'RFOP15', 'RFMM10', 'RFMM11', 'RFMM12', 'RAI7', 'RAI11', 'RAI12', 'RAI14', 'RAI15', 'RAI16', 'RAI17', 'RFMI4']\n",
      "For subject D0117, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['RMOF15', 'RINSA15', 'RINSA16', 'RLOF10', 'RLOF11', 'RLOF16', 'RINSA3', 'RINSA11', 'RINSA12', 'RINSA13', 'RINSA14']\n",
      "For subject D0121, G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes are: ['LFOA15', 'LFOP12', 'LFO5', 'LFO6', 'LFO8', 'LFO9', 'LFO12', 'LFO13', 'LFO14', 'LFO15', 'LFO16', 'LFO17', 'LFO18', 'LFA1', 'LFA2', 'LFP16', 'LFP17', 'LFPS1', 'LFPS2', 'LFPP1', 'LFOP13', 'LFOP15', 'LFOP16', 'LFOP18', 'LFA7', 'LFA8', 'LFAS6', 'LFAM8', 'LFAM9', 'LFA9', 'LFA10', 'LFA11', 'LFA12', 'LFAM10', 'LFAM11', 'LFAM12', 'LFMM8', 'LFMM9', 'LFMM10', 'LFMM11', 'LFMM12', 'LFAS7', 'LFAS8', 'LFAS9', 'LFAS10', 'LFMS7', 'LFMS9', 'LFMS10', 'LFMS11', 'LFMS12', 'LFPS6', 'LFPS7', 'LFPS9', 'LFPS10', 'LFPP7', 'LFPP9', 'LFPP10', 'LFMI4', 'LFMI5', 'LFMI8', 'LFMI9']\n",
      "Subject D0057 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0059 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0063 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0065 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0069 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0071 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0077 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0090 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0094 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0100 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0102 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0103 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: ['LFAM8', 'LFAM9', 'LAI13', 'LAI14', 'LAI4', 'LAI7', 'LAI8', 'LFAI4']\n",
      "Subject D0107A significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0110 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0116 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0117 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "Subject D0121 significant G_front_inf-Opercular, G_front_inf-Orbital, G_front_inf-Triangul, G_front_middle, G_front_sup, Lat_Fis-ant-Horizont, Lat_Fis-ant-Vertical, S_circular_insula_ant, S_circular_insula_sup, S_front_inf, S_front_middle, S_front_sup electrodes: []\n",
      "For subject D0057, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['RPIT1']\n",
      "For subject D0059, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['LPT13']\n",
      "For subject D0063, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['RMMT1', 'RMMT2']\n",
      "For subject D0065, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['RPMT1', 'RPIT1', 'RPIT2']\n",
      "For subject D0069, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: []\n",
      "For subject D0071, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['RTPI1', 'RO1', 'RO2', 'RO10']\n",
      "For subject D0077, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['RTPI2', 'ROPI5', 'ROPI6', 'ROPI8', 'ROPM9', 'ROPM11', 'ROPM12', 'ROPM1', 'ROPM8']\n",
      "For subject D0090, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['RTPO1', 'RTPI1', 'RTPI2']\n",
      "For subject D0094, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: []\n",
      "For subject D0100, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['LTOJ1', 'LTPI1', 'LOAI1', 'LOPI1', 'LOPI4', 'LOPM1', 'LTOJ3', 'LTOJ4', 'LOMM3', 'LOMM4', 'LOMM5', 'LOPM2', 'LTOJ13', 'LOAI12', 'LOAI13', 'LOAI14', 'LOAI15', 'LOMI8', 'LOMI9', 'LOMI11', 'LOMI12', 'LOPI9', 'LOPI10', 'LOAM15', 'LOMM15', 'LOPM10', 'LOMS13', 'LOMS14', 'LOMS15', 'LOPS9', 'LOPS11', 'LOPS12', 'LPPI14', 'LPPI15', 'LOMM13', 'LOMM14', 'LOMS11', 'LOMS12', 'LOPM8', 'LOPM9', 'LPPI7', 'LPPI8', 'LPPI9']\n",
      "For subject D0102, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['RTAI6', 'RTPI1']\n",
      "For subject D0103, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['LTLI3', 'LTPI2', 'LTPI3', 'LTPI4']\n",
      "For subject D0107A, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: ['RTPI3']\n",
      "For subject D0110, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: []\n",
      "For subject D0116, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: []\n",
      "For subject D0117, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: []\n",
      "For subject D0121, G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes are: []\n",
      "Subject D0057 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0059 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0063 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0065 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0069 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0071 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0077 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0090 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0094 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0100 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0102 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0103 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: ['LTPI2', 'LTPI3', 'LTPI4']\n",
      "Subject D0107A significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0110 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0116 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0117 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n",
      "Subject D0121 significant G_cuneus, G_and_S_occipital_inf, G_occipital_middle, G_occipital_sup, G_oc-temp_lat-fusifor, G_oc-temp_med-Lingual, Pole_occipital, S_calcarine, S_oc_middle_and_Lunatus, S_oc_sup_and_transversal, S_occipital_ant electrodes: []\n"
     ]
    }
   ],
   "source": [
    "# rois_dict = {\n",
    "#     'dlpfc': [\"G_front_middle\", \"G_front_sup\", \"S_front_inf\", \"S_front_middle\", \"S_front_sup\"],\n",
    "#     'acc': [\"G_and_S_cingul-Ant\", \"G_and_S_cingul-Mid-Ant\"],\n",
    "#     'parietal': [\"G_parietal_sup\", \"S_intrapariet_and_P_trans\", \"G_pariet_inf-Angular\", \"G_pariet_inf-Supramar\"],\n",
    "#     'lpfc': [\"G_front_inf-Opercular\", \"G_front_inf-Orbital\", \"G_front_inf-Triangul\", \"G_front_middle\", \"G_front_sup\", \"Lat_Fis-ant-Horizont\", \"Lat_Fis-ant-Vertical\", \"S_circular_insula_ant\", \"S_circular_insula_sup\", \"S_front_inf\", \"S_front_middle\", \"S_front_sup\"],\n",
    "#     'v1': [\"G_oc-temp_med-Lingual\", \"S_calcarine\", \"G_cuneus\"],\n",
    "#     'occ': [\"G_cuneus\", \"G_and_S_occipital_inf\", \"G_occipital_middle\", \"G_occipital_sup\", \"G_oc-temp_lat-fusifor\", \"G_oc-temp_med-Lingual\", \"Pole_occipital\", \"S_calcarine\", \"S_oc_middle_and_Lunatus\", \"S_oc_sup_and_transversal\", \"S_occipital_ant\"]\n",
    "# }\n",
    "\n",
    "# rois_dict = {\n",
    "#     'lpfc': [\"G_front_inf-Opercular\", \"G_front_inf-Orbital\", \"G_front_inf-Triangul\", \"G_front_middle\", \"G_front_sup\", \"Lat_Fis-ant-Horizont\", \"Lat_Fis-ant-Vertical\", \"S_circular_insula_ant\", \"S_circular_insula_sup\", \"S_front_inf\", \"S_front_middle\", \"S_front_sup\"]\n",
    "# }\n",
    "\n",
    "rois_dict = {\n",
    "    'lpfc': [\"G_front_inf-Opercular\", \"G_front_inf-Orbital\", \"G_front_inf-Triangul\", \"G_front_middle\", \"G_front_sup\", \"Lat_Fis-ant-Horizont\", \"Lat_Fis-ant-Vertical\", \"S_circular_insula_ant\", \"S_circular_insula_sup\", \"S_front_inf\", \"S_front_middle\", \"S_front_sup\"],\n",
    "    'occ': [\"G_cuneus\", \"G_and_S_occipital_inf\", \"G_occipital_middle\", \"G_occipital_sup\", \"G_oc-temp_lat-fusifor\", \"G_oc-temp_med-Lingual\", \"Pole_occipital\", \"S_calcarine\", \"S_oc_middle_and_Lunatus\", \"S_oc_sup_and_transversal\", \"S_occipital_ant\"]\n",
    "}\n",
    "\n",
    "rois = list(rois_dict.keys())\n",
    "all_electrodes_per_subject_roi, sig_electrodes_per_subject_roi = make_sig_electrodes_per_subject_and_roi_dict(rois_dict, subjects_electrodestoROIs_dict, sig_chans_per_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "280f6688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sig elecs: 82\n"
     ]
    }
   ],
   "source": [
    "print('total sig elecs:', sum(len(sig_chans_per_subject[sub]) for sub in sig_chans_per_subject))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e40e3c2",
   "metadata": {},
   "source": [
    "get number of sig and all electrodes per subject and across subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf3fd540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject D0103, ROI lpfc, Num of Sig Electrodes: 8, Num of All Electrodes: 11\n",
      "Subject D0103, ROI occ, Num of Sig Electrodes: 3, Num of All Electrodes: 4\n"
     ]
    }
   ],
   "source": [
    "for roi in rois:\n",
    "    for sub in subjects:\n",
    "        sig_elecs = sig_electrodes_per_subject_roi.get(roi, {}).get(sub, [])\n",
    "        all_elecs = all_electrodes_per_subject_roi.get(roi, {}).get(sub, [])\n",
    "        print(f\"Subject {sub}, ROI {roi}, Num of Sig Electrodes: {len(sig_elecs)}, Num of All Electrodes: {len(all_elecs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb5ec907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of significant lpfc electrodes across all subjects: 8\n",
      "Total number of lpfc electrodes across all subjects: 265\n",
      "Total number of significant occ electrodes across all subjects: 3\n",
      "Total number of occ electrodes across all subjects: 73\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "total_electrodes_info = calculate_total_electrodes(sig_electrodes_per_subject_roi, all_electrodes_per_subject_roi)\n",
    "for roi, counts in total_electrodes_info.items():\n",
    "    print(f\"Total number of significant {roi} electrodes across all subjects:\", counts['total_significant_electrodes'])\n",
    "    print(f\"Total number of {roi} electrodes across all subjects:\", counts['total_electrodes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459a911b",
   "metadata": {},
   "source": [
    "#### 1. For each electrode, make multitaper using all training trials, for both conditions to be compared (this can only do two conditions, can't do more rn)\n",
    "\n",
    "TODO: Need to loop over the trials and use some as training set, and have some held out as a test set that isn't used to find clusters. Maybe do this in the stats step though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e60e7a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing TFR data. Loading from: C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\spec\\multitaper\\subjects_tfr_objects\\stimulus_big_letter_conditions_Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False_1_subjects_multitaper\n"
     ]
    }
   ],
   "source": [
    "subjects_tfr_objects = load_or_make_subjects_tfr_objects(\n",
    "    layout=layout,\n",
    "    spec_method=spec_method,\n",
    "    conditions_save_name=conditions_save_name,\n",
    "    subjects=subjects,\n",
    "    conditions=conditions,\n",
    "    signal_times=signal_times,\n",
    "    freqs=freqs,\n",
    "    n_cycles=n_cycles,\n",
    "    time_bandwidth=time_bandwidth,\n",
    "    return_itc=return_itc,\n",
    "    n_jobs=n_jobs,\n",
    "    average=average,\n",
    "    acc_trials_only=acc_trials_only,\n",
    "    error_trials_only=error_trials_only \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f756323",
   "metadata": {},
   "source": [
    "#### 2. Find clusters that are significantly different between the two conditions, in the multitaper spectrogram  \n",
    "do this for all channels per subject, and then also for all channels in an roi across subjects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84da78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THIS TAKES FOREVER DON'T RUN IT, JUST FOR DEBUGGING SO I CAN PLOT THE SIG CLUSTERS\n",
    "\n",
    "# # For per-subject analysis (no electrode filtering needed)\n",
    "# sig_elec_masks_per_subject, sig_elec_pvals_per_subject = get_sig_tfr_differences_per_subject(subjects_tfr_objects=subjects_tfr_objects, condition_names=condition_names, stat_func=stat_func, p_thresh=p_thresh, n_perm=n_perm, ignore_adjacency=ignore_adjacency, n_jobs=n_jobs, seed=seed, tails=tails)\n",
    "\n",
    "# all_elec_masks_per_subject, all_elec_pvals_per_subject = get_sig_tfr_differences_per_subject(subjects_tfr_objects=subjects_tfr_objects, condition_names=condition_names, stat_func=stat_func, p_thresh=p_thresh, n_perm=n_perm, ignore_adjacency=ignore_adjacency, n_jobs=n_jobs, seed=seed, tails=tails)\n",
    "\n",
    "# # For per-ROI analysis (with electrode filtering)\n",
    "# sig_elec_masks_per_roi, sig_elec_pvals_per_roi = get_sig_tfr_differences_per_roi(subjects_tfr_objects=subjects_tfr_objects, electrodes_per_subject_roi=sig_electrodes_per_subject_roi, condition_names=condition_names, stat_func=stat_func, p_thresh=p_thresh, n_perm=n_perm, ignore_adjacency=ignore_adjacency, n_jobs=n_jobs, seed=seed, tails=tails)\n",
    "\n",
    "# all_elec_masks_per_roi, all_elec_pvals_per_roi = get_sig_tfr_differences_per_roi(subjects_tfr_objects=subjects_tfr_objects, electrodes_per_subject_roi=all_electrodes_per_subject_roi, condition_names=condition_names, stat_func=stat_func, p_thresh=p_thresh, n_perm=n_perm, ignore_adjacency=ignore_adjacency, n_jobs=n_jobs, seed=seed, tails=tails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fa06aa",
   "metadata": {},
   "source": [
    "plot these clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a83c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Update the mean differences to be per subject and per roi, currently copied over from wavelet_differences.\n",
    "# first_sub = subjects[0]\n",
    "# first_condition = list(subjects_tfr_objects[first_sub].keys())[0]\n",
    "# ch_names = subjects_tfr_objects[first_sub][first_condition].ch_names\n",
    "# times = subjects_tfr_objects[first_sub][first_condition].times\n",
    "# freqs = subjects_tfr_objects[first_sub][first_condition].freqs\n",
    "\n",
    "# subjects_tfr_objects_save_dir = os.path.join(layout.root, 'derivatives', 'spec', spec_method, 'subjects_tfr_objects')\n",
    "\n",
    "# # Now plot the mask pages:\n",
    "# for sub in subjects:\n",
    "#     sig_elecs_mask = sig_elec_masks_per_subject[sub]\n",
    "#     sig_elecs_mask_pages = plot_mask_pages(sig_elecs_mask,\n",
    "#                     ch_names,\n",
    "#                     times=times,\n",
    "#                     freqs=freqs,\n",
    "#                     channels_per_page=60,\n",
    "#                     grid_shape=(6, 10),\n",
    "#                     cmap=parula_map,\n",
    "#                     title_prefix=f\"{sub} \",\n",
    "#                     log_freq=True,\n",
    "#                     show=False)\n",
    "\n",
    "#     # Save each page as a separate figure file:\n",
    "#     for i, fig in enumerate(sig_elecs_mask_pages):\n",
    "#         fig_name = f\"{sub}_sig_elecs_sig_{spec_method}_clusters_{conditions_save_name}_page_{i+1}.png\"\n",
    "#         fig_pathname = os.path.join(subjects_tfr_objects_save_dir, fig_name)\n",
    "#         fig.savefig(fig_pathname, bbox_inches='tight')\n",
    "#         print(\"Saved figure:\", fig_name)\n",
    "\n",
    "#     all_elecs_mask = all_elec_masks_per_subject[sub]\n",
    "#     all_elecs_mask_pages = plot_mask_pages(all_elecs_mask,\n",
    "#                         ch_names,\n",
    "#                         times=times,\n",
    "#                         freqs=freqs,\n",
    "#                         channels_per_page=60,\n",
    "#                         grid_shape=(6, 10),\n",
    "#                         cmap=parula_map,\n",
    "#                         title_prefix=f\"{sub} \",\n",
    "#                         log_freq=True,\n",
    "#                         show=False)\n",
    "\n",
    "#     # Save each page as a separate figure file:\n",
    "#     for i, fig in enumerate(all_elecs_mask_pages):\n",
    "#         fig_name = f\"{sub}_all_elecs_sig_{spec_method}_clusters_{conditions_save_name}_page_{i+1}.png\"\n",
    "#         fig_pathname = os.path.join(subjects_tfr_objects_save_dir, fig_name)\n",
    "#         fig.savefig(fig_pathname, bbox_inches='tight')\n",
    "#         print(\"Saved figure:\", fig_name)\n",
    "\n",
    "# for roi in rois:\n",
    "#     sig_elecs_roi_mask = sig_elec_masks_per_roi[roi]\n",
    "#     sig_elecs_roi_mask_pages = plot_mask_pages(sig_elecs_roi_mask,\n",
    "#                     ch_names,\n",
    "#                     times=times,\n",
    "#                     freqs=freqs,\n",
    "#                     channels_per_page=60,\n",
    "#                     grid_shape=(6, 10),\n",
    "#                     cmap=parula_map,\n",
    "#                     title_prefix=f\"{sub} {roi} \",\n",
    "#                     log_freq=True,\n",
    "#                     show=False)\n",
    "\n",
    "#     # Save each page as a separate figure file:\n",
    "#     for i, fig in enumerate(sig_elecs_roi_mask_pages):\n",
    "#         fig_name = f\"{sub}_{roi}_sig_elecs_sig_{spec_method}_clusters_{conditions_save_name}_page_{i+1}.png\"\n",
    "#         fig_pathname = os.path.join(subjects_tfr_objects_save_dir, fig_name)\n",
    "#         fig.savefig(fig_pathname, bbox_inches='tight')\n",
    "#         print(\"Saved figure:\", fig_name)\n",
    "\n",
    "#     all_elecs_roi_mask = all_elec_masks_per_roi[roi]\n",
    "#     all_elecs_roi_mask_pages = plot_mask_pages(all_elecs_roi_mask,\n",
    "#                     ch_names,\n",
    "#                     times=times,\n",
    "#                     freqs=freqs,\n",
    "#                     channels_per_page=60,\n",
    "#                     grid_shape=(6, 10),\n",
    "#                     cmap=parula_map,\n",
    "#                     title_prefix=f\"{sub} {roi} \",\n",
    "#                     log_freq=True,\n",
    "#                     show=False)\n",
    "\n",
    "#     # Save each page as a separate figure file:\n",
    "#     for i, fig in enumerate(all_elecs_roi_mask_pages):\n",
    "#         fig_name = f\"{sub}_{roi}_all_elecs_sig_{spec_method}_clusters_{conditions_save_name}_page_{i+1}.png\"\n",
    "#         fig_pathname = os.path.join(subjects_tfr_objects_save_dir, fig_name)\n",
    "#         fig.savefig(fig_pathname, bbox_inches='tight')\n",
    "#         print(\"Saved figure:\", fig_name)\n",
    "            \n",
    "# # # get the mean differences themselves and plot them\n",
    "# # mean_diff_inc_vs_con = mean_diff(incongruent_spec._data, congruent_spec._data, axis=0)\n",
    "# # mean_diff_switch_vs_repeat = mean_diff(switch_spec._data, repeat_spec._data, axis=0)\n",
    "\n",
    "# # # Now, plot the mean differences directly:\n",
    "# # congruency_mean_diff_pages = plot_mask_pages(\n",
    "# #     mean_diff_inc_vs_con,\n",
    "# #     incongruent_spec.ch_names,\n",
    "# #     times=incongruent_spec.times,\n",
    "# #     freqs=incongruent_spec.freqs,\n",
    "# #     grid_shape=(6, 10),\n",
    "# #     cmap=parula_map,  # play with color maps\n",
    "# #     title_prefix=f\"{sub} Mean Inc-Con Diff: \",\n",
    "# #     log_freq=True,\n",
    "# #     show=False\n",
    "# # )\n",
    "\n",
    "# # # Save each page as a separate figure file:\n",
    "# # for i, fig in enumerate(congruency_mean_diff_pages):\n",
    "# #     if rescaled:\n",
    "# #         fig_name = f\"{sub}_inc-con_mean_diff_multitaper_{conditions_and_output_names_and_events['incongruent']['output_name']}-{conditions_and_output_names_and_events['congruent']['output_name']}_rescaled_page_{i+1}.jpg\"\n",
    "# #     else:\n",
    "# #         fig_name = f\"{sub}_inc-con_mean_diff_multitaper_{conditions_and_output_names_and_events['incongruent']['output_name']}-{conditions_and_output_names_and_events['congruent']['output_name']}_uncorrected_page_{i+1}.jpg\"\n",
    "# #     fig_pathname = os.path.join(save_dir, fig_name)\n",
    "# #     fig.savefig(fig_pathname, bbox_inches='tight')\n",
    "# #     print(\"Saved figure:\", fig_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c66f9d5",
   "metadata": {},
   "source": [
    "let's try creating roi labeled arrays and then masking these and decoding using them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4c8d35",
   "metadata": {},
   "source": [
    "make roi_labeled_arrays, a dict where the keys are rois and the values are LabeledArrays. Index the same way as a nested dict. Use .labels to get labels from current level.\n",
    "\n",
    "\n",
    "#### roi_labeled_arrays structure\n",
    "- **roi1**: ROI name, string\n",
    "  - **conditions**: condition name\n",
    "    - **trials**: This is the maximal number of trials across subjects for any condition, filled with nans for subjects who don't have this many trials\n",
    "      - **channels**: This is the number of channels in the roi, each channel is labeled as subject-channel name. Concatenated across subjects.\n",
    "        - **freqs**: This is the number of frequencies in the roi, each frequency is labeled as frequency name.\n",
    "          - **samples**: 1 sample as a float. This is the time for this sample.\n",
    "\n",
    "- **roi2**: ROI name, string\n",
    "  - **conditions**: condition name\n",
    "    - **trials**: This is the maximal number of trials across subjects for any condition, filled with nans for subjects who don't have this many trials\n",
    "      - **channels**: This is the number of channels in the roi, each channel is labeled as subject-channel name. Concatenated across subjects.\n",
    "        - **freqs**: This is the number of frequencies in the roi, each frequency is labeled as frequency name.\n",
    "          - **samples**: 1 sample as a float. This is the time for this sample.\n",
    "\n",
    "- **roiX**: ROI name, string\n",
    "  - **conditions**: condition name\n",
    "    - **trials**: This is the maximal number of trials across subjects for any condition, filled with nans for subjects who don't have this many trials\n",
    "      - **channels**: This is the number of channels in the roi, each channel is labeled as subject-channel name. Concatenated across subjects.\n",
    "        - **freqs**: This is the number of frequencies in the roi, each frequency is labeled as frequency name.\n",
    "          - **samples**: 1 sample as a float. This is the time for this sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08b236c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROI 'lpfc': Maximum trials per condition:\n",
      "  Condition 'Stimulus_bigS': 176 trials from subjects ['D0103']\n",
      "  Condition 'Stimulus_bigH': 183 trials from subjects ['D0103']\n",
      "in roi lpfc, subject D0103 has 176 trials for condition Stimulus_bigS\n",
      "in roi lpfc, subject D0103 has 183 trials for condition Stimulus_bigH\n",
      "Subject D0103, ROI lpfc, LabeledArray shape: (2, 183, 11, 3, 142)\n",
      "ROI 'occ': Maximum trials per condition:\n",
      "  Condition 'Stimulus_bigS': 176 trials from subjects ['D0103']\n",
      "  Condition 'Stimulus_bigH': 183 trials from subjects ['D0103']\n",
      "in roi occ, subject D0103 has 176 trials for condition Stimulus_bigS\n",
      "in roi occ, subject D0103 has 183 trials for condition Stimulus_bigH\n",
      "Subject D0103, ROI occ, LabeledArray shape: (2, 183, 4, 3, 142)\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "\n",
    "# choose whether to use sig elecs or all elecs\n",
    "electrodes = all_electrodes_per_subject_roi # toggle this to sig_electrodes_per_subject_roi if just using sig elecs, or electrodes_per_subject_roi if using all elecs\n",
    "\n",
    "if electrodes == all_electrodes_per_subject_roi:\n",
    "    elec_string_to_add_to_filename = 'all_elecs'\n",
    "elif electrodes == sig_electrodes_per_subject_roi:\n",
    "    elec_string_to_add_to_filename = 'sig_elecs'\n",
    "else:\n",
    "    elec_string_to_add_to_filename = None\n",
    "\n",
    "roi_labeled_arrays = put_data_in_labeled_array_per_roi_subject(\n",
    "    subjects_tfr_objects,\n",
    "    condition_names,\n",
    "    rois,\n",
    "    subjects,\n",
    "    electrodes, \n",
    "    obs_axs=0,  # Trials dimension (ignoring the conditions dimension for now cuz we iterate over it)\n",
    "    chans_axs=1,  # Channels dimension\n",
    "    freq_axs=2,\n",
    "    time_axs=3,   # Time dimension\n",
    "    random_state=42  # For reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e04536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# move all this into decoding.py once i get it working\n",
    "def balance_and_decode_with_tfr_masking(\n",
    "    X_train_raw, y_train, X_test_raw, y_test,\n",
    "    train_indices, test_indices, \n",
    "    roi_labeled_array, condition_names, obs_axs, chans_axs,\n",
    "    stat_func, p_thresh, n_perm,\n",
    "    Decoder, cats, balance_method, \n",
    "    n_bootstraps, random_state,\n",
    "    ignore_adjacency=1, seed=42, tails=2\n",
    "):\n",
    "    \"\"\"\n",
    "    Balance data and decode with TFR cluster masking.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train_raw : np.ndarray\n",
    "        Raw training data (trials, channels, freqs, times)\n",
    "    y_train : np.ndarray\n",
    "        Training labels\n",
    "    X_test_raw : np.ndarray\n",
    "        Raw test data\n",
    "    y_test : np.ndarray\n",
    "        Test labels\n",
    "    train_indices : np.ndarray\n",
    "        Indices of training trials in the original roi_labeled_array\n",
    "    test_indices : np.ndarray\n",
    "        Indices of test trials\n",
    "    roi_labeled_array : LabeledArray\n",
    "        The full roi labeled array for extracting channel info\n",
    "    condition_names : list\n",
    "        List of condition names to compare\n",
    "    obs_axs : int\n",
    "        Axis of the roi_labeled_array that contains trial labels\n",
    "    chans_axs : int\n",
    "        Axis of the roi_labeled_array that contains channel labels\n",
    "    stat_func : callable\n",
    "        Statistical function to use for permutation cluster test\n",
    "    p_thresh : float\n",
    "        P-value threshold for permutation cluster test\n",
    "    n_perm : int\n",
    "        Number of permutations for permutation cluster test\n",
    "    Decoder : class\n",
    "        Decoder class to use for decoding\n",
    "    cats : list\n",
    "        List of category names\n",
    "    balance_method : str\n",
    "        Method to balance data ('subsample' or 'pad_with_nans')\n",
    "    n_bootstraps : int\n",
    "        Number of bootstraps for decoding\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "    ignore_adjacency : int\n",
    "        Number of adjacent channels to ignore when computing clusters\n",
    "    seed : int\n",
    "        Random seed for permutation cluster test\n",
    "    tails : int\n",
    "        Number of tails for permutation cluster test\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get channel labels from the labeled array as a list\n",
    "    channel_labels = roi_labeled_array.labels[chans_axs+1] # add 1 to the chans axs because of the conditions axis that got addd\n",
    "    \n",
    "    # Step 1: Create training-only TFR masks\n",
    "    channel_masks = compute_tfr_masks_from_roi_labeled_array(\n",
    "        roi_labeled_array, train_indices, condition_names, \n",
    "        obs_axs, chans_axs,\n",
    "        stat_func, p_thresh, n_perm, \n",
    "        ignore_adjacency, seed, tails\n",
    "    )\n",
    "    \n",
    "    # Step 2: Apply masks and flatten (now passing channel_labels)\n",
    "    X_train = apply_tfr_masks_and_flatten(X_train_raw, channel_masks, channel_labels)\n",
    "    X_test = apply_tfr_masks_and_flatten(X_test_raw, channel_masks, channel_labels)\n",
    "    \n",
    "    # Step 3: Balance data - wait hold on, is this not already balanced using concatenate_and_balance_data_for_decoding\n",
    "    if balance_method == 'subsample':\n",
    "        # Remove NaN trials\n",
    "        valid_train = ~np.isnan(X_train).any(axis=1)\n",
    "        X_train = X_train[valid_train]\n",
    "        y_train = y_train[valid_train]\n",
    "        \n",
    "        # Subsample to balance classes\n",
    "        X_train, y_train = subsample_to_balance(X_train, y_train, random_state) # this function doesn't exist\n",
    "    \n",
    "    # Step 4: Decode\n",
    "    decoder = Decoder(cats, 0.80, n_splits=1, n_repeats=1, oversample=(balance_method==balance_method))\n",
    "    decoder.fit(X_train, y_train)\n",
    "    preds = decoder.predict(X_test)\n",
    "    \n",
    "    return confusion_matrix(y_test, preds)\n",
    "\n",
    "\n",
    "def compute_tfr_masks_from_roi_labeled_array(\n",
    "    roi_labeled_array, train_indices, condition_names,\n",
    "    obs_axs, chans_axs, stat_func, p_thresh, n_perm, \n",
    "    ignore_adjacency=1, seed=42, tails=2\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute significant TFR clusters using only training trials from roi_labeled_array.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    channel_masks : dict\n",
    "        {channel_label: mask_array} where mask is (n_freqs, n_times)\n",
    "    \"\"\"\n",
    "    channel_masks = {}\n",
    "    \n",
    "    # Get channel labels from the labeled array\n",
    "    channel_labels = roi_labeled_array.labels[chans_axs+1]\n",
    "    \n",
    "    # Split training data by condition\n",
    "    train_data_by_condition = {}\n",
    "    for cond in condition_names:  # hmm the stats only work for two conditions, so just do two conditions for now. Can expand to >2 conditions in the future, would just need to do ANOVA i think instead of time perm cluster.\n",
    "        # Extract training trials for this condition\n",
    "        cond_data = roi_labeled_array[cond]  # Shape: (trials, channels, freqs, times). Test this!\n",
    "        cond_train_data = np.take(cond_data, train_indices, axis=obs_axs) # TODO: keep going through this code 4:45 on 8/1 - huh? this is a 4d array, check what train_indices is. Should grab along the trials axis. Maybe do np.take(train_indices, axis=obs_axs) to be safe.\n",
    "        train_data_by_condition[cond] = cond_train_data\n",
    "    \n",
    "    # For each channel, compute significant clusters\n",
    "    n_channels = len(channel_labels)\n",
    "    for ch_idx in range(n_channels):\n",
    "        ch_label = channel_labels[ch_idx]\n",
    "        \n",
    "        # Get data for this channel across conditions\n",
    "        cond_data_this_chan = {}\n",
    "        for cond in condition_names:\n",
    "            cond_data_this_chan[cond] = np.take(train_data_by_condition[cond], ch_idx, axis=chans_axs)\n",
    "        \n",
    "        if cond_data_this_chan.keys() != 2:\n",
    "            raise ValueError(\"For now, just doing perm test instead of ANOVA, so this will only work for two conditions, sorry!\")\n",
    "        \n",
    "        # TODO: Once I implement ANOVA, don't hard code cond1 and cond2 data, just use the cond_data_this_chan dict and feed it into some get_sig_tfr_differences function that implements ANOVA instead of time perm cluster. \n",
    "        cond1_data = cond_data_this_chan[condition_names[0]]\n",
    "        cond2_data = cond_data_this_chan[condition_names[1]]\n",
    "        # Run time perm cluster (TODO: implement ANOVA too as another option for when i'm decoding more than two conditions)\n",
    "        if len(cond1_data) > 0 and len(cond2_data) > 0:\n",
    "            mask, _ = get_sig_tfr_differences(\n",
    "                cond1_data, cond2_data,\n",
    "                stat_func=stat_func,\n",
    "                p_thresh=p_thresh,\n",
    "                n_perm=n_perm,\n",
    "                axis=obs_axs,  # trials axis\n",
    "                ignore_adjacency=ignore_adjacency,\n",
    "                seed=seed,\n",
    "                tails=tails\n",
    "            )\n",
    "            channel_masks[ch_label] = mask\n",
    "        else:\n",
    "            # No data for comparison\n",
    "            channel_masks[ch_label] = np.zeros(\n",
    "                (cond1_data.shape[1], cond1_data.shape[2]), dtype=bool\n",
    "            )\n",
    "            raise ValueError(f\"cond 1 has {len(cond1_data)} trials and cond 2 has {len(cond2_data)} trials. Can't decode! \")\n",
    "    \n",
    "    return channel_masks\n",
    "\n",
    "\n",
    "def apply_tfr_masks_and_flatten(data, obs_axs, chans_axs, channel_masks, channel_labels):\n",
    "    \"\"\"\n",
    "    Apply channel-specific TFR masks and flatten to create feature matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : np.ndarray\n",
    "        Shape: (n_trials, n_channels, n_freqs, n_times)\n",
    "    obs_axs : int\n",
    "        Axis of the data that contains trial labels\n",
    "    chans_axs : int\n",
    "        Axis of the data that contains channel labels\n",
    "    channel_masks : dict\n",
    "        {channel_label: mask_array} where channel_label is like \"D0103-LPFC1\"\n",
    "    channel_labels : list\n",
    "        List of channel labels in the same order as data's channel dimension\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    feature_matrix : np.ndarray\n",
    "        Shape: (n_trials, n_features)\n",
    "    \"\"\"\n",
    "    n_trials, n_channels = data.shape[obs_axs], data.shape[chans_axs]\n",
    "    feature_vectors = []\n",
    "    \n",
    "    # Iterate through each channel\n",
    "    for ch_idx in range(n_channels):\n",
    "        # Extract this channel's data for all trials\n",
    "        # Shape changes from (n_trials, n_channels, n_freqs, n_times) \n",
    "        # to (n_trials, n_freqs, n_times)\n",
    "        channel_data = np.take(data, ch_idx, chans_axs)\n",
    "        \n",
    "        # Get the label for this channel index\n",
    "        ch_label = channel_labels[ch_idx]  # e.g., \"D0103-LPFC1\"\n",
    "        \n",
    "        # Check if we have a mask for this channel\n",
    "        if ch_label in channel_masks:\n",
    "            # Get the boolean mask (n_freqs, n_times)\n",
    "            mask = channel_masks[ch_label]\n",
    "            \n",
    "            # Apply mask to each trial's data\n",
    "            # channel_data[:, mask] selects only the True positions in mask, because mask is n freqs x n times\n",
    "            # This reduces (n_trials, n_freqs, n_times) to (n_trials, n_significant_features)\n",
    "            masked_features = channel_data[:, mask]\n",
    "            \n",
    "            # Note: reshape(n_trials, -1) is redundant here since it's already 2D\n",
    "            # masked_features is already (n_trials, n_features)\n",
    "        else:\n",
    "            # No mask for this channel - use all time-frequency points\n",
    "            # Flatten from (n_trials, n_freqs, n_times) to (n_trials, n_freqs*n_times)\n",
    "            masked_features = channel_data.reshape(n_trials, -1)\n",
    "        \n",
    "        # Add this channel's features to our list\n",
    "        feature_vectors.append(masked_features)\n",
    "    \n",
    "    # Concatenate all channels' features horizontally\n",
    "    # If channel 1 has 50 features and channel 2 has 30 features,\n",
    "    # result will have 80 features total\n",
    "    feature_matrix = np.concatenate(feature_vectors, axis=chans_axs)\n",
    "    return feature_matrix\n",
    "\n",
    "def get_and_plot_confusion_matrix_for_rois_tfr_cluster(\n",
    "    roi_labeled_arrays, rois, condition_comparison, strings_to_find, \n",
    "    save_dir, stat_func, p_thresh=0.05, n_perm=100,\n",
    "    time_interval_name=None, other_string_to_add=None, \n",
    "    n_splits=5, n_repeats=5, obs_axs=0, \n",
    "    balance_method='subsample', random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Modified version that uses TFR cluster masking.\n",
    "    \"\"\"\n",
    "    confusion_matrices = {}\n",
    "    \n",
    "    for roi in rois:\n",
    "        print(f\"Processing ROI: {roi}\")\n",
    "        \n",
    "        # Get data and labels\n",
    "        concatenated_data, labels, cats = concatenate_and_balance_data_for_decoding(\n",
    "            roi_labeled_arrays, roi, strings_to_find, obs_axs, balance_method, random_state\n",
    "        )\n",
    "        \n",
    "        # Set up cross-validation\n",
    "        all_cms = []\n",
    "        \n",
    "        for repeat in range(n_repeats):\n",
    "            repeat_seed = random_state + repeat * 1000\n",
    "            skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=repeat_seed)\n",
    "            \n",
    "            fold_cms = []\n",
    "            \n",
    "            for fold_idx, (train_indices, test_indices) in enumerate(skf.split(concatenated_data, labels)):\n",
    "                print(f\"  Repeat {repeat+1}/{n_repeats}, Fold {fold_idx+1}/{n_splits}\")\n",
    "                \n",
    "                # Get train/test data\n",
    "                X_train_raw = concatenated_data[train_indices]\n",
    "                X_test_raw = concatenated_data[test_indices]\n",
    "                y_train = labels[train_indices]\n",
    "                y_test = labels[test_indices]\n",
    "                \n",
    "                # Balance and decode with TFR masking\n",
    "                cm = balance_and_decode_with_tfr_masking(\n",
    "                    X_train_raw, y_train, X_test_raw, y_test,\n",
    "                    train_indices, test_indices,\n",
    "                    roi_labeled_arrays[roi], list(roi_labeled_arrays[roi].keys()), obs_axs, chans_axs,\n",
    "                    stat_func, p_thresh, n_perm,\n",
    "                    Decoder, cats, balance_method, \n",
    "                    1, repeat_seed + fold_idx\n",
    "                )\n",
    "                \n",
    "                fold_cms.append(cm)\n",
    "            \n",
    "            # Sum across folds\n",
    "            repeat_cm = np.sum(fold_cms, axis=0)\n",
    "            all_cms.append(repeat_cm)\n",
    "        \n",
    "        # Average across repeats\n",
    "        final_cm = np.mean(all_cms, axis=0)\n",
    "        confusion_matrices[roi] = final_cm\n",
    "        \n",
    "        # Plot (your existing plotting code)\n",
    "        # ...\n",
    "    \n",
    "    return confusion_matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def9876",
   "metadata": {},
   "source": [
    "below code is older approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a15cb79",
   "metadata": {},
   "source": [
    "decoding functions that concatenate the significant cluster mask for each electrode in an roi, made using training data, and test on just the concatenated significant cluster masks in an roi on test data  \n",
    "commenting this out cuz i don't understand it, will build my own code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f0b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collect_roi_data(subjects_tfr_objects, electrodes_per_subject_roi, roi, subjects, condition_names):\n",
    "#     \"\"\"\n",
    "#     Collect all data for a specific ROI across subjects and conditions.\n",
    "    \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     all_data : np.array\n",
    "#         Shape: (n_trials, n_channels, n_freqs, n_time)\n",
    "#     trial_labels : list\n",
    "#         Labels for each trial (e.g., \"D0057_r25_trial0\")\n",
    "#     condition_labels : list\n",
    "#         Condition name for each trial\n",
    "#     channel_labels : list\n",
    "#         Channel names for this ROI\n",
    "#     freq_labels : list\n",
    "#         Frequency values\n",
    "#     time_labels : list\n",
    "#         Time values\n",
    "#     \"\"\"\n",
    "#     all_data_list = []\n",
    "#     trial_labels = []\n",
    "#     condition_labels = []\n",
    "#     channel_labels = None\n",
    "#     freq_labels = None\n",
    "#     time_labels = None\n",
    "    \n",
    "#     for sub in subjects:\n",
    "#         roi_electrodes = electrodes_per_subject_roi.get(roi, {}).get(sub, [])\n",
    "#         if not roi_electrodes:\n",
    "#             continue\n",
    "            \n",
    "#         for cond_name in condition_names:\n",
    "#             tfr = subjects_tfr_objects[sub][cond_name]\n",
    "            \n",
    "#             # Get indices of ROI electrodes\n",
    "#             electrode_indices = []\n",
    "#             for elec in roi_electrodes:\n",
    "#                 if elec in tfr.ch_names:\n",
    "#                     electrode_indices.append(tfr.ch_names.index(elec))\n",
    "            \n",
    "#             if not electrode_indices:\n",
    "#                 continue\n",
    "            \n",
    "#             # Extract data for these electrodes\n",
    "#             data = tfr.data[:, electrode_indices, :, :]  # (trials, channels, freqs, time)\n",
    "            \n",
    "#             # Add to lists\n",
    "#             for trial_idx in range(len(data)):\n",
    "#                 all_data_list.append(data[trial_idx])\n",
    "#                 trial_labels.append(f\"{sub}_{cond_name}_trial{trial_idx}\")\n",
    "#                 condition_labels.append(cond_name)\n",
    "            \n",
    "#             # Set labels if not set\n",
    "#             if channel_labels is None:\n",
    "#                 channel_labels = [tfr.ch_names[i] for i in electrode_indices]\n",
    "#             if freq_labels is None:\n",
    "#                 freq_labels = tfr.freqs.tolist()\n",
    "#             if time_labels is None:\n",
    "#                 time_labels = tfr.times.tolist()\n",
    "    \n",
    "#     all_data = np.array(all_data_list) if all_data_list else np.array([])\n",
    "    \n",
    "#     return all_data, trial_labels, condition_labels, channel_labels, freq_labels, time_labels\n",
    "\n",
    "\n",
    "# def create_train_tfr_subset(subjects_tfr_objects, train_trials, subjects, condition_names):\n",
    "#     \"\"\"\n",
    "#     Create a subset of TFR objects containing only training trials.\n",
    "    \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     train_tfrs : dict\n",
    "#         Deep copy of TFR objects filtered to only training trials\n",
    "#     \"\"\"\n",
    "#     train_tfrs = {}\n",
    "    \n",
    "#     for sub in subjects:\n",
    "#         train_tfrs[sub] = {}\n",
    "#         for cond in condition_names:\n",
    "#             # Find training trials for this subject/condition\n",
    "#             train_trial_indices = []\n",
    "#             for trial_label in train_trials:\n",
    "#                 if trial_label.startswith(f\"{sub}_{cond}_\"):\n",
    "#                     trial_num = int(trial_label.split('_trial')[1])\n",
    "#                     train_trial_indices.append(trial_num)\n",
    "            \n",
    "#             if train_trial_indices:\n",
    "#                 train_tfrs[sub][cond] = subjects_tfr_objects[sub][cond][train_trial_indices]\n",
    "#             else:\n",
    "#                 # Create empty TFR\n",
    "#                 train_tfrs[sub][cond] = subjects_tfr_objects[sub][cond][:0]\n",
    "    \n",
    "#     return train_tfrs\n",
    "\n",
    "\n",
    "# def create_feature_mask(train_masks, electrodes_per_subject_roi, roi, subjects, \n",
    "#                        channel_labels, freq_labels, time_labels, tfr):\n",
    "#     \"\"\"\n",
    "#     Create a feature mask based on significant clusters from training data.\n",
    "    \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     feature_mask : np.array\n",
    "#         Boolean mask of shape (n_channels, n_freqs, n_times)\n",
    "#     \"\"\"\n",
    "#     feature_mask = np.zeros((len(channel_labels), len(freq_labels), len(time_labels)), dtype=bool)\n",
    "    \n",
    "#     for sub in subjects:\n",
    "#         if sub not in train_masks:\n",
    "#             continue\n",
    "            \n",
    "#         roi_electrodes = electrodes_per_subject_roi.get(roi, {}).get(sub, [])\n",
    "        \n",
    "#         for elec_name in roi_electrodes:\n",
    "#             if elec_name in tfr.ch_names and elec_name in channel_labels:\n",
    "#                 orig_elec_idx = tfr.ch_names.index(elec_name)\n",
    "#                 la_elec_idx = channel_labels.index(elec_name)\n",
    "                \n",
    "#                 if orig_elec_idx < train_masks[sub].shape[0]:\n",
    "#                     feature_mask[la_elec_idx, :, :] = train_masks[sub][orig_elec_idx, :, :]\n",
    "    \n",
    "#     return feature_mask\n",
    "\n",
    "\n",
    "# def extract_and_clean_features(train_data, test_data, feature_mask):\n",
    "#     \"\"\"\n",
    "#     Extract features using mask and handle NaN values.\n",
    "    \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     X_train : np.array\n",
    "#         Training features with NaNs imputed\n",
    "#     X_test : np.array\n",
    "#         Test features with NaNs imputed\n",
    "#     \"\"\"\n",
    "#     # Reshape to (n_trials, n_features_total)\n",
    "#     # MODIFICATION: Use .data to get the raw NumPy array\n",
    "#     X_train = np.asarray(train_data).reshape(len(train_data), -1)\n",
    "#     X_test = np.asarray(test_data).reshape(len(test_data), -1)\n",
    "    \n",
    "#     # Apply feature mask\n",
    "#     mask_flat = feature_mask.flatten()\n",
    "#     X_train = X_train[:, mask_flat]\n",
    "#     X_test = X_test[:, mask_flat]\n",
    "    \n",
    "#     # Handle NaN values (Ahhhh this is wrong, replace either with what james does or do mixup)\n",
    "#     col_mean = np.zeros(X_train.shape[1])\n",
    "#     if np.any(np.isnan(X_train)):\n",
    "#         col_mean = np.nanmean(X_train, axis=0)\n",
    "#         nan_mask = np.isnan(X_train)\n",
    "#         X_train[nan_mask] = np.take(col_mean, np.where(nan_mask)[1])\n",
    "    \n",
    "#     if np.any(np.isnan(X_test)):\n",
    "#         nan_mask = np.isnan(X_test)\n",
    "#         # Use the mean calculated from the training data to fill NaNs in the test data\n",
    "#         X_test[nan_mask] = np.take(col_mean, np.where(nan_mask)[1])\n",
    "    \n",
    "#     return X_train, X_test\n",
    "\n",
    "# def balance_training_data(X_train, y_train, random_state):\n",
    "#     \"\"\"\n",
    "#     Balance training data by subsampling to the smallest class.\n",
    "    \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     X_train_balanced : np.array\n",
    "#         Balanced training features\n",
    "#     y_train_balanced : np.array\n",
    "#         Balanced training labels\n",
    "#     \"\"\"\n",
    "#     unique_labels, counts = np.unique(y_train, return_counts=True)\n",
    "#     min_trials = np.min(counts)\n",
    "    \n",
    "#     balanced_indices = []\n",
    "#     rng = np.random.RandomState(random_state)\n",
    "    \n",
    "#     for label_val in unique_labels:\n",
    "#         label_indices = np.where(y_train == label_val)[0]\n",
    "#         subsampled = rng.choice(label_indices, min_trials, replace=False)\n",
    "#         balanced_indices.extend(subsampled)\n",
    "    \n",
    "#     X_train_balanced = X_train[balanced_indices]\n",
    "#     y_train_balanced = y_train[balanced_indices]\n",
    "    \n",
    "#     return X_train_balanced, y_train_balanced\n",
    "\n",
    "# def run_single_cv_fold(fold_data):\n",
    "#     \"\"\"\n",
    "#     Run a single cross-validation fold using PCA-LDA classifier.\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     fold_data : dict\n",
    "#         Dictionary containing all necessary data for one fold\n",
    "        \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     confusion_matrix : np.array\n",
    "#         Confusion matrix for this fold\n",
    "#     \"\"\"\n",
    "#     # Extract parameters\n",
    "#     train_trials = fold_data['train_trials']\n",
    "#     test_trials = fold_data['test_trials']\n",
    "#     train_data = fold_data['train_data']\n",
    "#     test_data = fold_data['test_data']\n",
    "#     y_train = fold_data['y_train']\n",
    "#     y_test = fold_data['y_test']\n",
    "#     subjects_tfr_objects = fold_data['subjects_tfr_objects']\n",
    "#     subjects = fold_data['subjects']\n",
    "#     condition_names = fold_data['condition_names']\n",
    "#     stat_func = fold_data['stat_func']\n",
    "#     p_thresh = fold_data['p_thresh']\n",
    "#     n_perm = fold_data['n_perm']\n",
    "#     ignore_adjacency = fold_data['ignore_adjacency']\n",
    "#     random_state = fold_data['random_state']\n",
    "#     fold_idx = fold_data['fold_idx']\n",
    "#     electrodes_per_subject_roi = fold_data['electrodes_per_subject_roi']\n",
    "#     roi = fold_data['roi']\n",
    "#     channel_labels = fold_data['channel_labels']\n",
    "#     freq_labels = fold_data['freq_labels']\n",
    "#     time_labels = fold_data['time_labels']\n",
    "#     tfr = fold_data['tfr']\n",
    "#     cats = fold_data['cats']\n",
    "    \n",
    "#     print(f\"    Processing fold {fold_idx + 1}\")\n",
    "    \n",
    "#     # 1. Create training TFR subset\n",
    "#     train_tfrs = create_train_tfr_subset(\n",
    "#         subjects_tfr_objects, train_trials, subjects, condition_names\n",
    "#     )\n",
    "    \n",
    "#     # 2. Find significant clusters using only training data\n",
    "#     train_masks, _ = get_sig_tfr_differences_per_subject(\n",
    "#         subjects_tfr_objects=train_tfrs,\n",
    "#         condition_names=condition_names,\n",
    "#         stat_func=stat_func,\n",
    "#         p_thresh=p_thresh,\n",
    "#         n_perm=n_perm,\n",
    "#         ignore_adjacency=ignore_adjacency,\n",
    "#         seed=random_state,\n",
    "#         tails=2\n",
    "#     )\n",
    "    \n",
    "#     # 3. Create feature mask\n",
    "#     feature_mask = create_feature_mask(\n",
    "#         train_masks, electrodes_per_subject_roi, roi, subjects,\n",
    "#         channel_labels, freq_labels, time_labels, tfr\n",
    "#     )\n",
    "    \n",
    "#     n_features = np.sum(feature_mask)\n",
    "#     print(f\"    Using {n_features} features from significant clusters\")\n",
    "    \n",
    "#     # 4. Extract and clean features\n",
    "#     X_train, X_test = extract_and_clean_features(train_data, test_data, feature_mask)\n",
    "    \n",
    "#     # 5. Balance training data\n",
    "#     X_train_balanced, y_train_balanced = balance_training_data(\n",
    "#         X_train, y_train, random_state + fold_idx\n",
    "#     )\n",
    "    \n",
    "#     # 6. Use the Decoder class (PCA-LDA) instead of SVC\n",
    "#     # The Decoder class uses PcaLdaClassification internally\n",
    "#     # First parameter is the percentage of variance to retain (0.80 = 80%)\n",
    "#     decoder = Decoder(cats, 0.80, n_splits=1, n_repeats=1, oversample=False)\n",
    "    \n",
    "#     # Fit and predict using the decoder's methods\n",
    "#     decoder.fit(X_train_balanced, y_train_balanced)\n",
    "#     preds = decoder.predict(X_test)\n",
    "    \n",
    "#     return confusion_matrix(y_test, preds, labels=list(cats.values()))\n",
    "\n",
    "# def run_cv_cluster_decoding(\n",
    "#     subjects_tfr_objects,\n",
    "#     electrodes_per_subject_roi,\n",
    "#     rois,\n",
    "#     subjects,\n",
    "#     condition_names,\n",
    "#     stat_func,\n",
    "#     p_thresh,\n",
    "#     n_perm,\n",
    "#     ignore_adjacency,\n",
    "#     n_splits=5,\n",
    "#     n_repeats=5,\n",
    "#     random_state=42\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Main function that orchestrates the cross-validated cluster-based decoding.\n",
    "#     \"\"\"\n",
    "#     results_per_roi = {}\n",
    "#     cats = {name: i for i, name in enumerate(condition_names)}\n",
    "    \n",
    "#     for roi in rois:\n",
    "#         print(f\"\\n--- Starting Cross-Validation for ROI: {roi} ---\")\n",
    "        \n",
    "#         # 1. Collect all data for this ROI\n",
    "#         all_data, trial_labels, condition_labels, channel_labels, freq_labels, time_labels = \\\n",
    "#             collect_roi_data(subjects_tfr_objects, electrodes_per_subject_roi, \n",
    "#                            roi, subjects, condition_names)\n",
    "        \n",
    "#         if len(all_data) == 0:\n",
    "#             print(f\"  No data found for ROI: {roi}\")\n",
    "#             continue\n",
    "        \n",
    "#         # Create LabeledArray\n",
    "#         roi_data = LabeledArray(\n",
    "#             all_data,\n",
    "#             labels=[trial_labels, channel_labels, freq_labels, time_labels]\n",
    "#         )\n",
    "        \n",
    "#         # Create label array for classification\n",
    "#         y_labels = np.array([cats[cond] for cond in condition_labels])\n",
    "        \n",
    "#         # 2. Set up cross-validation\n",
    "#         skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "#         # Store CMs organized by repeat\n",
    "#         cms_per_repeat = []\n",
    "        \n",
    "#         # Get a sample TFR for channel name lookups\n",
    "#         sample_tfr = subjects_tfr_objects[subjects[0]][condition_names[0]]\n",
    "\n",
    "#         for repeat_idx in range(n_repeats):\n",
    "#             cms_this_repeat = []\n",
    "#             # different random state for each repeat\n",
    "#             repeat_random_state = random_state + repeat_idx * 1000\n",
    "#             skf = StratifiedKFold(n_splits=n_splits, shuffle=True, \n",
    "#                                 random_state=repeat_random_state)\n",
    "\n",
    "#             for fold_idx, (train_indices, test_indices) in enumerate(skf.split(np.zeros(len(y_labels)), y_labels)):\n",
    "#                 print(f\"  --- Running Fold {fold_idx + 1}/{n_splits} ---\")\n",
    "\n",
    "#                 # Create the list of trial labels for indexing\n",
    "#                 train_label_list = [trial_labels[i] for i in train_indices]\n",
    "#                 test_label_list = [trial_labels[i] for i in test_indices]\n",
    "                \n",
    "#                 # Prepare fold data with corrected, explicit indexing\n",
    "#                 fold_data = {\n",
    "#                     'train_trials': train_label_list,\n",
    "#                     'test_trials': test_label_list,\n",
    "#                     # Corrected Lines: Wrap the list in a tuple to ensure it's treated as an index for the first dimension.\n",
    "#                     'train_data': roi_data[(train_label_list,)], \n",
    "#                     'test_data': roi_data[(test_label_list,)],\n",
    "#                     'y_train': y_labels[train_indices],\n",
    "#                     'y_test': y_labels[test_indices],\n",
    "#                     'subjects_tfr_objects': subjects_tfr_objects,\n",
    "#                     'subjects': subjects,\n",
    "#                     'condition_names': condition_names,\n",
    "#                     'stat_func': stat_func,\n",
    "#                     'p_thresh': p_thresh,\n",
    "#                     'n_perm': n_perm,\n",
    "#                     'ignore_adjacency': ignore_adjacency,\n",
    "#                     'random_state': random_state,\n",
    "#                     'fold_idx': fold_idx,\n",
    "#                     'electrodes_per_subject_roi': electrodes_per_subject_roi,\n",
    "#                     'roi': roi,\n",
    "#                     'channel_labels': channel_labels,\n",
    "#                     'freq_labels': freq_labels,\n",
    "#                     'time_labels': time_labels,\n",
    "#                     'tfr': sample_tfr,\n",
    "#                     'cats': cats\n",
    "#                 }\n",
    "\n",
    "#                 # Run single fold\n",
    "#                 cm = run_single_cv_fold(fold_data)\n",
    "#                 cms_this_repeat.append(cm)\n",
    "#             # sum across folds for this repeat\n",
    "#             cm_summed = np.sum(cms_this_repeat, axis=0)\n",
    "#             cms_per_repeat.append(cm_summed)\n",
    "        \n",
    "#         # Average across repeats\n",
    "#         final_cm = np.mean(cms_per_repeat, axis=0)\n",
    "#         accuracy = np.trace(final_cm) / np.sum(final_cm)\n",
    "        \n",
    "#         results_per_roi[roi] = {\n",
    "#             'confusion_matrix': final_cm,\n",
    "#             'accuracy': accuracy,\n",
    "#             'cats': cats,\n",
    "#             'labeled_array': roi_data,\n",
    "#             'fold_cms': cms_per_repeat  # Keep individual fold results for analysis\n",
    "#         }\n",
    "        \n",
    "#         print(f\"  --> Final Accuracy for {roi}: {accuracy:.3f}\")\n",
    "    \n",
    "#     return results_per_roi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0e15b9",
   "metadata": {},
   "source": [
    "okay trying again using the approach from my notes 7/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9380747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_subjects_tfr_objects_for_specific_roi(subjects_tfr_objects: Dict, \n",
    "                                              electrodes_per_subject_roi: Dict,\n",
    "                                              roi: str, \n",
    "                                              subjects: List[str], \n",
    "                                              condition_names: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Filter subjects_tfr_objects to only include electrodes from a specific ROI.\n",
    "    Maintains the same structure as the input but with filtered channels.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    subjects_tfr_objects : dict\n",
    "        Original structure: {subject: {condition: tfr_object}}\n",
    "    electrodes_per_subject_roi : dict\n",
    "        Structure: {roi: {subject: [electrode_names]}}\n",
    "    roi : str\n",
    "        The ROI to filter for\n",
    "    subjects : list\n",
    "        List of subject IDs\n",
    "    condition_names : list\n",
    "        List of condition names\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> import mne\n",
    "    >>> import numpy as np\n",
    "    >>> from typing import Dict, List\n",
    "    >>> # 1. Create fake data for the example\n",
    "    >>> ch_names = ['CH1', 'CH2', 'CH3', 'CH4', 'CH5']\n",
    "    >>> sfreq = 200.0\n",
    "    >>> info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='seeg')\n",
    "    >>> n_epochs, n_freqs, n_times = 2, 10, 50\n",
    "    >>> data = np.random.randn(n_epochs, len(ch_names), n_freqs, n_times)\n",
    "    >>> freqs = np.arange(10., 20., 1.)\n",
    "    >>> times = np.linspace(-0.5, 1.0, n_times)\n",
    "    >>> tfr_mock = mne.time_frequency.EpochsTFR(info, data, times, freqs)\n",
    "    >>> subjects_tfr_objects_mock = {'sub_1': {'condition_1': tfr_mock, 'condition_2': tfr_mock.copy()}}\n",
    "    >>> # Define which electrodes belong to which ROI for the subject\n",
    "    >>> electrodes_per_subject_roi_mock = {\n",
    "    ...     'roi_1': {'sub_1': ['CH1', 'CH2', 'CH3']},\n",
    "    ...     'roi_2': {'sub_1': ['CH4', 'CH5']}\n",
    "    ... }\n",
    "    >>> # 2. Run the function\n",
    "    >>> subjects_tfr_objects_for_roi_1 = get_subjects_tfr_objects_for_specific_roi(\n",
    "    ...     subjects_tfr_objects=subjects_tfr_objects_mock,\n",
    "    ...     electrodes_per_subject_roi=electrodes_per_subject_roi_mock,\n",
    "    ...     roi='roi_1',\n",
    "    ...     subjects=['sub_1'],\n",
    "    ...     condition_names=['condition_1', 'condition_2']\n",
    "    ... )\n",
    "    Subject sub_1, Condition condition_1: Selected 3/5 channels for ROI roi_1\n",
    "    Subject sub_1, Condition condition_2: Selected 3/5 channels for ROI roi_1\n",
    "    >>> # 3. Check the result\n",
    "    >>> print(\"Original channels:\", subjects_tfr_objects_mock['sub_1']['condition_1'].ch_names)\n",
    "    Original channels: ['CH1', 'CH2', 'CH3', 'CH4', 'CH5']\n",
    "    >>> print(\"Filtered channels for roi_1:\", subjects_tfr_objects_for_roi_1['sub_1']['condition_1'].ch_names)\n",
    "    Filtered channels for roi_1: ['CH1', 'CH2', 'CH3']\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    subjects_tfr_objects_for_specific_roi : dict\n",
    "        Same structure as input: {subject: {condition: tfr_object}}\n",
    "        but tfr_objects only contain channels from the specified ROI\n",
    "    \"\"\"\n",
    "    subjects_tfr_objects_for_specific_roi = {}\n",
    "    \n",
    "    for sub in subjects:\n",
    "        # Get electrodes for this subject in this ROI\n",
    "        roi_electrodes = electrodes_per_subject_roi.get(roi, {}).get(sub, [])\n",
    "        \n",
    "        # Skip subject if they have no electrodes in this ROI\n",
    "        if not roi_electrodes:\n",
    "            print(f\"Subject {sub} has no electrodes in ROI {roi}, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        subjects_tfr_objects_for_specific_roi[sub] = {}\n",
    "        \n",
    "        for cond in condition_names:\n",
    "            # Get the original TFR object\n",
    "            tfr = subjects_tfr_objects[sub][cond]\n",
    "            \n",
    "            # Find which ROI electrodes are actually in this TFR epochs object\n",
    "            available_roi_electrodes = [elec for elec in roi_electrodes if elec in tfr.ch_names]\n",
    "            \n",
    "            if available_roi_electrodes:\n",
    "                # Use MNE's pick method to select only ROI channels\n",
    "                roi_tfr = tfr.copy().pick(available_roi_electrodes)\n",
    "                subjects_tfr_objects_for_specific_roi[sub][cond] = roi_tfr\n",
    "                \n",
    "                print(f\"Subject {sub}, Condition {cond}: Selected {len(available_roi_electrodes)}/{len(tfr.ch_names)} channels for ROI {roi}\")\n",
    "            else:\n",
    "                print(f\"Warning: Subject {sub}, Condition {cond} has no channels in ROI {roi}\")\n",
    "                # You could either skip this condition or include an empty TFR\n",
    "                # Option 1: Skip (don't add to dictionary)\n",
    "                # Option 2: Add empty TFR (uncomment below)\n",
    "                # subjects_tfr_objects_for_specific_roi[sub][cond] = tfr.copy().pick([])\n",
    "    \n",
    "    return subjects_tfr_objects_for_specific_roi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b94221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# untested\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import mne\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def prepare_data_with_nan_padding(roi_tfr_objects: Dict, \n",
    "                                 condition_names: List[str]) -> Tuple[np.ndarray, np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Prepare data matrix with NaN padding to ensure consistent trial counts across subjects.\n",
    "    Modified to work with multi-channel TFR objects.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    data_matrix : np.ndarray\n",
    "        Shape: (n_total_trials, n_total_channels, n_freqs, n_times)\n",
    "    labels : np.ndarray\n",
    "        Condition labels for each trial\n",
    "    metadata : dict\n",
    "        Information about data organization\n",
    "    \"\"\"\n",
    "    # First, find maximum trials per condition across all subjects\n",
    "    max_trials_per_condition = {cond: 0 for cond in condition_names}\n",
    "    subject_channel_info = []\n",
    "    \n",
    "    for sub, sub_data in roi_tfr_objects.items():\n",
    "        for cond in condition_names:\n",
    "            if cond in sub_data:\n",
    "                tfr = sub_data[cond]\n",
    "                n_trials = len(tfr)\n",
    "                max_trials_per_condition[cond] = max(max_trials_per_condition[cond], n_trials)\n",
    "                # Store info about channels for this subject\n",
    "                for ch_idx, ch_name in enumerate(tfr.ch_names):\n",
    "                    subject_channel_info.append((sub, ch_name, ch_idx))\n",
    "    \n",
    "    print(f\"Maximum trials per condition: {max_trials_per_condition}\")\n",
    "    \n",
    "    # Get unique subject-channel pairs\n",
    "    subject_channels = list(set(subject_channel_info))\n",
    "    n_total_channels = len(subject_channels)\n",
    "    \n",
    "    # Get dimensions from first TFR\n",
    "    sample_tfr = next(iter(next(iter(roi_tfr_objects.values())).values()))\n",
    "    n_freqs = len(sample_tfr.freqs)\n",
    "    n_times = len(sample_tfr.times)\n",
    "    \n",
    "    # Calculate total trials\n",
    "    total_trials = sum(max_trials_per_condition.values())\n",
    "    \n",
    "    # Initialize data matrix with NaNs\n",
    "    data_matrix = np.full((total_trials, n_total_channels, n_freqs, n_times), np.nan)\n",
    "    labels = np.zeros(total_trials, dtype=int)\n",
    "    \n",
    "    # Create channel index mapping\n",
    "    channel_to_idx = {(sub, ch): idx for idx, (sub, ch, _) in enumerate(subject_channels)}\n",
    "    \n",
    "    # Fill the matrix\n",
    "    trial_idx = 0\n",
    "    trial_metadata = []\n",
    "    \n",
    "    for cond_idx, cond in enumerate(condition_names):\n",
    "        max_trials = max_trials_per_condition[cond]\n",
    "        \n",
    "        for trial_num in range(max_trials):\n",
    "            # For each subject, add this trial if it exists\n",
    "            for sub in roi_tfr_objects:\n",
    "                if cond in roi_tfr_objects[sub]:\n",
    "                    tfr = roi_tfr_objects[sub][cond]\n",
    "                    if trial_num < len(tfr):\n",
    "                        # Extract trial data for all channels of this subject\n",
    "                        trial_data = tfr.data[trial_num, :, :, :]  # (channels, freqs, times)\n",
    "                        \n",
    "                        # Place each channel's data in the correct position\n",
    "                        for ch_idx, ch_name in enumerate(tfr.ch_names):\n",
    "                            global_ch_idx = channel_to_idx[(sub, ch_name)]\n",
    "                            data_matrix[trial_idx, global_ch_idx, :, :] = trial_data[ch_idx, :, :]\n",
    "            \n",
    "            labels[trial_idx] = cond_idx\n",
    "            trial_metadata.append({\n",
    "                'trial_idx': trial_idx,\n",
    "                'condition': cond,\n",
    "                'condition_idx': cond_idx,\n",
    "                'trial_num_in_condition': trial_num\n",
    "            })\n",
    "            trial_idx += 1\n",
    "    \n",
    "    metadata = {\n",
    "        'subject_channels': subject_channels,  # List of (subject, channel) tuples\n",
    "        'channel_to_idx': channel_to_idx,\n",
    "        'conditions': condition_names,\n",
    "        'max_trials_per_condition': max_trials_per_condition,\n",
    "        'trial_metadata': trial_metadata,\n",
    "        'freqs': sample_tfr.freqs,\n",
    "        'times': sample_tfr.times,\n",
    "        'subjects': list(roi_tfr_objects.keys())\n",
    "    }\n",
    "    \n",
    "    return data_matrix, labels, metadata\n",
    "\n",
    "def split_tfr_objects_by_trials(roi_tfr_objects: Dict, \n",
    "                               train_indices: np.ndarray,\n",
    "                               metadata: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Create a subset of TFR objects containing only training trials.\n",
    "    Modified to work with the simpler structure.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    train_tfr_objects : dict\n",
    "        Same structure as roi_tfr_objects but with only training trials\n",
    "    \"\"\"\n",
    "    train_tfr_objects = deepcopy(roi_tfr_objects)\n",
    "    \n",
    "    # Map train indices to condition/trial pairs\n",
    "    train_trials_by_condition = {cond: [] for cond in metadata['conditions']}\n",
    "    \n",
    "    for idx in train_indices:\n",
    "        trial_info = metadata['trial_metadata'][idx]\n",
    "        cond = trial_info['condition']\n",
    "        trial_num = trial_info['trial_num_in_condition']\n",
    "        train_trials_by_condition[cond].append(trial_num)\n",
    "    \n",
    "    # Filter TFR objects to only include training trials\n",
    "    for sub in train_tfr_objects:\n",
    "        for cond in metadata['conditions']:\n",
    "            if cond in train_tfr_objects[sub]:\n",
    "                tfr = train_tfr_objects[sub][cond]\n",
    "                valid_trials = [t for t in train_trials_by_condition[cond] if t < len(tfr)]\n",
    "                if valid_trials:\n",
    "                    train_tfr_objects[sub][cond] = tfr[valid_trials]\n",
    "                else:\n",
    "                    # Create empty TFR if no valid trials\n",
    "                    train_tfr_objects[sub][cond] = tfr[:0]\n",
    "    \n",
    "    return train_tfr_objects\n",
    "\n",
    "\n",
    "def compute_tfr_masks_from_training(train_tfr_objects: Dict,\n",
    "                                   condition_names: List[str],\n",
    "                                   stat_func: callable,\n",
    "                                   p_thresh: float,\n",
    "                                   n_perm: int,\n",
    "                                   metadata: Dict,\n",
    "                                   ignore_adjacency: int = 1,\n",
    "                                   seed: int = 42,\n",
    "                                   tails: int = 2) -> Dict:\n",
    "    \"\"\"\n",
    "    Compute significant TFR clusters using only training data.\n",
    "    Modified to handle multi-channel TFR objects.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    channel_masks : dict\n",
    "        {(subject, channel): mask_array} where mask is (n_freqs, n_times)\n",
    "    \"\"\"\n",
    "    channel_masks = {}\n",
    "    \n",
    "    # Process each subject's data\n",
    "    for sub in metadata['subjects']:\n",
    "        if sub not in train_tfr_objects:\n",
    "            continue\n",
    "        \n",
    "        # Get TFRs for both conditions\n",
    "        tfrs_for_comparison = []\n",
    "        for cond in condition_names[:2]:  # Assuming binary comparison\n",
    "            if cond in train_tfr_objects[sub]:\n",
    "                tfr = train_tfr_objects[sub][cond]\n",
    "                if len(tfr) > 0:\n",
    "                    tfrs_for_comparison.append(tfr)\n",
    "        \n",
    "        if len(tfrs_for_comparison) == 2:\n",
    "            # Run statistical test on multi-channel data\n",
    "            mask, _ = get_sig_tfr_differences(\n",
    "                tfr_data_cond1=tfrs_for_comparison[0],\n",
    "                tfr_data_cond2=tfrs_for_comparison[1],\n",
    "                stat_func=stat_func,\n",
    "                p_thresh=p_thresh,\n",
    "                n_perm=n_perm,\n",
    "                axis=0,  # trials axis\n",
    "                ignore_adjacency=ignore_adjacency,\n",
    "                seed=seed,\n",
    "                tails=tails\n",
    "            )\n",
    "            # mask shape: (n_channels, n_freqs, n_times)\n",
    "            \n",
    "            # Store mask for each channel\n",
    "            tfr = tfrs_for_comparison[0]  # Use to get channel names\n",
    "            for ch_idx, ch_name in enumerate(tfr.ch_names):\n",
    "                if ch_idx < mask.shape[0]:\n",
    "                    channel_masks[(sub, ch_name)] = mask[ch_idx, :, :]\n",
    "                else:\n",
    "                    # If mask doesn't have enough channels, use zeros\n",
    "                    channel_masks[(sub, ch_name)] = np.zeros((len(metadata['freqs']), \n",
    "                                                              len(metadata['times'])), dtype=bool)\n",
    "        else:\n",
    "            # No data for comparison, create zero masks\n",
    "            if sub in train_tfr_objects:\n",
    "                for cond in train_tfr_objects[sub]:\n",
    "                    if cond in train_tfr_objects[sub]:\n",
    "                        tfr = train_tfr_objects[sub][cond]\n",
    "                        for ch_name in tfr.ch_names:\n",
    "                            channel_masks[(sub, ch_name)] = np.zeros((len(metadata['freqs']), \n",
    "                                                                      len(metadata['times'])), dtype=bool)\n",
    "    \n",
    "    return channel_masks\n",
    "\n",
    "\n",
    "def apply_masks_and_flatten(data_matrix: np.ndarray, \n",
    "                           channel_masks: Dict,\n",
    "                           metadata: Dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply TFR masks to data and flatten to create feature matrix.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    feature_matrix : np.ndarray\n",
    "        Shape: (n_trials, n_features) where features are concatenated across channels\n",
    "    \"\"\"\n",
    "    n_trials = data_matrix.shape[0]\n",
    "    feature_vectors = []\n",
    "    \n",
    "    # For each channel, apply mask and flatten\n",
    "    for (sub, ch_name), global_ch_idx in metadata['channel_to_idx'].items():\n",
    "        channel_data = data_matrix[:, global_ch_idx, :, :]  # (n_trials, n_freqs, n_times)\n",
    "        \n",
    "        if (sub, ch_name) in channel_masks:\n",
    "            mask = channel_masks[(sub, ch_name)]\n",
    "            \n",
    "            # Apply mask and flatten for each trial\n",
    "            masked_features = []\n",
    "            for trial in range(n_trials):\n",
    "                trial_data = channel_data[trial, :, :]\n",
    "                masked_data = trial_data[mask]  # 1D vector of masked features\n",
    "                masked_features.append(masked_data)\n",
    "            \n",
    "            masked_features = np.array(masked_features)  # (n_trials, n_masked_features)\n",
    "        else:\n",
    "            # If no mask, use all features\n",
    "            masked_features = channel_data.reshape(n_trials, -1)\n",
    "        \n",
    "        feature_vectors.append(masked_features)\n",
    "    \n",
    "    # Concatenate features across channels\n",
    "    feature_matrix = np.concatenate(feature_vectors, axis=1)\n",
    "    \n",
    "    return feature_matrix\n",
    "\n",
    "def run_tfr_cluster_decoding_pipeline(subjects_tfr_objects: Dict,\n",
    "                                     electrodes_per_subject_roi: Dict,\n",
    "                                     roi: str,\n",
    "                                     subjects: List[str],\n",
    "                                     condition_names: List[str],\n",
    "                                     stat_func: callable,\n",
    "                                     p_thresh: float = 0.05,\n",
    "                                     n_perm: int = 100,\n",
    "                                     n_splits: int = 5,\n",
    "                                     n_repeats: int = 10,\n",
    "                                     balance_method: str = 'subsample',\n",
    "                                     n_bootstraps: int = 10,\n",
    "                                     random_state: int = 42,\n",
    "                                     ignore_adjacency: int = 1,\n",
    "                                     seed: int = 42,\n",
    "                                     tails: int = 2) -> Dict:\n",
    "    \"\"\"\n",
    "    Main pipeline function that orchestrates the entire decoding process.\n",
    "    Modified to use get_subjects_tfr_objects_for_specific_roi.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Running TFR Cluster Decoding for ROI: {roi} ===\")\n",
    "    \n",
    "    # Step 1: Filter TFR objects for specific ROI\n",
    "    roi_tfr_objects = get_subjects_tfr_objects_for_specific_roi(\n",
    "        subjects_tfr_objects, electrodes_per_subject_roi, \n",
    "        roi, subjects, condition_names\n",
    "    )\n",
    "    \n",
    "    if not roi_tfr_objects:\n",
    "        print(f\"No data found for ROI: {roi}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Prepare data with NaN padding (BEFORE splitting)\n",
    "    data_matrix, labels, metadata = prepare_data_with_nan_padding(\n",
    "        roi_tfr_objects, condition_names\n",
    "    )\n",
    "    \n",
    "    print(f\"Data matrix shape: {data_matrix.shape}\")\n",
    "    print(f\"Number of channels: {len(metadata['subject_channels'])}\")\n",
    "    print(f\"Number of NaN trials: {np.isnan(data_matrix).all(axis=(1,2,3)).sum()}\")\n",
    "    \n",
    "    # Step 3: Set up cross-validation\n",
    "    cats = {name: i for i, name in enumerate(condition_names)}\n",
    "    \n",
    "    # Storage for results\n",
    "    all_cms = []\n",
    "    \n",
    "    # Step 4: Repeat the entire process\n",
    "    for repeat in range(n_repeats):\n",
    "        repeat_seed = random_state + repeat * 1000\n",
    "        skf_repeat = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=repeat_seed)\n",
    "        \n",
    "        fold_cms = []\n",
    "        \n",
    "        # Step 5: Cross-validation folds\n",
    "        for fold_idx, (train_indices, test_indices) in enumerate(skf_repeat.split(np.zeros(len(labels)), labels)):\n",
    "            print(f\"\\n  Repeat {repeat+1}/{n_repeats}, Fold {fold_idx+1}/{n_splits}\")\n",
    "            \n",
    "            # Step 6: Split data\n",
    "            X_train_raw = data_matrix[train_indices]\n",
    "            X_test_raw = data_matrix[test_indices]\n",
    "            y_train = labels[train_indices]\n",
    "            y_test = labels[test_indices]\n",
    "            \n",
    "            # Step 7: Create training-only TFR objects for computing masks\n",
    "            train_tfr_objects = split_tfr_objects_by_trials(\n",
    "                roi_tfr_objects, train_indices, metadata\n",
    "            )\n",
    "            \n",
    "            # Step 8: Compute masks using ONLY training data\n",
    "            channel_masks = compute_tfr_masks_from_training(\n",
    "                train_tfr_objects, condition_names, \n",
    "                stat_func, p_thresh, n_perm, metadata,\n",
    "                ignore_adjacency, seed, tails\n",
    "            )\n",
    "            \n",
    "            n_sig_channels = sum(1 for mask in channel_masks.values() if mask.any())\n",
    "            print(f\"    Found significant clusters in {n_sig_channels}/{len(metadata['subject_channels'])} channels\")\n",
    "            \n",
    "            # Step 9: Apply masks to both train and test data\n",
    "            X_train = apply_masks_and_flatten(X_train_raw, channel_masks, metadata)\n",
    "            X_test = apply_masks_and_flatten(X_test_raw, channel_masks, metadata)\n",
    "            \n",
    "            print(f\"    Feature matrix shape: {X_train.shape}\")\n",
    "            \n",
    "            # Step 10: Balance and decode\n",
    "            cm = balance_and_decode(\n",
    "                X_train, y_train, X_test, y_test,\n",
    "                Decoder, cats, balance_method, \n",
    "                n_bootstraps, repeat_seed + fold_idx\n",
    "            )\n",
    "            \n",
    "            fold_cms.append(cm)\n",
    "        \n",
    "        # Sum across folds for this repeat\n",
    "        repeat_cm = np.sum(fold_cms, axis=0)\n",
    "        all_cms.append(repeat_cm)\n",
    "    \n",
    "    # Average across repeats\n",
    "    final_cm = np.mean(all_cms, axis=0)\n",
    "    accuracy = np.trace(final_cm) / np.sum(final_cm)\n",
    "    \n",
    "    results = {\n",
    "        'confusion_matrix': final_cm,\n",
    "        'accuracy': accuracy,\n",
    "        'cats': cats,\n",
    "        'metadata': metadata,\n",
    "        'all_repeat_cms': all_cms\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n=== Final accuracy for {roi}: {accuracy:.3f} ===\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c304a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3988d0d8",
   "metadata": {},
   "source": [
    "plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77ad2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cv_confusion_matrices(cv_results, save_dir, conditions_save_name):\n",
    "    \"\"\"\n",
    "    Plot confusion matrices for cross-validated cluster-based decoding results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cv_results : dict\n",
    "        Results from run_cv_cluster_decoding containing confusion matrices per ROI\n",
    "    save_dir : str\n",
    "        Directory to save plots\n",
    "    conditions_save_name : str\n",
    "        Name identifier for the conditions being compared\n",
    "    \"\"\"\n",
    "    for roi, results in cv_results.items():\n",
    "        # Get the final aggregated confusion matrix\n",
    "        cm = results['confusion_matrix']\n",
    "        cats = results['cats']\n",
    "        accuracy = results['accuracy']\n",
    "        \n",
    "        # Also get individual fold confusion matrices\n",
    "        fold_cms = results.get('fold_cms', [])\n",
    "        \n",
    "        # 1. Plot the aggregated confusion matrix\n",
    "        plot_single_confusion_matrix(\n",
    "            cm, cats, accuracy, roi, \n",
    "            f'{roi}_CV_cluster_decoded_final_cm_{conditions_save_name}',\n",
    "            save_dir,\n",
    "            title_suffix='Final Aggregated'\n",
    "        )\n",
    "        \n",
    "        # 2. Plot individual fold confusion matrices in a grid\n",
    "        if fold_cms:\n",
    "            plot_fold_confusion_matrices(\n",
    "                fold_cms, cats, roi,\n",
    "                f'{roi}_CV_cluster_decoded_folds_{conditions_save_name}',\n",
    "                save_dir\n",
    "            )\n",
    "        \n",
    "        # 3. Plot normalized confusion matrix\n",
    "        plot_normalized_confusion_matrix(\n",
    "            cm, cats, accuracy, roi,\n",
    "            f'{roi}_CV_cluster_decoded_normalized_{conditions_save_name}',\n",
    "            save_dir\n",
    "        )\n",
    "\n",
    "\n",
    "def plot_single_confusion_matrix(cm, cats, accuracy, roi, filename, save_dir, \n",
    "                                title_suffix='', normalize=False):\n",
    "    \"\"\"\n",
    "    Plot a single confusion matrix with customizable normalization.\n",
    "    \"\"\"\n",
    "    # Normalize if requested\n",
    "    if normalize:\n",
    "        cm_plot = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm_plot = np.nan_to_num(cm_plot)  # Handle division by zero\n",
    "    else:\n",
    "        cm_plot = cm\n",
    "    \n",
    "    display_labels = list(cats.keys())\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Create the display\n",
    "    if normalize:\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm_plot, display_labels=display_labels)\n",
    "        im = disp.plot(ax=ax, cmap='Blues', values_format='.2f')\n",
    "        im.im_.set_clim(0, 1)\n",
    "    else:\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm_plot, display_labels=display_labels)\n",
    "        im = disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "    \n",
    "    # Add title\n",
    "    title = f'ROI: {roi} - Accuracy: {accuracy:.3f}'\n",
    "    if title_suffix:\n",
    "        title += f' ({title_suffix})'\n",
    "    ax.set_title(title, fontsize=14, pad=20)\n",
    "    \n",
    "    # Enhance the plot\n",
    "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "    ax.set_ylabel('True Label', fontsize=12)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    ax.set_xticks(np.arange(len(display_labels)) - 0.5, minor=True)\n",
    "    ax.set_yticks(np.arange(len(display_labels)) - 0.5, minor=True)\n",
    "    ax.grid(which='minor', color='gray', linestyle='-', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'{filename}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_fold_confusion_matrices(fold_cms, cats, roi, filename, save_dir):\n",
    "    \"\"\"\n",
    "    Plot all fold confusion matrices in a grid layout.\n",
    "    \"\"\"\n",
    "    n_folds = len(fold_cms)\n",
    "    n_cols = min(3, n_folds)\n",
    "    n_rows = (n_folds + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "    if n_folds == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    display_labels = list(cats.keys())\n",
    "    \n",
    "    for idx, cm in enumerate(fold_cms):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Normalize the confusion matrix\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm_normalized = np.nan_to_num(cm_normalized)\n",
    "        \n",
    "        # Calculate fold accuracy\n",
    "        fold_accuracy = np.trace(cm) / np.sum(cm)\n",
    "        \n",
    "        # Plot\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, \n",
    "                                    display_labels=display_labels)\n",
    "        im = disp.plot(ax=ax, cmap='Blues', values_format='.2f', colorbar=False)\n",
    "        im.im_.set_clim(0, 1)\n",
    "        \n",
    "        ax.set_title(f'Fold {idx+1} (Acc: {fold_accuracy:.3f})', fontsize=12)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "    \n",
    "    # Remove unused subplots\n",
    "    for idx in range(n_folds, len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "    \n",
    "    # Add overall title\n",
    "    fig.suptitle(f'ROI: {roi} - Individual Fold Confusion Matrices', fontsize=16)\n",
    "    \n",
    "    # Add shared labels\n",
    "    fig.text(0.5, 0.02, 'Predicted Label', ha='center', fontsize=14)\n",
    "    fig.text(0.02, 0.5, 'True Label', va='center', rotation='vertical', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'{filename}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_normalized_confusion_matrix(cm, cats, accuracy, roi, filename, save_dir):\n",
    "    \"\"\"\n",
    "    Plot a normalized confusion matrix showing percentages.\n",
    "    \"\"\"\n",
    "    # Calculate different normalizations\n",
    "    cm_row_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    cm_row_norm = np.nan_to_num(cm_row_norm)\n",
    "    \n",
    "    display_labels = list(cats.keys())\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Row-normalized (recall)\n",
    "    disp1 = ConfusionMatrixDisplay(confusion_matrix=cm_row_norm, \n",
    "                                  display_labels=display_labels)\n",
    "    im1 = disp1.plot(ax=ax1, cmap='Blues', values_format='.2%')\n",
    "    im1.im_.set_clim(0, 1)\n",
    "    ax1.set_title(f'Row-Normalized (Recall)\\nAccuracy: {accuracy:.3f}', fontsize=14)\n",
    "    \n",
    "    # Plot 2: Raw counts with percentages\n",
    "    total_samples = np.sum(cm)\n",
    "    cm_percentage = cm / total_samples\n",
    "    \n",
    "    # Create custom annotation\n",
    "    im2 = ax2.imshow(cm_percentage, cmap='Blues', aspect='auto')\n",
    "    \n",
    "    # Add text annotations showing both count and percentage\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            count = cm[i, j]\n",
    "            pct = cm_percentage[i, j] * 100\n",
    "            text = f'{int(count)}\\n({pct:.1f}%)'\n",
    "            color = 'white' if cm_percentage[i, j] > 0.5 else 'black'\n",
    "            ax2.text(j, i, text, ha='center', va='center', color=color, fontsize=10)\n",
    "    \n",
    "    ax2.set_xticks(range(len(display_labels)))\n",
    "    ax2.set_yticks(range(len(display_labels)))\n",
    "    ax2.set_xticklabels(display_labels)\n",
    "    ax2.set_yticklabels(display_labels)\n",
    "    ax2.set_xlabel('Predicted Label')\n",
    "    ax2.set_ylabel('True Label')\n",
    "    ax2.set_title(f'Raw Counts with Percentages\\nTotal Samples: {int(total_samples)}', fontsize=14)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im2, ax=ax2)\n",
    "    cbar.set_label('Proportion of Total', rotation=270, labelpad=20)\n",
    "    \n",
    "    fig.suptitle(f'ROI: {roi} - Detailed Confusion Matrix Analysis', fontsize=16)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(save_dir, f'{filename}.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_accuracy_comparison_across_rois(cv_results, save_dir, conditions_save_name):\n",
    "    \"\"\"\n",
    "    Create a bar plot comparing accuracies across ROIs.\n",
    "    \"\"\"\n",
    "    rois = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for roi, results in cv_results.items():\n",
    "        rois.append(roi)\n",
    "        accuracies.append(results['accuracy'])\n",
    "    \n",
    "    # Create bar plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars = ax.bar(rois, accuracies, color='steelblue', alpha=0.8)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    # Add chance level line (assuming binary classification)\n",
    "    n_classes = len(list(cv_results.values())[0]['cats'])\n",
    "    chance_level = 1.0 / n_classes\n",
    "    ax.axhline(y=chance_level, color='red', linestyle='--', alpha=0.7, \n",
    "               label=f'Chance Level ({chance_level:.3f})')\n",
    "    \n",
    "    ax.set_xlabel('ROI', fontsize=14)\n",
    "    ax.set_ylabel('Accuracy', fontsize=14)\n",
    "    ax.set_title(f'Decoding Accuracy Across ROIs\\n{conditions_save_name}', fontsize=16)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f'accuracy_comparison_across_rois_{conditions_save_name}.png'\n",
    "    plt.savefig(os.path.join(save_dir, filename), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_cv_results_summary(cv_results, save_dir, conditions_save_name):\n",
    "    \"\"\"\n",
    "    Save a text summary of the cross-validation results.\n",
    "    \"\"\"\n",
    "    summary_file = os.path.join(save_dir, f'cv_results_summary_{conditions_save_name}.txt')\n",
    "    \n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(f\"Cross-Validation Cluster-Based Decoding Results\\n\")\n",
    "        f.write(f\"Conditions: {conditions_save_name}\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\\n\")\n",
    "        \n",
    "        for roi, results in cv_results.items():\n",
    "            f.write(f\"ROI: {roi}\\n\")\n",
    "            f.write(f\"Final Accuracy: {results['accuracy']:.4f}\\n\")\n",
    "            f.write(f\"Number of folds: {len(results.get('fold_cms', []))}\\n\")\n",
    "            \n",
    "            # Calculate per-class metrics\n",
    "            cm = results['confusion_matrix']\n",
    "            cats = results['cats']\n",
    "            \n",
    "            f.write(\"\\nPer-class metrics:\\n\")\n",
    "            for i, (cat_name, cat_idx) in enumerate(cats.items()):\n",
    "                if i < cm.shape[0]:  # Ensure we don't go out of bounds\n",
    "                    tp = cm[i, i]\n",
    "                    fp = np.sum(cm[:, i]) - tp\n",
    "                    fn = np.sum(cm[i, :]) - tp\n",
    "                    \n",
    "                    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "                    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "                    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "                    \n",
    "                    f.write(f\"  {cat_name}:\\n\")\n",
    "                    f.write(f\"    Precision: {precision:.4f}\\n\")\n",
    "                    f.write(f\"    Recall: {recall:.4f}\\n\")\n",
    "                    f.write(f\"    F1-score: {f1:.4f}\\n\")\n",
    "            \n",
    "            f.write(\"\\nConfusion Matrix:\\n\")\n",
    "            f.write(str(cm) + \"\\n\")\n",
    "            f.write(\"\\n\" + \"=\"*60 + \"\\n\\n\")\n",
    "\n",
    "\n",
    "# Enhanced main execution with plotting\n",
    "def run_analysis_with_plots(subjects_tfr_objects, all_electrodes_per_subject_roi, \n",
    "                           rois, subjects, condition_names, stat_func, p_thresh, \n",
    "                           n_perm, ignore_adjacency, n_splits, n_repeats, random_state,save_dir, conditions_save_name):\n",
    "    \"\"\"\n",
    "    Run the complete analysis pipeline with enhanced plotting.\n",
    "    \"\"\"\n",
    "    # Ensure save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Run the cross-validated analysis\n",
    "    print(\"Running cross-validated cluster-based decoding...\")\n",
    "    cv_results = run_cv_cluster_decoding(\n",
    "        subjects_tfr_objects=subjects_tfr_objects,\n",
    "        electrodes_per_subject_roi=all_electrodes_per_subject_roi,\n",
    "        rois=rois,\n",
    "        subjects=subjects,\n",
    "        condition_names=condition_names,\n",
    "        stat_func=stat_func,\n",
    "        p_thresh=p_thresh,\n",
    "        n_perm=n_perm,\n",
    "        ignore_adjacency=ignore_adjacency,\n",
    "        n_splits=n_splits,\n",
    "        n_repeats=n_repeats,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Generate all plots\n",
    "    print(\"Generating confusion matrix plots...\")\n",
    "    plot_cv_confusion_matrices(cv_results, save_dir, conditions_save_name)\n",
    "    \n",
    "    print(\"Generating accuracy comparison plot...\")\n",
    "    plot_accuracy_comparison_across_rois(cv_results, save_dir, conditions_save_name)\n",
    "    \n",
    "    print(\"Saving results summary...\")\n",
    "    save_cv_results_summary(cv_results, save_dir, conditions_save_name)\n",
    "    \n",
    "    print(\"Analysis complete!\")\n",
    "    return cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HOW TO RUN THE NEW PIPELINE ---\n",
    "\n",
    "# Make sure your save directory is defined\n",
    "save_dir = os.path.join(layout.root, 'derivatives', 'decoding', 'james_sun_cluster_decoding', conditions_save_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Run the enhanced analysis with all plots\n",
    "cv_results = run_analysis_with_plots(\n",
    "    subjects_tfr_objects=subjects_tfr_objects,\n",
    "    all_electrodes_per_subject_roi=all_electrodes_per_subject_roi,\n",
    "    rois=rois,\n",
    "    subjects=subjects,\n",
    "    condition_names=condition_names,\n",
    "    stat_func=stat_func,\n",
    "    p_thresh=p_thresh,\n",
    "    n_perm=n_perm,\n",
    "    ignore_adjacency=ignore_adjacency,\n",
    "    n_splits=n_splits,\n",
    "    n_repeats=n_repeats,\n",
    "    random_state=random_state,\n",
    "    save_dir=save_dir,\n",
    "    conditions_save_name=conditions_save_name\n",
    ")\n",
    "\n",
    "with open(os.path.join(save_dir, f'cv_results_{conditions_save_name}.pkl'), 'wb') as f:\n",
    "    pickle.dump(cv_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f83ddec",
   "metadata": {},
   "source": [
    "#### 3. Train a decoder on just the significant time-frequency clusters, test on the test trials\n",
    "\n",
    "#### 4. Repeat but with new test trials (cross-validate)\n",
    "\n",
    "#### 5. Figure out a way of plotting this. This is univariate approach, make things modular so I can do multivariate later once I figure that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f041336f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    import mne\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    import numpy as np\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    from typing import Dict, List\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    ch_names = ['CH1', 'CH2', 'CH3', 'CH4', 'CH5']\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    sfreq = 200.0\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types='seeg')\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    n_epochs, n_freqs, n_times = 2, 10, 50\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    data = np.random.randn(n_epochs, len(ch_names), n_freqs, n_times)\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    freqs = np.arange(10., 20., 1.)\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    times = np.linspace(-0.5, 1.0, n_times)\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    tfr_mock = mne.time_frequency.EpochsTFR(info, data, times, freqs)\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    subjects_tfr_objects_mock = {'sub_1': {'condition_1': tfr_mock, 'condition_2': tfr_mock.copy()}}\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    electrodes_per_subject_roi_mock = {\n",
      "        'roi_1': {'sub_1': ['CH1', 'CH2', 'CH3']},\n",
      "        'roi_2': {'sub_1': ['CH4', 'CH5']}\n",
      "    }\n",
      "Expecting nothing\n",
      "ok\n",
      "Trying:\n",
      "    subjects_tfr_objects_for_roi_1 = get_subjects_tfr_objects_for_specific_roi(\n",
      "        subjects_tfr_objects=subjects_tfr_objects_mock,\n",
      "        electrodes_per_subject_roi=electrodes_per_subject_roi_mock,\n",
      "        roi='roi_1',\n",
      "        subjects=['sub_1'],\n",
      "        condition_names=['condition_1', 'condition_2']\n",
      "    )\n",
      "Expecting nothing\n",
      "**********************************************************************\n",
      "File \"__main__\", line 40, in __main__.get_subjects_tfr_objects_for_specific_roi\n",
      "Failed example:\n",
      "    subjects_tfr_objects_for_roi_1 = get_subjects_tfr_objects_for_specific_roi(\n",
      "        subjects_tfr_objects=subjects_tfr_objects_mock,\n",
      "        electrodes_per_subject_roi=electrodes_per_subject_roi_mock,\n",
      "        roi='roi_1',\n",
      "        subjects=['sub_1'],\n",
      "        condition_names=['condition_1', 'condition_2']\n",
      "    )\n",
      "Expected nothing\n",
      "Got:\n",
      "    Subject sub_1, Condition condition_1: Selected 3/5 channels for ROI roi_1\n",
      "    Subject sub_1, Condition condition_2: Selected 3/5 channels for ROI roi_1\n",
      "Trying:\n",
      "    print(\"Original channels:\", subjects_tfr_objects_mock['sub_1']['condition_1'].ch_names)\n",
      "Expecting:\n",
      "    Original channels: ['CH1', 'CH2', 'CH3', 'CH4', 'CH5']\n",
      "ok\n",
      "Trying:\n",
      "    print(\"Filtered channels for roi_1:\", subjects_tfr_objects_for_roi_1['sub_1']['condition_1'].ch_names)\n",
      "Expecting:\n",
      "    Filtered channels for roi_1: ['CH1', 'CH2', 'CH3']\n",
      "ok\n",
      "1 items had no tests:\n",
      "    __main__\n",
      "**********************************************************************\n",
      "1 items had failures:\n",
      "   1 of  16 in __main__.get_subjects_tfr_objects_for_specific_roi\n",
      "16 tests in 2 items.\n",
      "15 passed and 1 failed.\n",
      "***Test Failed*** 1 failures.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TestResults(failed=1, attempted=16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doctest.testmod(verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
