{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "sys.path.append(\"C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/\") #need to do this cuz otherwise ieeg isn't added to path...\n",
    "\n",
    "from ieeg.navigate import channel_outlier_marker, trial_ieeg, crop_empty_data, \\\n",
    "    outliers_to_nan\n",
    "from ieeg.io import raw_from_layout, get_data\n",
    "from ieeg.timefreq.utils import crop_pad\n",
    "from ieeg.timefreq import gamma\n",
    "from ieeg.calc.scaling import rescale\n",
    "import mne\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ieeg.calc.reshape import make_data_same\n",
    "from ieeg.calc.stats import time_perm_cluster\n",
    "from ieeg.calc.mat import LabeledArray, combine\n",
    "\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas import read_csv\n",
    "import scipy.stats as stats\n",
    "import joblib\n",
    "\n",
    "from scipy.ndimage import label\n",
    "from scipy.stats import norm\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# rsa toolbox imports\n",
    "from rsatoolbox.io.mne import read_epochs\n",
    "from rsatoolbox.data.ops import merge_datasets\n",
    "from rsatoolbox.rdm import calc_rdm_movie\n",
    "from rsatoolbox.rdm.calc import _parse_input\n",
    "from rsatoolbox.util.build_rdm import _build_rdms\n",
    "from rsatoolbox.rdm import compare\n",
    "from rsatoolbox.vis import show_rdm\n",
    "from rsatoolbox.vis.timecourse import plot_timecourse\n",
    "\n",
    "from os.path import join, expanduser, basename\n",
    "import glob, json\n",
    "import numpy, tqdm, mne, pandas\n",
    "import rsatoolbox\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from ieeg.decoding.decoders import PcaLdaClassification\n",
    "from ieeg.calc.oversample import MinimumNaNSplit\n",
    "from ieeg.calc.fast import mixup\n",
    "from jim_decoding_functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['D0057','D0059', 'D0063', 'D0065', 'D0069', 'D0071', 'D0077', 'D0090', 'D0094', 'D0100', 'D0102', 'D0103']\n",
    "# subjects = ['D0107A']\n",
    "# load in subjects electrodes to rois dict. If it doesn't already exist, make it and then load it.\n",
    "filename = 'subjects_electrodestoROIs_dict.json'\n",
    "subjects_electrodestoROIs_dict = make_or_load_subjects_electrodes_to_rois_dict(filename, subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_experiment_conditions = {\n",
    "    \"Stimulus/i25.0/s25.0\": {\n",
    "        \"BIDS_events\": \"Stimulus/i25.0/s25.0\",\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus/i25.0/s75.0\": {\n",
    "        \"BIDS_events\": \"Stimulus/i25.0/s75.0\",\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus/i75.0/s25.0\": {\n",
    "        \"BIDS_events\": \"Stimulus/i75.0/s25.0\",\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus/i75.0/s75.0\": {\n",
    "        \"BIDS_events\": \"Stimulus/i75.0/s75.0\",\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus/i25.0/r25.0\": {\n",
    "        \"BIDS_events\": \"Stimulus/i25.0/r25.0\",\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus/i25.0/r75.0\": {\n",
    "        \"BIDS_events\": \"Stimulus/i25.0/r75.0\",\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus/i75.0/r25.0\": {\n",
    "        \"BIDS_events\": \"Stimulus/i75.0/r25.0\",\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus/i75.0/r75.0\": {\n",
    "        \"BIDS_events\": \"Stimulus/i75.0/r75.0\",\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus/c25.0/s25.0\": {\n",
    "        \"BIDS_events\": \"Stimulus/c25.0/s25.0\",\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus/c25.0/s75.0\": {\n",
    "        \"BIDS_events\": \"Stimulus/c25.0/s75.0\",\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus/c75.0/s25.0\": {\n",
    "        \"BIDS_events\": \"Stimulus/c75.0/s25.0\",\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus/c75.0/s75.0\": {\n",
    "        \"BIDS_events\": \"Stimulus/c75.0/s75.0\",\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus/c25.0/r25.0\": {\n",
    "        \"BIDS_events\": \"Stimulus/c25.0/r25.0\",\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus/c25.0/r75.0\": {\n",
    "        \"BIDS_events\": \"Stimulus/c25.0/r75.0\",\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus/c75.0/r25.0\": {\n",
    "        \"BIDS_events\": \"Stimulus/c75.0/r25.0\",\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus/c75.0/r75.0\": {\n",
    "        \"BIDS_events\": \"Stimulus/c75.0/r75.0\",\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# congruency_conditions = {\n",
    "#     \"Stimulus_c\": {\n",
    "#         \"BIDS_events\": [\"Stimulus/c25/s25\", \"Stimulus/c25/s75\", \"Stimulus/c75/s25\", \"Stimulus/c75/s75\", \"Stimulus/c25/r25\", \"Stimulus/c25/r75\", \"Stimulus/c75/r25\", \"Stimulus/c75/r75\"],\n",
    "#         \"congruency\": \"c\"\n",
    "#     }\n",
    "# }\n",
    "\n",
    "stimulus_conditions = {\n",
    "    \"Stimulus/BigLetters/SmallLetterh/Taskg\": {\n",
    "        \"BIDS_events\": \"Stimulus/BigLetters/SmallLetterh/Taskg\",\n",
    "        \"bigLetter\": \"s\",\n",
    "        \"smallLetter\": \"h\",\n",
    "        \"task\": \"g\"\n",
    "    },\n",
    "    \"Stimulus/BigLetters/SmallLetterh/Taskl\": {\n",
    "        \"BIDS_events\": \"Stimulus/BigLetters/SmallLetterh/Taskl\",\n",
    "        \"bigLetter\": \"s\",\n",
    "        \"smallLetter\": \"h\",\n",
    "        \"task\": \"l\"\n",
    "    },\n",
    "    \"Stimulus/BigLetters/SmallLetters/Taskg\": {\n",
    "        \"BIDS_events\": \"Stimulus/BigLetters/SmallLetters/Taskg\",\n",
    "        \"bigLetter\": \"s\",\n",
    "        \"smallLetter\": \"s\",\n",
    "        \"task\": \"g\"\n",
    "    },\n",
    "    \"Stimulus/BigLetters/SmallLetters/Taskl\": {\n",
    "        \"BIDS_events\": \"Stimulus/BigLetters/SmallLetters/Taskl\",\n",
    "        \"bigLetter\": \"s\",\n",
    "        \"smallLetter\": \"s\",\n",
    "        \"task\": \"l\"\n",
    "    },\n",
    "    \"Stimulus/BigLetterh/SmallLetterh/Taskg\": {\n",
    "        \"BIDS_events\": \"Stimulus/BigLetterh/SmallLetterh/Taskg\",\n",
    "        \"bigLetter\": \"h\",\n",
    "        \"smallLetter\": \"h\",\n",
    "        \"task\": \"g\"\n",
    "    },\n",
    "    \"Stimulus/BigLetterh/SmallLetterh/Taskl\": {\n",
    "        \"BIDS_events\": \"Stimulus/BigLetterh/SmallLetterh/Taskl\",\n",
    "        \"bigLetter\": \"h\",\n",
    "        \"smallLetter\": \"h\",\n",
    "        \"task\": \"l\"\n",
    "    },\n",
    "    \"Stimulus/BigLetterh/SmallLetters/Taskg\": {\n",
    "        \"BIDS_events\": \"Stimulus/BigLetterh/SmallLetters/Taskg\",\n",
    "        \"bigLetter\": \"h\",\n",
    "        \"smallLetter\": \"s\",\n",
    "        \"task\": \"g\"\n",
    "    },\n",
    "    \"Stimulus/BigLetterh/SmallLetters/Taskl\": {\n",
    "        \"BIDS_events\": \"Stimulus/BigLetterh/SmallLetters/Taskl\",\n",
    "        \"bigLetter\": \"h\",\n",
    "        \"smallLetter\": \"s\",\n",
    "        \"task\": \"l\"\n",
    "    }\n",
    "}\n",
    "\n",
    "response_experiment_conditions = {\n",
    "    \"Response/i25.0/s25.0\": {\n",
    "        \"BIDS_events\": \"Response/i25.0/s25.0\",\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Response/i25.0/s75.0\": {\n",
    "        \"BIDS_events\": \"Response/i25.0/s75.0\",\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Response/i75.0/s25.0\": {\n",
    "        \"BIDS_events\": \"Response/i75.0/s25.0\",\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Response/i75.0/s75.0\": {\n",
    "        \"BIDS_events\": \"Response/i75.0/s75.0\",\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Response/i25.0/r25.0\": {\n",
    "        \"BIDS_events\": \"Response/i25.0/r25.0\",\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Response/i25.0/r75.0\": {\n",
    "        \"BIDS_events\": \"Response/i25.0/r75.0\",\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Response/i75.0/r25.0\": {\n",
    "        \"BIDS_events\": \"Response/i75.0/r25.0\",\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Response/i75.0/r75.0\": {\n",
    "        \"BIDS_events\": \"Response/i75.0/r75.0\",\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Response/c25.0/s25.0\": {\n",
    "        \"BIDS_events\": \"Response/c25.0/s25.0\",\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Response/c25.0/s75.0\": {\n",
    "        \"BIDS_events\": \"Response/c25.0/s75.0\",\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Response/c75.0/s25.0\": {\n",
    "        \"BIDS_events\": \"Response/c75.0/s25.0\",\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Response/c75.0/s75.0\": {\n",
    "        \"BIDS_events\": \"Response/c75.0/s75.0\",\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Response/c25.0/r25.0\": {\n",
    "        \"BIDS_events\": \"Response/c25.0/r25.0\",\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Response/c25.0/r75.0\": {\n",
    "        \"BIDS_events\": \"Response/c25.0/r75.0\",\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Response/c75.0/r25.0\": {\n",
    "        \"BIDS_events\": \"Response/c75.0/r25.0\",\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Response/c75.0/r75.0\": {\n",
    "        \"BIDS_events\": \"Response/c75.0/r75.0\",\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    }\n",
    "}\n",
    "\n",
    "response_conditions = {\n",
    "    \"Response/BigLetters/SmallLetterh/Taskg\": {\n",
    "        \"BIDS_events\": \"Response/BigLetters/SmallLetterh/Taskg\",\n",
    "        \"bigLetter\": \"s\",\n",
    "        \"smallLetter\": \"h\",\n",
    "        \"task\": \"g\"\n",
    "    },\n",
    "    \"Response/BigLetters/SmallLetterh/Taskl\": {\n",
    "        \"BIDS_events\": \"Response/BigLetters/SmallLetterh/Taskl\",\n",
    "        \"bigLetter\": \"s\",\n",
    "        \"smallLetter\": \"h\",\n",
    "        \"task\": \"l\"\n",
    "    },\n",
    "    \"Response/BigLetters/SmallLetters/Taskg\": {\n",
    "        \"BIDS_events\": \"Response/BigLetters/SmallLetters/Taskg\",\n",
    "        \"bigLetter\": \"s\",\n",
    "        \"smallLetter\": \"s\",\n",
    "        \"task\": \"g\"\n",
    "    },\n",
    "    \"Response/BigLetters/SmallLetters/Taskl\": {\n",
    "        \"BIDS_events\": \"Response/BigLetters/SmallLetters/Taskl\",\n",
    "        \"bigLetter\": \"s\",\n",
    "        \"smallLetter\": \"s\",\n",
    "        \"task\": \"l\"\n",
    "    },\n",
    "    \"Response/BigLetterh/SmallLetterh/Taskg\": {\n",
    "        \"BIDS_events\": \"Response/BigLetterh/SmallLetterh/Taskg\",\n",
    "        \"bigLetter\": \"h\",\n",
    "        \"smallLetter\": \"h\",\n",
    "        \"task\": \"g\"\n",
    "    },\n",
    "    \"Response/BigLetterh/SmallLetterh/Taskl\": {\n",
    "        \"BIDS_events\": \"Response/BigLetterh/SmallLetterh/Taskl\",\n",
    "        \"bigLetter\": \"h\",\n",
    "        \"smallLetter\": \"h\",\n",
    "        \"task\": \"l\"\n",
    "    },\n",
    "    \"Response/BigLetterh/SmallLetters/Taskg\": {\n",
    "        \"BIDS_events\": \"Response/BigLetterh/SmallLetters/Taskg\",\n",
    "        \"bigLetter\": \"h\",\n",
    "        \"smallLetter\": \"s\",\n",
    "        \"task\": \"g\"\n",
    "    },\n",
    "    \"Response/BigLetterh/SmallLetters/Taskl\": {\n",
    "        \"BIDS_events\": \"Response/BigLetterh/SmallLetters/Taskl\",\n",
    "        \"bigLetter\": \"h\",\n",
    "        \"smallLetter\": \"s\",\n",
    "        \"task\": \"l\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='GlobalLocal'\n",
    "\n",
    "conditions = stimulus_conditions # toggle\n",
    "\n",
    "if conditions == stimulus_conditions or stimulus_experiment_conditions:\n",
    "    epochs_root_file = \"Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8\"\n",
    "elif conditions == response_conditions or response_experiment_conditions:\n",
    "    epochs_root_file = \"Response_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8\"\n",
    "else:\n",
    "    raise ValueError(\"Unknown condition type.\")\n",
    "\n",
    "condition_names = list(conditions.keys()) # get the condition names as a list\n",
    "\n",
    "# This breaks if just_HG_ev1_rescaled is set to False currently. Fix this! 8/11\n",
    "subjects_mne_objects = create_subjects_mne_objects_dict(subjects=subjects, epochs_root_file=epochs_root_file, conditions=conditions, task=\"GlobalLocal\", just_HG_ev1_rescaled=True, acc_trials_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_names = list(conditions.keys()) # get the condition names as a list\n",
    "condition_names_bids = [condition['BIDS_events'] for condition in conditions.values()] # get the condition names in bids format\n",
    "condition_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get significant channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_chans_per_subject = get_sig_chans_per_subject(subjects, epochs_root_file, task='GlobalLocal', LAB_root=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a list of subjects\n",
    "root_dir = rf\"C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your ROIs dictionary and other parameters\n",
    "rois_dict = {\n",
    "    'lpfc': [\"G_front_inf-Opercular\", \"G_front_inf-Orbital\", \"G_front_inf-Triangul\", \"G_front_middle\", \"G_front_sup\", \"Lat_Fis-ant-Horizont\", \"Lat_Fis-ant-Vertical\", \"S_circular_insula_ant\", \"S_circular_insula_sup\", \"S_front_inf\", \"S_front_middle\", \"S_front_sup\"],\n",
    "    'v1': [\"G_oc-temp_med-Lingual\", \"S_calcarine\", \"G_cuneus\"],\n",
    "    'occ': [\"G_cuneus\", \"G_and_S_occipital_inf\", \"G_occipital_middle\", \"G_occipital_sup\", \"G_oc-temp_lat-fusifor\", \"G_oc-temp_med-Lingual\", \"Pole_occipital\", \"S_calcarine\", \"S_oc_middle_and_Lunatus\", \"S_oc_sup_and_transversal\", \"S_occipital_ant\"],\n",
    "    'occ_filtered': [],\n",
    "    'occ_best_filtered': []\n",
    "}\n",
    "\n",
    "rois = list(rois_dict.keys())\n",
    "# Assuming you have subjects_electrodestoROIs_dict and sig_chans_per_subject dictionaries\n",
    "electrodes_per_subject_roi, sig_electrodes_per_subject_roi, = make_sig_electrodes_per_subject_and_roi_dict(\n",
    "    rois_dict, subjects_electrodestoROIs_dict, sig_chans_per_subject\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's manually make the occ filtered sig electrodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_electrodes_per_subject_roi['occ_filtered'] = {\n",
    "    'D0057': [],\n",
    "    'D0059': [],\n",
    "    'D0071': ['RO1', 'RO10'], #RO10 is iffy, big drop from fix onset\n",
    "    'D0077': ['ROPM1', 'ROPM8'],\n",
    "    'D0090': ['RTPO1', 'RTPI1'],\n",
    "    'D0100': ['LOAI12', 'LOAI13', 'LOAI14', 'LOAI15', 'LOMI9', 'LOMI11', 'LOPI8', 'LOPI9', 'LOPM8', 'LOPM9', 'LPPI7', 'LPPI8', 'LPPI9'],\n",
    "    'D0102': ['RTPI1'],\n",
    "    'D0103': ['LTPI2', 'LTPI3', 'LTPI4']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now make the best of the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_electrodes_per_subject_roi['occ_best_filtered'] = {\n",
    "    'D0057': [],\n",
    "    'D0059': [],\n",
    "    'D0071': [],\n",
    "    'D0077': [],\n",
    "    'D0090': [],\n",
    "    'D0100': ['LOAI12', 'LOMI9', 'LOPI8', 'LOPI9', 'LOPM8', 'LOPM9'],\n",
    "    'D0102': ['RTPI1'],\n",
    "    'D0103': ['LTPI2', 'LTPI3', 'LTPI4']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path where you want to save the dictionary\n",
    "save_path = 'sig_electrodes_per_subject_roi.json'\n",
    "\n",
    "# Use json to save the dictionary\n",
    "with open(save_path, 'w') as file:\n",
    "    json.dump(sig_electrodes_per_subject_roi, file, indent=4)\n",
    "\n",
    "print(f\"Dictionary saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get electrode counts for each roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_electrodes_info = calculate_total_electrodes(sig_electrodes_per_subject_roi, electrodes_per_subject_roi)\n",
    "for roi, counts in total_electrodes_info.items():\n",
    "    print(f\"Total number of {roi} electrodes across all subjects:\", counts['total_electrodes'])\n",
    "    print(f\"Total number of significant {roi} electrodes across all subjects:\", counts['total_significant_electrodes'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if any subjects have a weird sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'subjects_mne_objects' is your dictionary containing MNE objects for each subject\n",
    "sampling_rate = 256\n",
    "subject_rates = check_sampling_rates(subjects_mne_objects, expected_sampling_rate=sampling_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a dat dict like in the Temporal example from rsatoolbox 7/19  \n",
    "#### Data Dictionary (dat) Structure\n",
    "- **roi1**\n",
    "  - **data**: 3D data array for this ROI\n",
    "  - **channel_names**: List of significant channels for this ROI\n",
    "  - **cond_names**: Dictionary where keys are condition names and values are integer indices\n",
    "  - **cond_idx**: 1D array where each entry is an integer index corresponding to a cond_name\n",
    "  - **times**: 1D array of times, corresponding to each sample\n",
    "- **roi2**\n",
    "  - **data**: 3D data array for this ROI\n",
    "  - **channel_names**: List of significant channels for this ROI\n",
    "  - **cond_names**: Dictionary where keys are condition names and values are integer indices\n",
    "  - **cond_idx**: 1D array where each entry is an integer index corresponding to a cond_name\n",
    "  - **times**: 1D array of times, corresponding to each sample\n",
    "- **roiX**\n",
    "  - **data**: 3D data array for this ROI\n",
    "  - **channel_names**: List of significant channels for this ROI\n",
    "  - **cond_names**: Dictionary where keys are condition names and values are integer indices\n",
    "  - **cond_idx**: 1D array where each entry is an integer index corresponding to a cond_name\n",
    "  - **times**: 1D array of times, corresponding to each sample\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example call to the function (you need to replace the arguments with actual data)\n",
    "dat = prepare_data_for_temporal_dataset(subjects_mne_objects, condition_names, rois, subjects, sig_electrodes_per_subject_roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make roi_labeled_arrays, a dict where the keys are rois and the values are LabeledArrays. Index the same way as a nested dict. Use .labels to get labels from current level.\n",
    "\n",
    "\n",
    "#### roi_labeled_arrays structure\n",
    "- **roi1**: ROI name, string\n",
    "  - **conditions**: condition name\n",
    "    - **trials**: This is the maximal number of trials across subjects for this condition, filled with nans for subjects who don't have this many trials\n",
    "      - **channels**: This is the number of channels in the roi, each channel is labeled as subject-channel name. Concatenated across subjects.\n",
    "        - **samples**: 1 sample as a float. This is the time for this sample.\n",
    "\n",
    "- **roi2**: ROI name, string\n",
    "  - **conditions**: condition name\n",
    "    - **trials**: This is the maximal number of trials across subjects for this condition, filled with nans for subjects who don't have this many trials\n",
    "      - **channels**: This is the number of channels in the roi, each channel is labeled as subject-channel name. Concatenated across subjects.\n",
    "        - **samples**: 1 sample as a float. This is the time for this sample.\n",
    "\n",
    "- **roiX**: ROI name, string\n",
    "  - **conditions**: condition name\n",
    "    - **trials**: This is the maximal number of trials across subjects for this condition, filled with nans for subjects who don't have this many trials\n",
    "      - **channels**: This is the number of channels in the roi, each channel is labeled as subject-channel name. Concatenated across subjects.\n",
    "        - **samples**: 1 sample as a float. This is the time for this sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def put_data_in_labeled_array_per_roi_subject(subjects_mne_objects, condition_names, rois, subjects, sig_electrodes_per_subject_roi):\n",
    "    \"\"\"\n",
    "    Organize the MNE data into separate LabeledArray for each ROI and subject, with NaN padding for trial dimension.\n",
    "    Concatenates subject data along the channels axis.\n",
    "    \n",
    "    Parameters:\n",
    "    - subjects_mne_objects: Dictionary of MNE objects, structured as {subject: {condition: MNE epoch objects}}\n",
    "    - condition_names: List of condition names.\n",
    "    - rois: List of region of interest (ROI) names.\n",
    "    - subjects: List of subjects.\n",
    "    - sig_electrodes_per_subject_roi: Dictionary that maps ROIs to subjects and their corresponding electrodes.\n",
    "    \n",
    "    Returns:\n",
    "    - roi_labeled_arrays: Dictionary of LabeledArrays for each ROI. \n",
    "                          Each LabeledArray concatenates the subject data along the channels dimension.\n",
    "                          Dimensions of LabeledArray: [Conditions, Electrodes (concatenated), Trials, Timepoints]\n",
    "    \"\"\"\n",
    "    roi_labeled_arrays = {}\n",
    "\n",
    "    # Loop through each ROI\n",
    "    for roi in rois:\n",
    "        roi_labeled_array = None\n",
    "\n",
    "        # Find the maximum number of trials for each condition across all subjects\n",
    "        max_trials_per_condition = {condition: 0 for condition in condition_names}\n",
    "\n",
    "        # First pass to find the max number of trials per condition\n",
    "        for sub in subjects:\n",
    "            sig_electrodes = sig_electrodes_per_subject_roi.get(roi, {}).get(sub, [])\n",
    "            if not sig_electrodes:\n",
    "                continue\n",
    "            for condition_name in condition_names:\n",
    "                epochs = subjects_mne_objects[sub][condition_name]['HG_ev1_rescaled'].copy().pick(sig_electrodes)\n",
    "                epochs_data = epochs.get_data(copy=True)  # Shape: (trials, channels, timepoints)\n",
    "                max_trials_per_condition[condition_name] = max(max_trials_per_condition[condition_name], epochs_data.shape[0])\n",
    "\n",
    "        # Second pass to gather data for each subject and condition\n",
    "        for sub in subjects:\n",
    "            sig_electrodes = sig_electrodes_per_subject_roi.get(roi, {}).get(sub, [])\n",
    "            if not sig_electrodes:\n",
    "                continue\n",
    "\n",
    "            subject_nested_dict = {}\n",
    "\n",
    "            # Get channel names for this subject's ROI\n",
    "            sub_channel_names = [f\"{sub}-{sig_electrode}\" for sig_electrode in sig_electrodes]\n",
    "\n",
    "            # Loop through each condition\n",
    "            for condition_name in condition_names:\n",
    "                # Extract the epoch data for the current condition and subject, pick only the electrodes of interest\n",
    "                epochs = subjects_mne_objects[sub][condition_name]['HG_ev1_rescaled'].copy().pick(sig_electrodes)\n",
    "                epochs_data = epochs.get_data(copy=True)  # Shape: (trials, channels, timepoints)\n",
    "                # Get the number of trials and the target number of trials\n",
    "                n_trials, n_channels, n_timepoints = epochs_data.shape\n",
    "                max_trials = max_trials_per_condition[condition_name]\n",
    "\n",
    "                # Pad with NaNs if necessary\n",
    "                if n_trials < max_trials:\n",
    "                    # Create a padded array with NaNs\n",
    "                    padded_data = np.full((max_trials, n_channels, n_timepoints), np.nan)\n",
    "                    padded_data[:n_trials, :, :] = epochs_data  # Copy existing data into the padded array\n",
    "                else:\n",
    "                    padded_data = epochs_data\n",
    "\n",
    "                subject_nested_dict[condition_name] = padded_data\n",
    "            times = epochs.times # same times across conditions, so just grab times of last condition\n",
    "            str_times = [str(time) for time in times]\n",
    "            np_array_str_times = np.array(str_times) # make this a numpy array of strings so it plays nice with aaron's code\n",
    "\n",
    "            # Create a LabeledArray for the subject\n",
    "            subject_labeled_array = LabeledArray.from_dict(subject_nested_dict)\n",
    "            subject_labeled_array.labels[2] = sub_channel_names  # Add channel labels for this subject\n",
    "            subject_labeled_array.labels[3] = np_array_str_times # Add time labels for this subject. This breaks things. But why????? 9/10. \n",
    "            \n",
    "            if roi_labeled_array is None:\n",
    "                roi_labeled_array = subject_labeled_array\n",
    "            else:\n",
    "                roi_labeled_array = roi_labeled_array.concatenate(subject_labeled_array, axis=2) # concatenate along channels axis\n",
    "\n",
    "        # Add the concatenated LabeledArray to the ROI dictionary\n",
    "        roi_labeled_arrays[roi] = roi_labeled_array\n",
    "\n",
    "    return roi_labeled_arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example call to the function\n",
    "roi_labeled_arrays = put_data_in_labeled_array_per_roi_subject(subjects_mne_objects, condition_names, rois, subjects, sig_electrodes_per_subject_roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up time bins, time indexing, array reshaping, and decoding functions 9/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_bins(time_points, bin_size=0.2):\n",
    "    \"\"\"\n",
    "    Create time bins from the given time points array.\n",
    "\n",
    "    Parameters:\n",
    "    - time_points: 1D numpy array of time points.\n",
    "    - bin_size: The size of each time bin in seconds (default is 0.1s, which is 100 ms).\n",
    "\n",
    "    Returns:\n",
    "    - bins: A list of numpy arrays, where each array contains the time points within that bin.\n",
    "    \"\"\"\n",
    "    bins = []\n",
    "    start_time = np.min(time_points)\n",
    "    end_time = np.max(time_points)\n",
    "    current_time = start_time\n",
    "    \n",
    "    while current_time < end_time:\n",
    "        next_time = current_time + bin_size\n",
    "        if next_time > end_time:\n",
    "            next_time = end_time\n",
    "        bins.append(time_points[(time_points >= current_time) & (time_points < next_time)])\n",
    "        current_time = next_time\n",
    "\n",
    "    # Ensure that the last bin includes the endpoint\n",
    "    if time_points[-1] not in bins[-1]:\n",
    "        bins[-1] = np.append(bins[-1], time_points[-1])\n",
    "\n",
    "    return bins\n",
    "\n",
    "def get_bin_centers(time_bins):\n",
    "    \"\"\"\n",
    "    Calculate the center of each time bin.\n",
    "\n",
    "    Parameters:\n",
    "    - time_bins: List of numpy arrays, where each array represents a time bin.\n",
    "\n",
    "    Returns:\n",
    "    - bin_centers: A list of bin center values (floats).\n",
    "    \"\"\"\n",
    "    bin_centers = [np.mean(bin) for bin in time_bins]\n",
    "    return bin_centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_timepoints_by_bins(labeled_array, time_bins):\n",
    "    \"\"\"\n",
    "    Index time points from a LabeledArray using given time bins.\n",
    "\n",
    "    Parameters:\n",
    "    - labeled_array: The LabeledArray from which to extract timepoints (e.g., roi_labeled_arrays['lpfc']).\n",
    "    - time_bins: List of numpy arrays, where each array represents a time bin.\n",
    "\n",
    "    Returns:\n",
    "    - bin_indices: A list of lists, where each list contains the indices of the time points that fall into the corresponding bin.\n",
    "    \"\"\"\n",
    "    # Get the time points from the LabeledArray\n",
    "    time_points = labeled_array.labels[3].astype(float)  # Convert the time labels to floats\n",
    "\n",
    "    bin_indices = []\n",
    "    \n",
    "    # Loop over the time bins and find the corresponding indices\n",
    "    for time_bin in time_bins:\n",
    "        indices_in_bin = np.where(np.isin(time_points, time_bin))[0]  # Get indices of time points in this bin\n",
    "        bin_indices.append(indices_in_bin)\n",
    "    \n",
    "    return bin_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hm to use mixup i think i shouldn't remove nan values in reshape_labeled_array_for_decoding. But then how to concatenate trials? Actually i think might be fine to just not do the nan removal step. Try this tonight 9/23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_labeled_array_for_decoding(labeled_array):\n",
    "    \"\"\"\n",
    "    Reshape a single LabeledArray, removing trials that have NaN values and reshaping to be trials by channels*timepoints.\n",
    "\n",
    "    Parameters:\n",
    "    - labeled_array: A LabeledArray where the 0th axis labels are conditions, 1st axis labels are trials, \n",
    "                     2nd axis labels are channels, and 3rd axis labels are timepoints.\n",
    "\n",
    "    Returns:\n",
    "    - reshaped_data: Dictionary where keys are condition names and values are reshaped arrays (trials x channels*timepoints).\n",
    "    \"\"\"\n",
    "    reshaped_labeled_array = {}\n",
    "    \n",
    "    # Extract the condition labels (which are in labeled_array.labels[0])\n",
    "    condition_names = labeled_array.labels[0]\n",
    "\n",
    "    # Loop over each condition\n",
    "    for condition_name in condition_names:\n",
    "        # Extract the data for the current condition\n",
    "        condition_data = labeled_array[condition_name]\n",
    "        \n",
    "        # # Find the indices of trials that do not contain NaNs\n",
    "        # valid_trial_indices = ~np.isnan(condition_data).any(axis=(1, 2))\n",
    "\n",
    "        # # Find the indices of trials that are not trial outliers (all NaNs)\n",
    "        valid_trial_indices = ~np.isnan(condition_data).all(axis=(1, 2))\n",
    "\n",
    "        # Use the take method to select only the valid trials\n",
    "        condition_data_clean = condition_data.take(indices=np.where(valid_trial_indices)[0], axis=0)\n",
    "        \n",
    "        # Combine the channels and timepoints axes (axis 1 and axis 2) into a single axis\n",
    "        channel_time_combined_array = condition_data_clean.reshape(condition_data_clean.shape[0], -1)\n",
    "        \n",
    "        # # combine channels and timepoints axes into a single axis, but include all nan trials too. Hm should I get rid of trial outliers though? Those are NaN for all channels right..? Instead of just some?\n",
    "        # channel_time_combined_array = condition_data.reshape(condition_data.shape[0], -1)\n",
    "        \n",
    "        # Store the processed data for this condition\n",
    "        reshaped_labeled_array[condition_name] = channel_time_combined_array\n",
    "\n",
    "    return reshaped_labeled_array\n",
    "\n",
    "def process_all_rois_for_decoding(roi_labeled_arrays):\n",
    "    \"\"\"\n",
    "    Loop through all ROIs and apply reshaping function to each LabeledArray.\n",
    "\n",
    "    Parameters:\n",
    "    - roi_labeled_arrays: Dictionary of LabeledArrays for each ROI.\n",
    "\n",
    "    Returns:\n",
    "    - all_processed_data: Dictionary where keys are ROIs and values are dictionaries of reshaped data for each condition.\n",
    "    \"\"\"\n",
    "    reshaped_roi_labeled_arrays_for_decoding = {}\n",
    "\n",
    "    # Loop through each ROI in the dictionary\n",
    "    for roi, labeled_array in roi_labeled_arrays.items():\n",
    "        # Apply the reshaping function to the current labeled array\n",
    "        reshaped_labeled_array = reshape_labeled_array_for_decoding(labeled_array)\n",
    "\n",
    "        # Store the reshaped data for this ROI\n",
    "        reshaped_roi_labeled_arrays_for_decoding[roi] = reshaped_labeled_array\n",
    "\n",
    "    return reshaped_roi_labeled_arrays_for_decoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_conditions_by_string(roi_labeled_arrays, roi, strings_to_find):\n",
    "    \"\"\"\n",
    "    Concatenate trials across condition names that contain specific strings.\n",
    "    Assign labels based on the groupings of the conditions.\n",
    "\n",
    "    Parameters:\n",
    "    - roi_labeled_arrays: Dictionary of LabeledArrays for each ROI.\n",
    "    - roi: The specific ROI to process.\n",
    "    - strings_to_find: List of strings or list of lists of strings to search for in condition names.\n",
    "                      If a list of strings is provided, each string is treated as its own condition group.\n",
    "                      If a list of lists is provided, each sublist represents a group of conditions.\n",
    "\n",
    "    Returns:\n",
    "    - concatenated_data: The concatenated trials by channels*timepoints across the matching conditions.\n",
    "    - labels: A numpy array of labels (0, 1, 2, ...) corresponding to each group of conditions.\n",
    "    - cats: Dictionary of {condition_name: index} for decoding.\n",
    "    \"\"\"\n",
    "    concatenated_data = []\n",
    "    labels = []\n",
    "    cats = {}\n",
    "\n",
    "    # Track current label index\n",
    "    current_label = 0\n",
    "\n",
    "    # Normalize strings_to_find so each entry is a list (whether it's a string or a list of strings)\n",
    "    if isinstance(strings_to_find, list) and all(isinstance(s, str) for s in strings_to_find):\n",
    "        # If it's a flat list of strings, convert each string into its own single-item list\n",
    "        strings_to_find = [[s] for s in strings_to_find]\n",
    "\n",
    "    # Iterate over each group (whether it's a single string or a list of strings)\n",
    "    for string_group in strings_to_find:\n",
    "        # Find condition names that match any of the strings in the current string_group\n",
    "        matching_conditions = [cond for cond in roi_labeled_arrays[roi].keys() if any(s in cond for s in string_group)]\n",
    "\n",
    "        if not matching_conditions:\n",
    "            continue\n",
    "\n",
    "        # Concatenate data for all matching conditions\n",
    "        data_to_concatenate = []\n",
    "        for cond in matching_conditions:\n",
    "            # Extract data for the current condition\n",
    "            data = roi_labeled_arrays[roi][cond]  # Shape: (trials, channels, timepoints)\n",
    "            data_to_concatenate.append(data)\n",
    "            \n",
    "            # Update labels for the current condition group\n",
    "            labels.extend([current_label] * data.shape[0])\n",
    "        \n",
    "        # Check if we have data to concatenate for this condition group\n",
    "        if data_to_concatenate:\n",
    "            concatenated_data.append(np.concatenate(data_to_concatenate, axis=0))\n",
    "\n",
    "        # Assign current label to the condition group (based on the first string in the group for reference)\n",
    "        cats[tuple(string_group)] = current_label\n",
    "        current_label += 1\n",
    "\n",
    "    # Ensure there is data to concatenate\n",
    "    if not concatenated_data:\n",
    "        raise ValueError(f\"No matching conditions found for ROI: {roi} and strings: {strings_to_find}\")\n",
    "\n",
    "    # Concatenate all condition data along the trials axis\n",
    "    concatenated_data = np.concatenate(concatenated_data, axis=0)\n",
    "    \n",
    "    return concatenated_data, np.array(labels), cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up reshaped arrays for each time bin 9/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming temporal_datasets is a dictionary\n",
    "first_sub = next(iter(subjects_mne_objects.keys()))\n",
    "first_cond = next(iter(subjects_mne_objects[first_sub].keys()))\n",
    "\n",
    "time_points = subjects_mne_objects[first_sub][first_cond]['HG_ev1_rescaled'].times\n",
    "\n",
    "# Create time bins\n",
    "bin_size = 0.2\n",
    "time_bins = create_time_bins(time_points, bin_size=bin_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeBinnedROILabeledArray:\n",
    "    def __init__(self, bin_center, bin_size, indices, data):\n",
    "        \"\"\"\n",
    "        Initialize a TimeBinnedROILabeledArray object.\n",
    "\n",
    "        Parameters:\n",
    "        - bin_center: The center of the time bin.\n",
    "        - indices: The indices of the time points that belong to this bin.\n",
    "        - data: The reshaped ROI labeled arrays for decoding.\n",
    "        \"\"\"\n",
    "        self.bin_center = bin_center\n",
    "        self.indices = indices\n",
    "        self.data = data\n",
    "        self.bin_size = bin_size\n",
    "        self.bin_start = bin_center - bin_size/2\n",
    "        self.bin_end = bin_center + bin_size/2\n",
    "        \n",
    "class TimeBinnedArraysDataManager:\n",
    "    def __init__(self, roi_labeled_arrays, time_bins, bin_size, rois):\n",
    "        \"\"\"\n",
    "        Initialize the class to manage all time-binned reshaped ROI labeled arrays.\n",
    "\n",
    "        Parameters:\n",
    "        - roi_labeled_arrays: Dictionary of LabeledArrays for each ROI.\n",
    "        - time_bins: The time bins generated for the data.\n",
    "        - rois: The regions of interest to process.\n",
    "        \"\"\"\n",
    "        self.roi_labeled_arrays = roi_labeled_arrays\n",
    "        self.time_bins = time_bins\n",
    "        self.rois = rois\n",
    "        self.bin_centers = get_bin_centers(time_bins)\n",
    "        self.bin_size = bin_size\n",
    "        self.bin_indices = None\n",
    "        self.binned_data = []  # List of TimeBinnedROILabeledArray objects\n",
    "\n",
    "    def index_time_bins(self):\n",
    "        \"\"\"Index the time points for each bin.\"\"\"\n",
    "        first_roi = next(iter(self.roi_labeled_arrays.keys()))\n",
    "        self.bin_indices = index_timepoints_by_bins(self.roi_labeled_arrays[first_roi], self.time_bins)\n",
    "\n",
    "    def process_bins(self):\n",
    "        \"\"\"Process each time bin and reshape the ROI labeled arrays for decoding.\"\"\"\n",
    "        self.index_time_bins()  # Ensure bin indexing is done before processing bins\n",
    "\n",
    "        for i, indices in enumerate(self.bin_indices):\n",
    "            bin_center = self.bin_centers[i]\n",
    "            roi_labeled_arrays_this_bin = {}\n",
    "\n",
    "            for roi in self.rois:\n",
    "                # Index the ROI labeled array and grab just these time indices\n",
    "                roi_labeled_arrays_this_bin[roi] = self.roi_labeled_arrays[roi][:, :, :, indices]\n",
    "\n",
    "            # Drop trial NaNs and concatenate channel-time for the time-indexed arrays\n",
    "            reshaped_roi_labeled_arrays_for_decoding_this_bin = process_all_rois_for_decoding(roi_labeled_arrays_this_bin)\n",
    "\n",
    "            # Create a TimeBinnedROILabeledArray object and store it\n",
    "            self.binned_data.append(TimeBinnedROILabeledArray(bin_center, bin_size, indices, reshaped_roi_labeled_arrays_for_decoding_this_bin))\n",
    "\n",
    "    def get_binned_data(self):\n",
    "        \"\"\"Return the list of TimeBinnedROILabeledArray objects.\"\"\"\n",
    "        return self.binned_data\n",
    "\n",
    "# make the wrapper class first with all the attributes, then get the list of individual time binned arrays. Name better?\n",
    "TimeBinnedArraysDataManager = TimeBinnedArraysDataManager(roi_labeled_arrays, time_bins, bin_size, rois)\n",
    "TimeBinnedArraysDataManager.process_bins()\n",
    "time_binned_arrays = TimeBinnedArraysDataManager.get_binned_data()\n",
    "\n",
    "# Print the first bin's center for example\n",
    "time_binned_arrays[0].data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up decoding output directory and conditions to compare 9/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine LAB_root based on the operating system\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "LAB_root = os.path.join(HOME, \"Box\", \"CoganLab\") if os.name == 'nt' else os.path.join(HOME, \"Library\", \"CloudStorage\", \"Box-Box\", \"CoganLab\")\n",
    "save_dir = os.path.join(LAB_root, 'BIDS-1.1_GlobalLocal', 'BIDS', 'derivatives', 'decoding', 'figs')\n",
    "\n",
    "if conditions == stimulus_experiment_conditions:\n",
    "    strings_to_find = [['c25.0', 'c75.0'], ['i25.0', 'i75.0']]  # Strings to search for in condition names\n",
    "    condition_0_name = 'congruent' # this is for plot names\n",
    "    condition_1_name = 'incongruent'\n",
    "elif conditions == stimulus_conditions:\n",
    "    strings_to_find = ['BigLetters', 'BigLetterh']\n",
    "    condition_0_name = 'BigLetters'\n",
    "    condition_1_name = 'BigLetterh'\n",
    "\n",
    "    # strings_to_find = ['SmallLetters', 'SmallLetters']\n",
    "    # condition_0_name = 'SmallLetters'\n",
    "    # condition_1_name = 'SmallLetterh'\n",
    "\n",
    "    # strings_to_find = ['Taskg', 'Taskl']\n",
    "    # condition_0_name = 'Taskg'\n",
    "    # condition_1_name = 'Taskl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do decoding on the reshaped time indexed arrays 9/12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try a class for storing and plotting the decoding scores for each roi 9/13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROIScoreData:\n",
    "    def __init__(self, roi, bin_size):\n",
    "        self.roi = roi\n",
    "        self.bin_size = bin_size\n",
    "        self.bin_centers = []\n",
    "        self.accuracy_scores = []\n",
    "        self.precision_scores = []\n",
    "        self.recall_scores = []\n",
    "        self.f1_scores = []\n",
    "        self.d_prime_scores = []\n",
    "\n",
    "    def add_scores(self, bin_center, accuracy, precision, recall, f1, d_prime):\n",
    "        self.bin_centers.append(bin_center)\n",
    "        self.accuracy_scores.append(accuracy)\n",
    "        self.precision_scores.append(precision)\n",
    "        self.recall_scores.append(recall)\n",
    "        self.f1_scores.append(f1)\n",
    "        self.d_prime_scores.append(d_prime)\n",
    "\n",
    "    def plot_scores(self, strings_to_find, save_dir):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Plot accuracy\n",
    "        plt.plot(self.bin_centers, self.accuracy_scores, label='Accuracy', marker='o')\n",
    "\n",
    "        # # Plot precision\n",
    "        # plt.plot(self.bin_centers, self.precision_scores, label='Precision', marker='o')\n",
    "\n",
    "        # # Plot recall\n",
    "        # plt.plot(self.bin_centers, self.recall_scores, label='Recall', marker='o')\n",
    "\n",
    "        # # Plot f1 score\n",
    "        # plt.plot(self.bin_centers, self.f1_scores, label='F1 Score', marker='o')\n",
    "\n",
    "        # Plot d-prime\n",
    "        plt.plot(self.bin_centers, self.d_prime_scores, label='D-Prime', marker='o')\n",
    "\n",
    "        # Set plot details\n",
    "        plt.xlabel('Time Bin Centers (s)')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title(f'{condition_0_name} vs. {condition_1_name} Decoding Over Time for ROI: {self.roi}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.ylim(0,1)\n",
    "\n",
    "        # Save the figure\n",
    "        file_name = f'{self.roi}_{condition_0_name}_vs_{condition_1_name}_decoding_over_time_dprime_acc.png'\n",
    "        plt.savefig(os.path.join(save_dir, file_name))\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to hold the ROIScoreData objects\n",
    "roi_score_data = {}\n",
    "\n",
    "# Process each time bin and region of interest\n",
    "for roi in rois:\n",
    "    # Initialize the ROIScoreData object for the current ROI\n",
    "    roi_scores = ROIScoreData(roi, bin_size)\n",
    "    \n",
    "    for time_binned_array in time_binned_arrays:\n",
    "\n",
    "        # Concatenate the trials and get labels\n",
    "        concatenated_data, labels, cats = concatenate_conditions_by_string(time_binned_array.data, roi, strings_to_find)\n",
    "        print(concatenated_data.shape)\n",
    "        \n",
    "        # Iterate over the groups in strings_to_find\n",
    "        for string_group in strings_to_find:\n",
    "            # If it's a single string (old behavior)\n",
    "            if isinstance(string_group, str):\n",
    "                # Count trials for the single condition group\n",
    "                num_trials = len([label for label in labels if label == cats[(string_group,)]])\n",
    "                print(f\"{string_group} has {num_trials} trials\")\n",
    "            \n",
    "            # If it's a list of strings (new behavior)\n",
    "            elif isinstance(string_group, list):\n",
    "                # Count trials for the group of strings (e.g., ['i25.0', 'i75.0'])\n",
    "                num_trials = len([label for label in labels if label == cats[tuple(string_group)]])\n",
    "                print(f\"{string_group} has {num_trials} trials\")\n",
    "\n",
    "        # Create a Decoder and run cross-validation\n",
    "        decoder = Decoder(cats, 0.80, oversample=True, n_splits=5, n_repeats=5)\n",
    "\n",
    "        # Use the concatenated data for the decoder and get scores. Use normalize of none or else the scores will be skewed.\n",
    "        cm_avg, scores = decoder.cv_cm_return_scores(concatenated_data, labels, normalize=None)\n",
    "\n",
    "        # Store the decoding scores and bin center for plotting\n",
    "        bin_center = time_binned_array.bin_center  # Get the bin center for x-axis\n",
    "\n",
    "        # Add scores to the ROIScoreData object\n",
    "        roi_scores.add_scores(\n",
    "            bin_center,\n",
    "            scores['accuracy'],\n",
    "            np.mean(scores['precision']),  # Average across classes if multi-class\n",
    "            np.mean(scores['recall']),     # Average across classes\n",
    "            np.mean(scores['f1']),         # Average across classes\n",
    "            np.mean(scores['d_prime'])     # Average across classes\n",
    "        )\n",
    "\n",
    "    # Save the scores for this ROI\n",
    "    roi_score_data[roi] = roi_scores\n",
    "\n",
    "# Plot the decoding scores for each ROI\n",
    "for roi, roi_scores in roi_score_data.items():\n",
    "    roi_scores.plot_scores(strings_to_find, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do decoding across all timepoints (hm refactor this to use the ROILabeledArray objects maybe 9/14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_plot_confusion_matrix_for_rois(roi_labeled_arrays, rois, strings_to_find, condition_0_name, condition_1_name, save_dir, time_interval_name=None, n_splits=5, n_repeats=5):\n",
    "    \"\"\"\n",
    "    Compute the confusion matrix for each ROI and return it. Does this using all time points for training.\n",
    "\n",
    "    Parameters:\n",
    "    - roi_labeled_arrays: Dictionary containing the reshaped data for each ROI.\n",
    "    - rois: List of regions of interest (ROIs) to process.\n",
    "    - strings_to_find: List of strings or string groups to identify conditions.\n",
    "    - condition_0_name: Name for condition 0 (used for labeling).\n",
    "    - condition_1_name: Name for condition 1 (used for labeling).\n",
    "    - save_dir: Directory to save the confusion matrix plots.\n",
    "    - time_interval_name: Optional string to add to the filename for the time interval (e.g., 'pre_stimulus' or 'post_stimulus').\n",
    "\n",
    "    Returns:\n",
    "    - confusion_matrices: Dictionary containing confusion matrices for each ROI.\n",
    "    \"\"\"\n",
    "    confusion_matrices = {}\n",
    "\n",
    "    reshaped_roi_labeled_arrays_for_decoding = process_all_rois_for_decoding(roi_labeled_arrays)\n",
    "\n",
    "    for roi in rois:\n",
    "        print(f\"Processing ROI: {roi}\")\n",
    "        \n",
    "        # Concatenate the trials and get labels\n",
    "        concatenated_data, labels, cats = concatenate_conditions_by_string(reshaped_roi_labeled_arrays_for_decoding, roi, strings_to_find)\n",
    "        print(f\"Concatenated data shape for {roi}: {concatenated_data.shape}\")\n",
    "        \n",
    "        # Iterate over the groups in strings_to_find\n",
    "        for string_group in strings_to_find:\n",
    "            # If it's a single string (old behavior)\n",
    "            if isinstance(string_group, str):\n",
    "                # Count trials for the single condition group\n",
    "                num_trials = len([label for label in labels if label == cats[(string_group,)]])\n",
    "                print(f\"{string_group} has {num_trials} trials\")\n",
    "            \n",
    "            # If it's a list of strings (new behavior)\n",
    "            elif isinstance(string_group, list):\n",
    "                # Count trials for the group of strings (e.g., ['i25.0', 'i75.0'])\n",
    "                num_trials = len([label for label in labels if label == cats[tuple(string_group)]])\n",
    "                print(f\"{string_group} has {num_trials} trials\")\n",
    "\n",
    "        # Create a Decoder and run cross-validation\n",
    "        decoder = Decoder(cats, 0.80, oversample=True, n_splits=n_splits, n_repeats=n_repeats)\n",
    "\n",
    "        # Use the concatenated data for the decoder\n",
    "        cm = decoder.cv_cm(concatenated_data, labels, normalize='true')\n",
    "        cm_avg = np.mean(cm, axis=0)\n",
    "\n",
    "        # Store the confusion matrix in the dictionary\n",
    "        confusion_matrices[roi] = cm_avg\n",
    "\n",
    "        # Convert tuple labels to simple strings for display\n",
    "        display_labels = [\n",
    "            key[0] if isinstance(key, tuple) and len(key) == 1 else str(key)\n",
    "            for key in cats.keys()\n",
    "        ]\n",
    "        \n",
    "        # Plot the Confusion Matrix\n",
    "        fig, ax = plt.subplots()\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm_avg, display_labels=display_labels)\n",
    "        disp.plot(ax=ax)\n",
    "\n",
    "        # Save the figure with the time interval in the filename\n",
    "        time_str = f\"_{time_interval_name}\" if time_interval_name else \"\"\n",
    "        file_name = f'{roi}_{condition_0_name}_vs_{condition_1_name}{time_str}_time_averaged_confusion_matrix_{n_splits}splits_{n_repeats}repeats.png'\n",
    "        plt.savefig(os.path.join(save_dir, file_name))\n",
    "        plt.close()\n",
    "\n",
    "    return confusion_matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the new function with the appropriate parameters\n",
    "confusion_matrices = get_and_plot_confusion_matrix_for_rois(\n",
    "    roi_labeled_arrays=roi_labeled_arrays,\n",
    "    rois=rois,\n",
    "    strings_to_find=strings_to_find,\n",
    "    condition_0_name=condition_0_name,\n",
    "    condition_1_name=condition_1_name,\n",
    "    save_dir=save_dir,\n",
    "    time_interval_name=None,  # If not dealing with specific time intervals\n",
    "    n_splits=5,\n",
    "    n_repeats=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replaced with a function 9/21\n",
    "# reshaped_roi_labeled_arrays_for_decoding = process_all_rois_for_decoding(roi_labeled_arrays)\n",
    "\n",
    "# for roi in rois:\n",
    "#     print(roi)\n",
    "    \n",
    "#     # Concatenate the trials and get labels\n",
    "#     concatenated_data, labels, cats = concatenate_conditions_by_string(reshaped_roi_labeled_arrays_for_decoding, roi, strings_to_find)\n",
    "#     print(concatenated_data.shape)\n",
    "    \n",
    "#     # Iterate over the groups in strings_to_find\n",
    "#     for string_group in strings_to_find:\n",
    "#         # If it's a single string (old behavior)\n",
    "#         if isinstance(string_group, str):\n",
    "#             # Count trials for the single condition group\n",
    "#             num_trials = len([label for label in labels if label == cats[(string_group,)]])\n",
    "#             print(f\"{string_group} has {num_trials} trials\")\n",
    "        \n",
    "#         # If it's a list of strings (new behavior)\n",
    "#         elif isinstance(string_group, list):\n",
    "#             # Count trials for the group of strings (e.g., ['i25.0', 'i75.0'])\n",
    "#             num_trials = len([label for label in labels if label == cats[tuple(string_group)]])\n",
    "#             print(f\"{string_group} has {num_trials} trials\")\n",
    "\n",
    "#     # Create a Decoder and run cross-validation\n",
    "#     decoder = Decoder(cats, 0.80, oversample=True, n_splits=5, n_repeats=5)\n",
    "\n",
    "#     # Use the concatenated data for the decoder\n",
    "#     cm = decoder.cv_cm(concatenated_data, labels, normalize='true')\n",
    "#     cm = np.mean(cm, axis=0)\n",
    "\n",
    "#     # Plot the Confusion Matrix\n",
    "#     fig, ax = plt.subplots()\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=cats.keys())\n",
    "#     disp.plot(ax=ax)\n",
    "\n",
    "#     # Save the figure with the strings_to_find reflected in the filename\n",
    "#     file_name = f'{roi}_{condition_0_name}_vs_{condition_1_name}_time_averaged_confusion_matrix.png'\n",
    "#     plt.savefig(os.path.join(save_dir, file_name))\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prestimulus vs poststimulus confusion matrices 9/21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_in_time_range(labeled_array, time_range):\n",
    "    \"\"\"\n",
    "    Extract data from a LabeledArray where the time points fall within a given range, using the LabeledArray `take` function.\n",
    "    \n",
    "    Parameters:\n",
    "    - labeled_array: The LabeledArray containing time points as labels.\n",
    "    - time_range: A tuple (start_time, end_time) representing the range of time.\n",
    "    \n",
    "    Returns:\n",
    "    - filtered_data: A LabeledArray containing only the data where the timepoints are within the specified range.\n",
    "    \"\"\"\n",
    "    start_time, end_time = time_range\n",
    "\n",
    "    # Assume that the time labels are stored in the 4th dimension (index 3)\n",
    "    time_points = np.array(labeled_array.labels[3], dtype=float)  # convert to floats, ensure conversion to float numpy array\n",
    "\n",
    "    # Find the indices of time points within the specified range\n",
    "    time_indices = np.where((time_points >= start_time) & (time_points <= end_time))[0]\n",
    "\n",
    "    # Use the take function to select the time indices along the time axis (axis=3)\n",
    "    filtered_data = labeled_array.take(time_indices, axis=3)\n",
    "\n",
    "    return filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time intervals of interest\n",
    "pre_stimulus_interval = (-1.0, 0.0)  # Pre-stimulus: time -1 to 0 seconds\n",
    "post_stimulus_interval = (0.0, 1.5)  # Post-stimulus: time 0 to 1.5 seconds\n",
    "pre_stimulus_roi_labeled_arrays = {}\n",
    "post_stimulus_roi_labeled_arrays = {}\n",
    "\n",
    "for roi, labeled_array in roi_labeled_arrays.items():\n",
    "    pre_stimulus_roi_labeled_arrays[roi] = get_data_in_time_range(labeled_array, pre_stimulus_interval)\n",
    "    post_stimulus_roi_labeled_arrays[roi] = get_data_in_time_range(labeled_array, post_stimulus_interval)\n",
    "\n",
    "\n",
    "pre_stimulus_confusion_matrices = get_and_plot_confusion_matrix_for_rois(\n",
    "    roi_labeled_arrays=pre_stimulus_roi_labeled_arrays,\n",
    "    rois=rois,\n",
    "    strings_to_find=strings_to_find,\n",
    "    condition_0_name=condition_0_name,\n",
    "    condition_1_name=condition_1_name,\n",
    "    save_dir=save_dir,\n",
    "    time_interval_name='pre_stimulus',  # If not dealing with specific time intervals\n",
    "    n_splits=5,\n",
    "    n_repeats=10\n",
    ")\n",
    "\n",
    "post_stimulus_confusion_matrices = get_and_plot_confusion_matrix_for_rois(\n",
    "    roi_labeled_arrays=post_stimulus_roi_labeled_arrays,\n",
    "    rois=rois,\n",
    "    strings_to_find=strings_to_find,\n",
    "    condition_0_name=condition_0_name,\n",
    "    condition_1_name=condition_1_name,\n",
    "    save_dir=save_dir,\n",
    "    time_interval_name='post_stimulus',  # If not dealing with specific time intervals\n",
    "    n_splits=5,\n",
    "    n_repeats=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define colors for plotting (not used yet as of 8/21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for the model names\n",
    "colors = {\n",
    "    'congruency': 'red',\n",
    "    'switchType': 'blue',\n",
    "    'congruencyProportion': 'pink',\n",
    "    'switchProportion': 'skyblue',\n",
    "    'congruency_congruencyProportion': 'hotpink',\n",
    "    'congruency_congruency_proportion': 'hotpink',\n",
    "    'switchType_switchProportion': 'gray',\n",
    "    'switch_type_switch_proportion': 'gray',\n",
    "    'bigLetter': 'green',\n",
    "    'big_letter': 'green',\n",
    "    'smallLetter': 'orange',\n",
    "    'small_letter': 'orange',\n",
    "    'task': 'gray',\n",
    "    'c75.0': 'pink',\n",
    "    'i75.0': 'pink',\n",
    "    'c25.0': 'gold',\n",
    "    'i25.0': 'gold',\n",
    "    'r25.0': 'lightblue',\n",
    "    's25.0': 'lightblue',\n",
    "    'r75.0': 'purple',\n",
    "    's75.0': 'purple'\n",
    "}\n",
    "\n",
    "# Define linestyles for the model names\n",
    "linestyles = {\n",
    "    'big letter S': '-',\n",
    "    'BigLetters': '-',\n",
    "\n",
    "    'big letter H': '--',\n",
    "    'BigLetterh': '--',\n",
    "\n",
    "    'small letter S': '-',\n",
    "    'SmallLetters': '-',\n",
    "\n",
    "    'small letter H': '--',\n",
    "    'SmallLetterh': '--',\n",
    "\n",
    "    'task G': '-',\n",
    "    'Taskg': '-',\n",
    "\n",
    "    'task L': '--',\n",
    "    'Taskl': '--',\n",
    "\n",
    "    'congruent': '-',\n",
    "    'c': '-',\n",
    "\n",
    "    'incongruent': '--',\n",
    "    'i': '--',\n",
    "\n",
    "    'repeat': '-',\n",
    "    'r': '-',\n",
    "\n",
    "    'switch': '--',\n",
    "    's': '--',\n",
    "\n",
    "    'c25.0': '-',\n",
    "    'c75.0': '-',\n",
    "    'i25.0': '--',\n",
    "    'i75.0': '--'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
