{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print(sys.path)\n",
    "sys.path.append(\"C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/\") #need to do this cuz otherwise ieeg isn't added to path...\n",
    "\n",
    "# Get the absolute path to the directory containing the current script\n",
    "# For GlobalLocal/src/analysis/preproc/make_epoched_data.py, this is GlobalLocal/src/analysis/preproc\n",
    "try:\n",
    "    # This will work if running as a .py script\n",
    "    current_file_path = os.path.abspath(__file__)\n",
    "    current_script_dir = os.path.dirname(current_file_path)\n",
    "except NameError:\n",
    "    # This will be executed if __file__ is not defined (e.g., in a Jupyter Notebook)\n",
    "    # os.getcwd() often gives the directory of the notebook,\n",
    "    # or the directory from which the Jupyter server was started.\n",
    "    current_script_dir = os.getcwd()\n",
    "\n",
    "# Navigate up three levels to get to the 'GlobalLocal' directory\n",
    "project_root = os.path.abspath(os.path.join(current_script_dir, '..', '..', '..'))\n",
    "\n",
    "# Add the 'GlobalLocal' directory to sys.path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root) # insert at the beginning to prioritize it\n",
    "\n",
    "from ieeg.navigate import channel_outlier_marker, trial_ieeg, crop_empty_data, \\\n",
    "    outliers_to_nan\n",
    "from ieeg.io import raw_from_layout, get_data\n",
    "from ieeg.timefreq.utils import crop_pad\n",
    "from ieeg.timefreq import gamma\n",
    "from ieeg.calc.scaling import rescale\n",
    "import mne\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ieeg.calc.reshape import make_data_same\n",
    "from ieeg.calc.stats import time_perm_cluster\n",
    "from ieeg.calc.mat import LabeledArray, combine\n",
    "\n",
    "# TODO: hmm fix these utils imports, import the funcs individually. 6/1/25.\n",
    "from src.analysis.utils.general_utils import *\n",
    "from src.analysis.utils.general_utils import make_or_load_subjects_electrodes_to_ROIs_dict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas import read_csv\n",
    "import scipy.stats as stats\n",
    "import joblib\n",
    "\n",
    "from scipy.ndimage import label\n",
    "from scipy.stats import norm\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# rsa toolbox imports\n",
    "from rsatoolbox.io.mne import read_epochs\n",
    "from rsatoolbox.data.ops import merge_datasets\n",
    "from rsatoolbox.rdm import calc_rdm_movie\n",
    "from rsatoolbox.rdm.calc import _parse_input\n",
    "from rsatoolbox.util.build_rdm import _build_rdms\n",
    "from rsatoolbox.rdm import compare\n",
    "from rsatoolbox.vis import show_rdm\n",
    "from rsatoolbox.vis.timecourse import plot_timecourse\n",
    "\n",
    "from os.path import join, expanduser, basename\n",
    "import glob, json\n",
    "import numpy, tqdm, mne, pandas\n",
    "import rsatoolbox\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from ieeg.decoding.decoders import PcaLdaClassification\n",
    "from ieeg.calc.oversample import MinimumNaNSplit\n",
    "from ieeg.calc.fast import mixup\n",
    "\n",
    "from src.analysis.config import experiment_conditions\n",
    "\n",
    "from src.analysis.utils.labeled_array_utils import (\n",
    "    put_data_in_labeled_array_per_roi_subject,\n",
    "    remove_nans_from_labeled_array,\n",
    "    remove_nans_from_all_roi_labeled_arrays,\n",
    "    concatenate_conditions_by_string,\n",
    "    get_data_in_time_range\n",
    ")\n",
    "\n",
    "from src.analysis.decoding.decoding import (\n",
    "    concatenate_and_balance_data_for_decoding, \n",
    "    get_and_plot_confusion_matrix_for_rois_jim,\n",
    "    Decoder, \n",
    "    windower,\n",
    "    get_confusion_matrices_for_rois_time_window_decoding_jim,\n",
    "    compute_accuracies,\n",
    "    perform_time_perm_cluster_test_for_accuracies,\n",
    "    plot_accuracies\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['D0057','D0059', 'D0063', 'D0065', 'D0069', 'D0071', 'D0077', 'D0090', 'D0094', 'D0100', 'D0102', 'D0103', 'D0107A', 'D0110', 'D0116', 'D0117', 'D0121']\n",
    "# subjects = ['D0057','D0059', 'D0063', 'D0065', 'D0069', 'D0071', 'D0077', 'D0090', 'D0094', 'D0100', 'D0102', 'D0103']\n",
    "# load in subjects electrodes to rois dict. If it doesn't already exist, make it and then load it.\n",
    "config_dir = r'C:\\Users\\jz421\\Desktop\\GlobalLocal\\src\\analysis\\config'\n",
    "subjects_electrodestoROIs_dict = make_or_load_subjects_electrodes_to_ROIs_dict(subjects, task='GlobalLocal', LAB_root=None, save_dir=config_dir, \n",
    "                                                filename='subjects_electrodestoROIs_dict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='GlobalLocal'\n",
    "conditions = experiment_conditions.stimulus_conditions # set this to whichever conditions you're running\n",
    "stimulus_locked = True  #toggle\n",
    "response_locked = not stimulus_locked\n",
    "\n",
    "if stimulus_locked:\n",
    "    # epochs_root_file = \"Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_4.0-8.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False\"\n",
    "    epochs_root_file = \"Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False\"\n",
    "    # epochs_root_file = \"Stimulus_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_0.0-30.0_padLength_0.5s_stat_func_ttest_ind_equal_var_False\"\n",
    "\n",
    "elif response_locked:\n",
    "    # epochs_root_file = \"Response_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_4.0-8.0_padLength_0.5s_stat_func_ttest_ind\"\n",
    "    epochs_root_file = \"Response_0.5sec_within1sec_randoffset_preStimulusBase_decFactor_8_outliers_10_passband_70.0-150.0_padLength_0.5s_stat_func_ttest_ind\"\n",
    "\n",
    "condition_names = list(conditions.keys()) # get the condition names as a list\n",
    "\n",
    "# This breaks if just_HG_ev1_rescaled is set to False currently. Fix this! 8/11\n",
    "subjects_mne_objects = create_subjects_mne_objects_dict(subjects=subjects, epochs_root_file=epochs_root_file, conditions=conditions, task=\"GlobalLocal\", just_HG_ev1_rescaled=True, acc_trials_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_names_bids = [condition['BIDS_events'] for condition in conditions.values()] # get the condition names in bids format\n",
    "condition_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get significant channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_chans_per_subject = get_sig_chans_per_subject(subjects, epochs_root_file, task='GlobalLocal', LAB_root=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your ROIs dictionary and other parameters\n",
    "rois_dict = {\n",
    "    'lpfc': [\"G_front_inf-Opercular\", \"G_front_inf-Orbital\", \"G_front_inf-Triangul\", \"G_front_middle\", \"G_front_sup\", \"Lat_Fis-ant-Horizont\", \"Lat_Fis-ant-Vertical\", \"S_circular_insula_ant\", \"S_circular_insula_sup\", \"S_front_inf\", \"S_front_middle\", \"S_front_sup\"],\n",
    "    'v1': [\"G_oc-temp_med-Lingual\", \"S_calcarine\", \"G_cuneus\"],\n",
    "    'occ': [\"G_cuneus\", \"G_and_S_occipital_inf\", \"G_occipital_middle\", \"G_occipital_sup\", \"G_oc-temp_lat-fusifor\", \"G_oc-temp_med-Lingual\", \"Pole_occipital\", \"S_calcarine\", \"S_oc_middle_and_Lunatus\", \"S_oc_sup_and_transversal\", \"S_occipital_ant\"],\n",
    "    'occ_filtered': [],\n",
    "    'occ_best_filtered': []\n",
    "}\n",
    "\n",
    "rois = list(rois_dict.keys())\n",
    "# Assuming you have subjects_electrodestoROIs_dict and sig_chans_per_subject dictionaries\n",
    "electrodes_per_subject_roi, sig_electrodes_per_subject_roi, = make_sig_electrodes_per_subject_and_roi_dict(\n",
    "    rois_dict, subjects_electrodestoROIs_dict, sig_chans_per_subject\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's manually make the occ filtered sig electrodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_electrodes_per_subject_roi['occ_filtered'] = {\n",
    "    'D0057': [],\n",
    "    'D0059': [],\n",
    "    'D0071': ['RO1', 'RO10'], #RO10 is iffy, big drop from fix onset\n",
    "    'D0077': ['ROPM1', 'ROPM8'],\n",
    "    'D0090': ['RTPO1', 'RTPI1'],\n",
    "    'D0100': ['LOAI12', 'LOAI13', 'LOAI14', 'LOAI15', 'LOMI9', 'LOMI11', 'LOPI8', 'LOPI9', 'LOPM8', 'LOPM9', 'LPPI7', 'LPPI8', 'LPPI9'],\n",
    "    'D0102': ['RTPI1'],\n",
    "    'D0103': ['LTPI2', 'LTPI3', 'LTPI4']\n",
    "}\n",
    "\n",
    "electrodes_per_subject_roi['occ_filtered'] = {\n",
    "    'D0057': [],\n",
    "    'D0059': [],\n",
    "    'D0071': ['RO1', 'RO10'], #RO10 is iffy, big drop from fix onset\n",
    "    'D0077': ['ROPM1', 'ROPM8'],\n",
    "    'D0090': ['RTPO1', 'RTPI1'],\n",
    "    'D0100': ['LOAI12', 'LOAI13', 'LOAI14', 'LOAI15', 'LOMI9', 'LOMI11', 'LOPI8', 'LOPI9', 'LOPM8', 'LOPM9', 'LPPI7', 'LPPI8', 'LPPI9'],\n",
    "    'D0102': ['RTPI1'],\n",
    "    'D0103': ['LTPI2', 'LTPI3', 'LTPI4']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now make the best of the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_electrodes_per_subject_roi['occ_best_filtered'] = {\n",
    "    'D0057': [],\n",
    "    'D0059': [],\n",
    "    'D0071': [],\n",
    "    'D0077': [],\n",
    "    'D0090': [],\n",
    "    'D0100': ['LOAI12', 'LOMI9', 'LOPI8', 'LOPI9', 'LOPM8', 'LOPM9'],\n",
    "    'D0102': ['RTPI1'],\n",
    "    'D0103': ['LTPI2', 'LTPI3', 'LTPI4']\n",
    "}\n",
    "\n",
    "electrodes_per_subject_roi['occ_best_filtered'] = {\n",
    "    'D0057': [],\n",
    "    'D0059': [],\n",
    "    'D0071': [],\n",
    "    'D0077': [],\n",
    "    'D0090': [],\n",
    "    'D0100': ['LOAI12', 'LOMI9', 'LOPI8', 'LOPI9', 'LOPM8', 'LOPM9'],\n",
    "    'D0102': ['RTPI1'],\n",
    "    'D0103': ['LTPI2', 'LTPI3', 'LTPI4']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path where you want to save the dictionary\n",
    "save_path = 'sig_electrodes_per_subject_roi.json'\n",
    "\n",
    "# Use json to save the dictionary\n",
    "with open(save_path, 'w') as file:\n",
    "    json.dump(sig_electrodes_per_subject_roi, file, indent=4)\n",
    "\n",
    "print(f\"Dictionary saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get electrode counts for each roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_electrodes_info = calculate_total_electrodes(sig_electrodes_per_subject_roi, electrodes_per_subject_roi)\n",
    "for roi, counts in total_electrodes_info.items():\n",
    "    print(f\"Total number of {roi} electrodes across all subjects:\", counts['total_electrodes'])\n",
    "    print(f\"Total number of significant {roi} electrodes across all subjects:\", counts['total_significant_electrodes'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if any subjects have a weird sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'subjects_mne_objects' is your dictionary containing MNE objects for each subject\n",
    "sampling_rate = 256\n",
    "subject_rates = check_sampling_rates(subjects_mne_objects, expected_sampling_rate=sampling_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make roi_labeled_arrays, a dict where the keys are rois and the values are LabeledArrays. Index the same way as a nested dict. Use .labels to get labels from current level.\n",
    "\n",
    "\n",
    "#### roi_labeled_arrays structure\n",
    "- **roi1**: ROI name, string\n",
    "  - **conditions**: condition name\n",
    "    - **trials**: This is the maximal number of trials across subjects for any condition, filled with nans for subjects who don't have this many trials\n",
    "      - **channels**: This is the number of channels in the roi, each channel is labeled as subject-channel name. Concatenated across subjects.\n",
    "        - **samples**: 1 sample as a float. This is the time for this sample.\n",
    "\n",
    "- **roi2**: ROI name, string\n",
    "  - **conditions**: condition name\n",
    "    - **trials**: This is the maximal number of trials across subjects for any condition, filled with nans for subjects who don't have this many trials\n",
    "      - **channels**: This is the number of channels in the roi, each channel is labeled as subject-channel name. Concatenated across subjects.\n",
    "        - **samples**: 1 sample as a float. This is the time for this sample.\n",
    "\n",
    "- **roiX**: ROI name, string\n",
    "  - **conditions**: condition name\n",
    "    - **trials**: This is the maximal number of trials across subjects for any condition, filled with nans for subjects who don't have this many trials\n",
    "      - **channels**: This is the number of channels in the roi, each channel is labeled as subject-channel name. Concatenated across subjects.\n",
    "        - **samples**: 1 sample as a float. This is the time for this sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "\n",
    "# choose whether to use sig elecs or all elecs\n",
    "electrodes = electrodes_per_subject_roi # toggle this to sig_electrodes_per_subject_roi if just using sig elecs, or electrodes_per_subject_roi if using all elecs\n",
    "\n",
    "if electrodes == electrodes_per_subject_roi:\n",
    "    elec_string_to_add_to_filename = 'all_elecs'\n",
    "elif electrodes == sig_electrodes_per_subject_roi:\n",
    "    elec_string_to_add_to_filename = 'sig_elecs'\n",
    "else:\n",
    "    elec_string_to_add_to_filename = None\n",
    "\n",
    "roi_labeled_arrays = put_data_in_labeled_array_per_roi_subject(\n",
    "    subjects_mne_objects,\n",
    "    condition_names,\n",
    "    rois,\n",
    "    subjects,\n",
    "    electrodes, \n",
    "    obs_axs=0,  # Trials dimension (ignoring the conditions dimension for now cuz we iterate over it)\n",
    "    chans_axs=1,  # Channels dimension\n",
    "    time_axs=2,   # Time dimension\n",
    "    random_state=42  # For reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove trials with nans from roi labeled arrays if just getting the minimum number of trials for all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_labeled_arrays_no_nans, conditions_with_no_valid_trials_per_roi = remove_nans_from_all_roi_labeled_arrays(roi_labeled_arrays, obs_axs=0, chans_axs=1, time_axs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up decoding output directory and conditions to compare 9/12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine LAB_root based on the operating system\n",
    "HOME = os.path.expanduser(\"~\")\n",
    "LAB_root = os.path.join(HOME, \"Box\", \"CoganLab\") if os.name == 'nt' else os.path.join(HOME, \"Library\", \"CloudStorage\", \"Box-Box\", \"CoganLab\")\n",
    "save_dir = os.path.join(LAB_root, 'BIDS-1.1_GlobalLocal', 'BIDS', 'derivatives', 'decoding', 'figs', f\"{epochs_root_file}\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "print(f\"Save directory created or already exists at: {save_dir}\")\n",
    "\n",
    "if conditions == experiment_conditions.stimulus_experiment_conditions:\n",
    "    condition_comparisons = {}\n",
    "    condition_comparisons['congruency'] = [['c25', 'c75'], ['i25', 'i75']]\n",
    "    condition_comparisons['switchType'] = [['r25', 'r75'], ['s25', 's75']]\n",
    "    \n",
    "elif conditions == experiment_conditions.stimulus_conditions:\n",
    "    condition_comparisons = {}\n",
    "    condition_comparisons['BigLetter'] = ['bigS', 'bigH']\n",
    "    condition_comparisons['SmallLetter'] = ['smallS', 'smallH']\n",
    "    condition_comparisons['Task'] = ['taskG', 'taskL']\n",
    "\n",
    "    # condition_comparisons['BigLetter'] = ['BigLetters', 'BigLetterh']\n",
    "    # condition_comparisons['SmallLetter'] = ['SmallLetters', 'SmallLetterh']\n",
    "    # condition_comparisons['Task'] = ['Taskg', 'Taskl']\n",
    "    # condition_comparisons['TaskBigLetterInteraction'] = [['BigLetters/SmallLetters/Taskg', 'BigLetters/SmallLetterh/Taskg'], ['BigLetters/SmallLetters/Taskl', 'BigLetters/SmallLetterh/Taskl'], ['BigLetterh/SmallLetters/Taskg', 'BigLetterh/SmallLetterh/Taskg'], ['BigLetterh/SmallLetters/Taskl', 'BigLetterh/SmallLetterh/Taskl']]\n",
    "    # condition_comparisons['TaskSmallLetterInteraction'] = [['BigLetters/SmallLetters/Taskg', 'BigLetterh/SmallLetters/Taskg'], ['BigLetters/SmallLetters/Taskl', 'BigLetterh/SmallLetters/Taskl'], ['BigLetters/SmallLetterh/Taskg', 'BigLetterh/SmallLetterh/Taskg'], ['BigLetters/SmallLetterh/Taskl', 'BigLetters/SmallLetterh/Taskl']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the confusion matrix using the downsampled version\n",
    "\n",
    "# add elec and subject info to filename 6/11/25\n",
    "other_string_to_add = elec_string_to_add_to_filename + '_' + str(len(subjects)) + '_subjects'\n",
    "\n",
    "for condition_comparison, strings_to_find in condition_comparisons.items():\n",
    "    confusion_matrices = get_and_plot_confusion_matrix_for_rois_jim(\n",
    "        roi_labeled_arrays=roi_labeled_arrays_no_nans,\n",
    "        rois=rois,\n",
    "        condition_comparison=condition_comparison,\n",
    "        strings_to_find=strings_to_find,\n",
    "        save_dir=save_dir,\n",
    "        time_interval_name=None,\n",
    "        other_string_to_add=elec_string_to_add_to_filename,\n",
    "        n_splits=5,\n",
    "        n_repeats=10,\n",
    "        obs_axs=0,\n",
    "        balance_method='subsample',  # Use 'subsample' to balance by subsampling\n",
    "        random_state=42  # For reproducibility\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the confusion matrix using the version with nan trials and doing nan trial mixup\n",
    "for condition_comparison, strings_to_find in condition_comparisons.items():\n",
    "    confusion_matrices = get_and_plot_confusion_matrix_for_rois_jim(\n",
    "        roi_labeled_arrays=roi_labeled_arrays,\n",
    "        rois=rois,\n",
    "        condition_comparison=condition_comparison,\n",
    "        strings_to_find=strings_to_find,\n",
    "        save_dir=save_dir,\n",
    "        time_interval_name=None,\n",
    "        n_splits=5,\n",
    "        n_repeats=10,\n",
    "        obs_axs=0,\n",
    "        balance_method='pad_with_nans',  # Use 'pad_with_nans' to balance by padding with nans\n",
    "        random_state=42  # For reproducibility\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prestimulus vs poststimulus confusion matrices 9/21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the time intervals of interest\n",
    "# pre_stimulus_interval = (-1.0, 0.0)  # Pre-stimulus: time -1 to 0 seconds\n",
    "# post_stimulus_interval = (0.0, 1.5)  # Post-stimulus: time 0 to 1.5 seconds\n",
    "# pre_stimulus_roi_labeled_arrays = {}\n",
    "# post_stimulus_roi_labeled_arrays = {}\n",
    "\n",
    "# for roi, labeled_array in roi_labeled_arrays.items():\n",
    "#     pre_stimulus_roi_labeled_arrays[roi] = get_data_in_time_range(labeled_array, pre_stimulus_interval)\n",
    "#     post_stimulus_roi_labeled_arrays[roi] = get_data_in_time_range(labeled_array, post_stimulus_interval)\n",
    "\n",
    "\n",
    "# pre_stimulus_confusion_matrices = get_and_plot_confusion_matrix_for_rois_jim(\n",
    "#     roi_labeled_arrays=pre_stimulus_roi_labeled_arrays,\n",
    "#     rois=rois,\n",
    "#     condition_comparison=condition_comparison,\n",
    "#     strings_to_find=strings_to_find,\n",
    "#     save_dir=save_dir,\n",
    "#     time_interval_name='pre_stimulus',  # If not dealing with specific time intervals\n",
    "#     n_splits=5,\n",
    "#     n_repeats=10,\n",
    "#     obs_axs=0,\n",
    "#     balance_method='subsample',  # Use 'subsample' to balance by subsampling\n",
    "#     random_state=42  # For reproducibility\n",
    "# )\n",
    "\n",
    "# post_stimulus_confusion_matrices = get_and_plot_confusion_matrix_for_rois_jim(\n",
    "#     roi_labeled_arrays=post_stimulus_roi_labeled_arrays,\n",
    "#     rois=rois,\n",
    "#     condition_comparison=condition_comparison,\n",
    "#     strings_to_find=strings_to_find,\n",
    "#     save_dir=save_dir,\n",
    "#     time_interval_name='post_stimulus',  # If not dealing with specific time intervals\n",
    "#     n_splits=5,\n",
    "#     n_repeats=10,\n",
    "#     obs_axs=0,\n",
    "#     balance_method='subsample',  # Use 'subsample' to balance by subsampling\n",
    "#     random_state=42  # For reproducibility\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define colors for plotting (not used yet as of 8/21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for the model names\n",
    "colors = {\n",
    "    'congruency': 'red',\n",
    "    'switchType': 'blue',\n",
    "    'congruencyProportion': 'pink',\n",
    "    'switchProportion': 'skyblue',\n",
    "    'congruency_congruencyProportion': 'hotpink',\n",
    "    'congruency_congruency_proportion': 'hotpink',\n",
    "    'switchType_switchProportion': 'gray',\n",
    "    'switch_type_switch_proportion': 'gray',\n",
    "    'bigLetter': 'green',\n",
    "    'big_letter': 'green',\n",
    "    'smallLetter': 'orange',\n",
    "    'small_letter': 'orange',\n",
    "    'task': 'gray',\n",
    "    'c75.0': 'pink',\n",
    "    'i75.0': 'pink',\n",
    "    'c25.0': 'gold',\n",
    "    'i25.0': 'gold',\n",
    "    'r25.0': 'lightblue',\n",
    "    's25.0': 'lightblue',\n",
    "    'r75.0': 'purple',\n",
    "    's75.0': 'purple'\n",
    "}\n",
    "\n",
    "# Define linestyles for the model names\n",
    "linestyles = {\n",
    "    'big letter S': '-',\n",
    "    'BigLetters': '-',\n",
    "\n",
    "    'big letter H': '--',\n",
    "    'BigLetterh': '--',\n",
    "\n",
    "    'small letter S': '-',\n",
    "    'SmallLetters': '-',\n",
    "\n",
    "    'small letter H': '--',\n",
    "    'SmallLetterh': '--',\n",
    "\n",
    "    'task G': '-',\n",
    "    'Taskg': '-',\n",
    "\n",
    "    'task L': '--',\n",
    "    'Taskl': '--',\n",
    "\n",
    "    'congruent': '-',\n",
    "    'c': '-',\n",
    "\n",
    "    'incongruent': '--',\n",
    "    'i': '--',\n",
    "\n",
    "    'repeat': '-',\n",
    "    'r': '-',\n",
    "\n",
    "    'switch': '--',\n",
    "    's': '--',\n",
    "\n",
    "    'c25.0': '-',\n",
    "    'c75.0': '-',\n",
    "    'i25.0': '--',\n",
    "    'i75.0': '--'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new attempt at time window sliding decoding with significance 11/23  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "actually do the time window decoding, run stats, and plot accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main code\n",
    "\n",
    "# Directory to save confusion matrices\n",
    "cm_save_dir = os.path.join(save_dir, \"confusion_matrices\")\n",
    "os.makedirs(cm_save_dir, exist_ok=True)\n",
    "\n",
    "condition_comparison_confusion_matrices = {}\n",
    "n_splits=5\n",
    "n_repeats=10\n",
    "balance_method='subsample'\n",
    "random_state=42\n",
    "window_size=64\n",
    "step_size=32\n",
    "n_permutations=100\n",
    "\n",
    "for condition_comparison, strings_to_find in condition_comparisons.items():\n",
    "    # Get confusion matrices for each ROI\n",
    "    cm_true_per_roi, cm_shuffle_per_roi = get_confusion_matrices_for_rois_time_window_decoding_jim(\n",
    "        roi_labeled_arrays=roi_labeled_arrays,\n",
    "        rois=rois,\n",
    "        condition_comparison=condition_comparison,\n",
    "        strings_to_find=strings_to_find,\n",
    "        n_splits=n_splits,\n",
    "        n_repeats=n_repeats,\n",
    "        obs_axs=0,\n",
    "        time_axs=-1,\n",
    "        balance_method=balance_method,\n",
    "        random_state=random_state,\n",
    "        window_size=window_size,\n",
    "        step_size=step_size,\n",
    "        n_permutations=n_permutations,\n",
    "        sampling_rate=sampling_rate,\n",
    "        first_time_point=-1\n",
    "    )\n",
    "\n",
    "    np.save(os.path.join(cm_save_dir, f'{condition_comparison}_{n_splits}_splits_{n_repeats}_repeats_{balance_method}_balance_method_{random_state}_random_state_{window_size}_window_size_{step_size}_step_size_{n_permutations}_permutations_{sampling_rate}_sampling_rate_cm_true_per_roi.npy'), cm_true_per_roi)\n",
    "    np.save(os.path.join(cm_save_dir, f'{condition_comparison}_{n_splits}_splits_{n_repeats}_repeats_{balance_method}_balance_method_{random_state}_random_state_{window_size}_window_size_{step_size}_step_size_{n_permutations}_permutations_{sampling_rate}_sampling_rate_cm_shuffle_per_roi.npy'), cm_shuffle_per_roi)\n",
    "\n",
    "    # Store the results in a dictionary\n",
    "    condition_comparison_confusion_matrices[condition_comparison] = {\n",
    "        'strings_to_find': strings_to_find,\n",
    "        'cm_true_per_roi': cm_true_per_roi,\n",
    "        'cm_shuffle_per_roi': cm_shuffle_per_roi\n",
    "    }\n",
    "\n",
    "    # Now compute accuracies and perform time permutation cluster test\n",
    "    for roi in rois:\n",
    "        cm_true = cm_true_per_roi[roi]['cm_true']\n",
    "        cm_shuffle = cm_shuffle_per_roi[roi]['cm_shuffle']\n",
    "        time_window_centers = cm_true_per_roi[roi]['time_window_centers']\n",
    "        window_size = cm_true_per_roi[roi]['window_size']\n",
    "        step_size = cm_true_per_roi[roi]['step_size']\n",
    "\n",
    "        # Compute accuracies\n",
    "        accuracies_true, accuracies_shuffle = compute_accuracies(cm_true, cm_shuffle)\n",
    "\n",
    "        # Perform time permutation cluster test\n",
    "        significant_clusters, p_values = perform_time_perm_cluster_test_for_accuracies(\n",
    "            accuracies_true, accuracies_shuffle, p_thresh=0.05, n_perm=100, seed=42\n",
    "        )\n",
    "\n",
    "        # Store significant clusters and p-values\n",
    "        cm_true_per_roi[roi]['significant_clusters'] = significant_clusters\n",
    "        cm_true_per_roi[roi]['p_values'] = p_values\n",
    "\n",
    "        # Optionally, store accuracies\n",
    "        cm_true_per_roi[roi]['accuracies_true'] = accuracies_true\n",
    "        cm_shuffle_per_roi[roi]['accuracies_shuffle'] = accuracies_shuffle\n",
    "        print(significant_clusters)\n",
    "\n",
    "        # Plot accuracies\n",
    "        plot_accuracies(\n",
    "            time_points=time_window_centers,\n",
    "            accuracies_true=accuracies_true,\n",
    "            accuracies_shuffle=accuracies_shuffle,\n",
    "            significant_clusters=significant_clusters,\n",
    "            window_size=window_size,\n",
    "            step_size=step_size,\n",
    "            sampling_rate=sampling_rate,\n",
    "            condition_comparison=condition_comparison,\n",
    "            roi=roi,\n",
    "            save_dir=save_dir\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for condition_comparison in condition_comparison_confusion_matrices:\n",
    "    cm_true_per_roi = condition_comparison_confusion_matrices[condition_comparison]['cm_true_per_roi']\n",
    "    for roi in cm_true_per_roi:\n",
    "        significant_clusters = cm_true_per_roi[roi]['significant_clusters']\n",
    "        print(f'significant clusters for {condition_comparison} and {roi} are: {significant_clusters}')\n",
    "\n",
    "        p_values = cm_true_per_roi[roi]['p_values']\n",
    "        print(f'p values for {condition_comparison} and {roi} are: {p_values}')\n",
    "\n",
    "        accuracies_true = cm_true_per_roi[roi]['accuracies_true']\n",
    "        print(f'accuracies for {condition_comparison} and {roi} are: {accuracies_true}')\n",
    "\n",
    "        # Now you can use significant_clusters, p_values, and accuracies_true as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
