{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\jz421\\\\Desktop\\\\GlobalLocal', 'C:\\\\Users\\\\jz421\\\\Desktop\\\\GlobalLocal\\\\IEEG_Pipelines', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\python311.zip', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\DLLs', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg', '', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\win32', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\jz421\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\site-packages\\\\Pythonwin', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\jz421\\\\AppData\\\\Local\\\\anaconda3\\\\envs\\\\ieeg\\\\Lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jz421\\AppData\\Local\\anaconda3\\envs\\ieeg\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "sys.path.append(\"C:/Users/jz421/Desktop/GlobalLocal/IEEG_Pipelines/\") #need to do this cuz otherwise ieeg isn't added to path...\n",
    "\n",
    "from ieeg.navigate import channel_outlier_marker, trial_ieeg, crop_empty_data, \\\n",
    "    outliers_to_nan\n",
    "from ieeg.io import raw_from_layout, get_data\n",
    "from ieeg.timefreq.utils import crop_pad\n",
    "from ieeg.timefreq import gamma\n",
    "from ieeg.calc.scaling import rescale\n",
    "import mne\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ieeg.calc.reshape import make_data_same\n",
    "from ieeg.calc.stats import time_perm_cluster\n",
    "\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pandas import read_csv\n",
    "import scipy.stats as stats\n",
    "import joblib\n",
    "\n",
    "from scipy.ndimage import label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make model rdms here (just do congruency and switch type for testing for now 5/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conditions\n",
    "conditions = [\"i25s25\", \"i25s75\", \"i75s25\", \"i75s75\", \"i25r25\", \"i25r75\", \"i75r25\", \"i75r75\",\n",
    "              \"c25s25\", \"c25s75\", \"c75s25\", \"c75s75\", \"c25r25\", \"c25r75\", \"c75r25\", \"c75r75\"]\n",
    "\n",
    "# Extract specific features from each condition\n",
    "congruency = [cond[0] for cond in conditions]\n",
    "switch_types = [cond[-3] for cond in conditions]\n",
    "congruency_proportion = [int(cond[1:3]) for cond in conditions]\n",
    "switch_proportion = [int(cond[-2:]) for cond in conditions]\n",
    "\n",
    "# Number of conditions\n",
    "n = len(conditions)\n",
    "\n",
    "# initialize dict to store model RDMs\n",
    "model_rdms = {'congruency': np.ones((n, n)), 'switchType': np.ones((n, n)),\n",
    "              'congruencyProportion': np.ones((n, n)), 'switchProportion': np.ones((n, n))}\n",
    "\n",
    "# Populate RDMs based on feature comparisons\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if congruency[i] == congruency[j]:\n",
    "            model_rdms['congruency'][i, j] = 0\n",
    "        if switch_types[i] == switch_types[j]:\n",
    "            model_rdms['switchType'][i, j] = 0\n",
    "        if congruency_proportion[i] == congruency_proportion[j]:\n",
    "            model_rdms['congruencyProportion'][i, j] = 0\n",
    "        if switch_proportion[i] == switch_proportion[j]:\n",
    "            model_rdms['switchProportion'][i, j] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load in epochs data  \n",
    "(get only acc trials, avg across epochs to make evoked using only acc trials, and then delete epochs from memory) 5/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = ['D0057','D0059', 'D0063', 'D0065', 'D0069', 'D0071', 'D0077', 'D0090', 'D0094', 'D0100', 'D0102', 'D0103']\n",
    "# subjects = ['D0077']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make or load subjects electrodes to rois dict (mapping from electrode names to roi labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load the subjects' electrodes-to-ROIs dictionary...\n",
      "Loaded data from subjects_electrodestoROIs_dict.json\n",
      "Dictionary loaded successfully. Ready to proceed!\n"
     ]
    }
   ],
   "source": [
    "# load in subjects electrodes to rois dict. If it doesn't already exist, make it and then load it.\n",
    "filename = 'subjects_electrodestoROIs_dict.json'\n",
    "subjects_electrodestoROIs_dict = utils.make_or_load_subjects_electrodes_to_rois_dict(filename, subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load accuracy arrays so we can filter by only accurate trials  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this makes numpy arrays for each subject that are 0 or 1 for each trial based on accuracy\n",
    "from makeRawBehavioralData import main\n",
    "main()\n",
    "\n",
    "# Directory where your .npy files are saved\n",
    "npy_directory = r'C:\\Users\\jz421\\Box\\CoganLab\\D_Data\\GlobalLocal\\accArrays'  # Replace with your directory path if you're not Jim\n",
    "\n",
    "acc_array = utils.load_acc_arrays(npy_directory, skip_subjects=['D107'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load in behavioral data and convert block types to congruency and switch proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.read_csv(r'C:\\Users\\jz421\\Box\\CoganLab\\D_Data\\GlobalLocal\\combinedData.csv')\n",
    "\n",
    "# Apply the function to each row and create new columns\n",
    "combined_data[['congruencyProportion', 'switchProportion']] = combined_data.apply(utils.map_block_type, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load epochs and evoked for all 16 conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test these functions to just load in HG_ev1_rescaled cuz memory issues 5/6. Replace the ones in utils.py if these work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mne_objects(sub, output_name, task, just_HG_ev1_rescaled=False, LAB_root=None):\n",
    "    \"\"\"\n",
    "    Load MNE objects for a given subject and output name, with an option to load only rescaled high gamma epochs.\n",
    "\n",
    "    Parameters:\n",
    "    - sub (str): Subject identifier.\n",
    "    - output_name (str): Output name used in the file naming.\n",
    "    - task (str): Task identifier.\n",
    "    - just_HG_ev1_rescaled (bool): If True, only the rescaled high gamma epochs are loaded.\n",
    "    - LAB_root (str, optional): Root directory for the lab. If None, it will be determined based on the OS.\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing loaded MNE objects.\n",
    "    \"\"\"\n",
    "\n",
    "    # Determine LAB_root based on the operating system\n",
    "    if LAB_root is None:\n",
    "        HOME = os.path.expanduser(\"~\")\n",
    "        LAB_root = os.path.join(HOME, \"Box\", \"CoganLab\") if os.name == 'nt' else os.path.join(HOME, \"Library\", \"CloudStorage\", \"Box-Box\", \"CoganLab\")\n",
    "\n",
    "    # Get data layout\n",
    "    layout = get_data(task, root=LAB_root)\n",
    "    save_dir = os.path.join(layout.root, 'derivatives', 'freqFilt', 'figs', sub)\n",
    "\n",
    "    # Ensure save directory exists\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Initialize the return dictionary\n",
    "    mne_objects = {}\n",
    "\n",
    "    if just_HG_ev1_rescaled:\n",
    "        # Define path and load only the rescaled high gamma epochs\n",
    "        HG_ev1_rescaled_file = f'{save_dir}/{sub}_{output_name}_HG_ev1_rescaled-epo.fif'\n",
    "        HG_ev1_rescaled = mne.read_epochs(HG_ev1_rescaled_file)\n",
    "        mne_objects['HG_ev1_rescaled'] = HG_ev1_rescaled\n",
    "    else:\n",
    "        # Define file paths\n",
    "        HG_ev1_file = f'{save_dir}/{sub}_{output_name}_HG_ev1-epo.fif'\n",
    "        HG_base_file = f'{save_dir}/{sub}_{output_name}_HG_base-epo.fif'\n",
    "        HG_ev1_rescaled_file = f'{save_dir}/{sub}_{output_name}_HG_ev1_rescaled-epo.fif'\n",
    "        \n",
    "        # Load the objects\n",
    "        HG_ev1 = mne.read_epochs(HG_ev1_file)\n",
    "        HG_base = mne.read_epochs(HG_base_file)\n",
    "        HG_ev1_evoke = HG_ev1.average(method=lambda x: np.nanmean(x, axis=0))\n",
    "        HG_ev1_rescaled = mne.read_epochs(HG_ev1_rescaled_file)\n",
    "        HG_ev1_evoke_rescaled = HG_ev1_rescaled.average(method=lambda x: np.nanmean(x, axis=0))\n",
    "\n",
    "        mne_objects['HG_ev1'] = HG_ev1\n",
    "        mne_objects['HG_base'] = HG_base\n",
    "        mne_objects['HG_ev1_evoke'] = HG_ev1_evoke\n",
    "        mne_objects['HG_ev1_rescaled'] = HG_ev1_rescaled\n",
    "        mne_objects['HG_ev1_evoke_rescaled'] = HG_ev1_evoke_rescaled\n",
    "\n",
    "    return mne_objects\n",
    "\n",
    "\n",
    "def create_subjects_mne_objects_dict(subjects, output_names_conditions, task, combined_data, acc_array, just_HG_ev1_rescaled=False, LAB_root=None):\n",
    "    \"\"\"\n",
    "    Adjusted to handle multiple conditions per output name, with multiple condition columns.\n",
    "\n",
    "    Parameters:\n",
    "    - subjects: List of subject IDs.\n",
    "    - output_names_conditions: Dictionary where keys are output names and values are dictionaries\n",
    "        of condition column names and their required values.\n",
    "    - task: Task identifier.\n",
    "    - combined_data: DataFrame with combined behavioral and trial information.\n",
    "    - acc_array: dict of numpy arrays of 0 for incorrect and 1 for correct trials for each subject\n",
    "    - LAB_root: Root directory for data (optional).\n",
    "    \"\"\"\n",
    "    subjects_mne_objects = {}\n",
    "\n",
    "    for sub in subjects:\n",
    "        print(f\"Loading data for subject: {sub}\")\n",
    "        sub_mne_objects = {}\n",
    "        for output_name, conditions in output_names_conditions.items():\n",
    "            print(f\"  Loading output: {output_name} with conditions: {conditions}\")\n",
    "            \n",
    "            # Build the filtering condition\n",
    "            sub_without_zeroes = \"D\" + sub[1:].lstrip('0') \n",
    "            condition_filter = (combined_data['subject_ID'] == sub) # this previously indexed using sub_without_zeroes, but now just uses sub. 3/17.\n",
    "                    \n",
    "            for condition_column, condition_value in conditions.items():\n",
    "                if isinstance(condition_value, list):\n",
    "                    # If the condition needs to match any value in a list\n",
    "                    condition_filter &= combined_data[condition_column].isin(condition_value)\n",
    "                else:\n",
    "                    # If the condition is a single value\n",
    "                    condition_filter &= (combined_data[condition_column] == condition_value)\n",
    "            \n",
    "            # Filter combinedData for the specific subject and conditions\n",
    "            subject_condition_data = combined_data[condition_filter]\n",
    "            \n",
    "            # Load MNE objects and update with accuracy data\n",
    "            mne_objects = load_mne_objects(sub, output_name, task, just_HG_ev1_rescaled=just_HG_ev1_rescaled, LAB_root=None)\n",
    "            \n",
    "            if sub in acc_array:\n",
    "                trial_counts = subject_condition_data['trialCount'].values.astype(int)\n",
    "                accuracy_data = [acc_array[sub][i-1] for i in trial_counts if i-1 < len(acc_array[sub])] # Subtract 1 here for zero-based indexing in acc array.\n",
    "                # Now pass trial_counts along with accuracy_data\n",
    "                mne_objects['HG_ev1_rescaled'] = utils.add_accuracy_to_epochs(mne_objects['HG_ev1_rescaled'], accuracy_data)\n",
    "\n",
    "            sub_mne_objects[output_name] = mne_objects\n",
    "        subjects_mne_objects[sub] = sub_mne_objects\n",
    "\n",
    "    return subjects_mne_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for subject: D0057\n",
      "  Loading output: Stimulus_i25s25_fixationCrossBase_1sec_mirror_0to1Test with conditions: {'congruency': 'i', 'congruencyProportion': '75%', 'switchType': 's', 'switchProportion': '25%'}\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_i25s25_fixationCrossBase_1sec_mirror_0to1Test_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "7 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 0 columns\n",
      "  Loading output: Stimulus_i25s75_fixationCrossBase_1sec_mirror_0to1Test with conditions: {'congruency': 'i', 'congruencyProportion': '75%', 'switchType': 's', 'switchProportion': '75%'}\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_i25s75_fixationCrossBase_1sec_mirror_0to1Test_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "22 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 0 columns\n",
      "  Loading output: Stimulus_i75s25_fixationCrossBase_1sec_mirror_0to1Test with conditions: {'congruency': 'i', 'congruencyProportion': '25%', 'switchType': 's', 'switchProportion': '25%'}\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_i75s25_fixationCrossBase_1sec_mirror_0to1Test_HG_ev1_rescaled-epo.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    1500.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "23 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Adding metadata with 0 columns\n",
      "  Loading output: Stimulus_i75s75_fixationCrossBase_1sec_mirror_0to1Test with conditions: {'congruency': 'i', 'congruencyProportion': '25%', 'switchType': 's', 'switchProportion': '75%'}\n",
      "Reading C:\\Users\\jz421\\Box\\CoganLab\\BIDS-1.1_GlobalLocal\\BIDS\\derivatives\\freqFilt\\figs\\D0057\\D0057_Stimulus_i75s75_fixationCrossBase_1sec_mirror_0to1Test_HG_ev1_rescaled-epo.fif ...\n"
     ]
    }
   ],
   "source": [
    "output_names = [\"Stimulus_i25s25_fixationCrossBase_1sec_mirror_0to1Test\", \"Stimulus_i25s75_fixationCrossBase_1sec_mirror_0to1Test\", \n",
    "                \"Stimulus_i75s25_fixationCrossBase_1sec_mirror_0to1Test\", \"Stimulus_i75s75_fixationCrossBase_1sec_mirror_0to1Test\", \n",
    "                \"Stimulus_i25r25_fixationCrossBase_1sec_mirror_0to1Test\", \"Stimulus_i25r75_fixationCrossBase_1sec_mirror_0to1Test\", \n",
    "                \"Stimulus_i75r25_fixationCrossBase_1sec_mirror_0to1Test\", \"Stimulus_i75r75_fixationCrossBase_1sec_mirror_0to1Test\", \n",
    "                \"Stimulus_c25s25_fixationCrossBase_1sec_mirror_0to1Test\", \"Stimulus_c25s75_fixationCrossBase_1sec_mirror_0to1Test\", \n",
    "                \"Stimulus_c75s25_fixationCrossBase_1sec_mirror_0to1Test\", \"Stimulus_c75s75_fixationCrossBase_1sec_mirror_0to1Test\",\n",
    "                \"Stimulus_c25r25_fixationCrossBase_1sec_mirror_0to1Test\", \"Stimulus_c25r75_fixationCrossBase_1sec_mirror_0to1Test\",\n",
    "                \"Stimulus_c75r25_fixationCrossBase_1sec_mirror_0to1Test\", \"Stimulus_c75r75_fixationCrossBase_1sec_mirror_0to1Test\",]\n",
    "                \n",
    "output_names_conditions = {\n",
    "    \"Stimulus_i25s25_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus_i25s75_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus_i75s25_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus_i75s75_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus_i25r25_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus_i25r75_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus_i75r25_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus_i75r75_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"i\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus_c25s25_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus_c25s75_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus_c75s25_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus_c75s75_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"s\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus_c25r25_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus_c25r75_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"75%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },\n",
    "    \"Stimulus_c75r25_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"25%\"\n",
    "    },\n",
    "    \"Stimulus_c75r75_fixationCrossBase_1sec_mirror_0to1Test\": {\n",
    "        \"congruency\": \"c\",\n",
    "        \"congruencyProportion\": \"25%\",\n",
    "        \"switchType\": \"r\",\n",
    "        \"switchProportion\": \"75%\"\n",
    "    },                                                            \n",
    "}\n",
    "\n",
    "task='GlobalLocal'\n",
    "\n",
    "# Assuming 'combined_data' is your DataFrame and 'subjects' is your list of subject IDs\n",
    "subjects_mne_objects = create_subjects_mne_objects_dict(subjects, output_names_conditions, task=\"GlobalLocal\", combined_data=combined_data, acc_array=acc_array, just_HG_ev1_rescaled=True)\n",
    "\n",
    "# Save the subjects_mne_objects dictionary to a file\n",
    "# joblib.dump(subjects_mne_objects, 'subjects_mne_objects.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the subjects_mne_objects from the file. This is not equal to when we create subjects_mne_objects directly...test this later. 5/6.\n",
    "# subjects_mne_objects_test = joblib.load('subjects_mne_objects.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load stimulus significant channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_chans_per_subject = utils.get_sig_chans_per_subject(subjects, task='GlobalLocal', LAB_root=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get electrodes for each roi (definitions based on destrieux atlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois_dict = {\n",
    "    # 'dlpfc': [\"G_front_middle\", \"G_front_sup\", \"S_front_inf\", \"S_front_middle\", \"S_front_sup\"],\n",
    "    # 'acc': [\"G_and_S_cingul-Ant\", \"G_and_S_cingul-Mid-Ant\"],\n",
    "    # 'parietal': [\"G_parietal_sup\", \"S_intrapariet_and_P_trans\", \"G_pariet_inf-Angular\", \"G_pariet_inf-Supramar\"],\n",
    "    'lpfc': [\"G_front_inf-Opercular\", \"G_front_inf-Orbital\", \"G_front_inf-Triangul\", \"G_front_middle\", \"G_front_sup\", \"Lat_Fis-ant-Horizont\", \"Lat_Fis-ant-Vertical\", \"S_circular_insula_ant\", \"S_circular_insula_sup\", \"S_front_inf\", \"S_front_middle\", \"S_front_sup\"],\n",
    "    'v1': [\"G_oc-temp_med-Lingual\", \"S_calcarine\", \"G_cuneus\"]\n",
    "}\n",
    "\n",
    "rois = list(rois_dict.keys())\n",
    "electrodes_per_subject_roi, sig_electrodes_per_subject_roi = utils.make_sig_electrodes_per_subject_and_roi_dict(rois_dict, subjects_electrodestoROIs_dict, sig_chans_per_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_electrodes_per_subject_roi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get electrode counts for each roi (just for fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_electrodes_info = utils.calculate_total_electrodes(sig_electrodes_per_subject_roi, electrodes_per_subject_roi)\n",
    "for roi, counts in total_electrodes_info.items():\n",
    "    print(f\"Total number of significant {roi} electrodes across all subjects:\", counts['total_significant_electrodes'])\n",
    "    print(f\"Total number of {roi} electrodes across all subjects:\", counts['total_electrodes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if any subjects have a different sampling rate than 2048 Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sampling_rates(subjects_mne_objects):\n",
    "    # This dictionary will store subjects with different sampling rates\n",
    "    different_sampling_rates = {}\n",
    "    \n",
    "    # Iterate through each subject and their corresponding data\n",
    "    for subject, data in subjects_mne_objects.items():\n",
    "        # Access the specific Epochs object and check its sampling rate\n",
    "        if 'Stimulus_i25s25_fixationCrossBase_1sec_mirror_0to1Test' in data:\n",
    "            epochs = data['Stimulus_i25s25_fixationCrossBase_1sec_mirror_0to1Test']['HG_ev1_rescaled']\n",
    "            sampling_rate = epochs.info['sfreq']\n",
    "            \n",
    "            # Check if the sampling rate is not 2048.0 Hz\n",
    "            if sampling_rate != 2048.0:\n",
    "                different_sampling_rates[subject] = sampling_rate\n",
    "    \n",
    "    return different_sampling_rates\n",
    "\n",
    "# Assuming 'subjects_mne_objects' is your dictionary containing MNE objects for each subject\n",
    "different_rates = check_sampling_rates(subjects_mne_objects)\n",
    "\n",
    "# Print the results\n",
    "if different_rates:\n",
    "    print(\"Subjects with different sampling rates:\")\n",
    "    for subject, rate in different_rates.items():\n",
    "        print(f\"Subject {subject} has a sampling rate of {rate} Hz.\")\n",
    "else:\n",
    "    print(\"All subjects have a sampling rate of 2048.0 Hz.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get trial averaged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time indices just for the function to work. Don't actually use time averaged data.\n",
    "time_indices = {\n",
    "    'firstHalfSecond': (2048, 3072),\n",
    "    'secondHalfSecond': (3072, 4096),\n",
    "    'fullSecond': (2048, 4096)\n",
    "}\n",
    "# Process the data\n",
    "data_trialAvg_lists, data_trialStd_lists, _, overall_electrode_mapping, electrode_mapping_per_roi = utils.process_data_for_roi(\n",
    "    subjects_mne_objects, output_names, rois, subjects, sig_electrodes_per_subject_roi, time_indices)\n",
    "\n",
    "# need to figure out how to use the overall_electrode_mapping to get the electrode names back. Apparently I do this in the ANOVA in roi analysis. 5/5.\n",
    "\n",
    "# Concatenate the data\n",
    "concatenated_trialAvg_data = utils.concatenate_data(data_trialAvg_lists, rois, output_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_std(data_dict):\n",
    "    \"\"\"\n",
    "    Computes the mean and standard deviation across electrodes for each time point in each ROI and output name.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dict (dict): A dictionary containing trial-averaged neural data (concatenated_trialAvg_data). It is expected to have a structure where each key is an output name,\n",
    "      and its value is another dictionary. This nested dictionary should have ROIs as keys, and arrays of shape (n_electrodes, n_timepoints)\n",
    "      as values.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing two dictionaries:\n",
    "        - mean_data: A dictionary with the same structure as input, but each array contains the mean across electrodes at each timepoint.\n",
    "        - std_data: A dictionary with the same structure as input, but each array contains the standard deviation across electrodes at each timepoint.\n",
    "\n",
    "    Example:\n",
    "    mean_trialAvg_data, std_trialAvg_data = compute_avg_std(concatenated_trialAvg_data)\n",
    "    print(\"Mean data for LPFC:\", mean_trialAvg_data['Stimulus_i25s25_fixationCrossBase_1sec_mirror_0to1Test']['lpfc'])\n",
    "    print(\"Standard deviation data for LPFC:\", std_trialAvg_data['Stimulus_i25s25_fixationCrossBase_1sec_mirror_0to1Test']['lpfc'])\n",
    "    \"\"\"\n",
    "    mean_data = {}\n",
    "    std_data = {}\n",
    "    for output_name, rois_data in data_dict.items():\n",
    "        mean_data[output_name] = {}\n",
    "        std_data[output_name] = {}\n",
    "        for roi, data in rois_data.items():\n",
    "            # Compute mean and std across the electrodes (axis=0)\n",
    "            mean_data[output_name][roi] = np.mean(data, axis=0)\n",
    "            std_data[output_name][roi] = np.std(data, axis=0)\n",
    "    return mean_data, std_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean and std across electrodes for each time point, in the already trial-averaged data.\n",
    "mean_across_elecs_trialAvg_data, std_across_elecs_trialAvg_data = compute_avg_std(concatenated_trialAvg_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try kumar's suggestion of flattening the data to be more data driven 5/7  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_trialAvg_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do RSA!  \n",
    "1. Create time windows of 20 ms width, at 5 ms steps through the electrode and trial-averaged data.  \n",
    "2. For each time window and ROI, calculate (1-Spearman correlation coefficient) between each condition. This will make a 16x16 neural RDM for each time window and ROI.  \n",
    "3. For each neural RDM, find its correlation with each model RDM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is old and doesn't incorporate p-values as of 5/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_windows(data, window_size_ms, step_size_ms, sampling_rate):\n",
    "    \"\"\"\n",
    "    Generates a list of tuples representing the start and end samples for time windows.\n",
    "\n",
    "    Parameters:\n",
    "    - data: ndarray\n",
    "        The electrode and trial averaged array from which to calculate the number of samples. Should just be a time series vector.\n",
    "    - window_size_ms: int\n",
    "        Size of each time window in milliseconds.\n",
    "    - step_size_ms: int\n",
    "        The step size to move from one window to the next in milliseconds.\n",
    "    - sampling_rate: float\n",
    "        The sampling rate of the data in Hz.\n",
    "\n",
    "    Returns:\n",
    "    - list of tuples\n",
    "        Each tuple contains the start and end sample indices for a time window.\n",
    "    \"\"\"\n",
    "    window_size_samples = int(window_size_ms * sampling_rate / 1000)\n",
    "    step_size_samples = int(step_size_ms * sampling_rate / 1000)\n",
    "    total_samples = data.shape[0]\n",
    "    windows = [(start, start + window_size_samples) for start in range(0, total_samples - window_size_samples + 1, step_size_samples)]\n",
    "    return windows\n",
    "\n",
    "def make_spearman_rdm(data_list):\n",
    "    \"\"\"\n",
    "    Flattens each 2D array in data_list and calculates pairwise Spearman correlations.\n",
    "\n",
    "    Parameters:\n",
    "    - data_list: list of ndarrays\n",
    "        List of 2D arrays where each array corresponds to a condition.\n",
    "\n",
    "    Returns:\n",
    "    - ndarray\n",
    "        A square array (RDM) where each element (i, j) is the Spearman correlation distance (1 - rho) between conditions i and j.\n",
    "    \"\"\"\n",
    "    num_conditions = len(data_list)\n",
    "    rdm = np.zeros((num_conditions, num_conditions))\n",
    "\n",
    "    # Flatten each array and compute pairwise correlations\n",
    "    # flattened_data = [data.reshape(-1) for data in data_list]\n",
    "\n",
    "    # Process each data array conditionally based on its dimensionality\n",
    "    flattened_data = [data.flatten() if data.ndim > 1 else data for data in data_list]\n",
    "\n",
    "    for i in range(num_conditions):\n",
    "        for j in range(num_conditions):\n",
    "            if i != j:\n",
    "                correlation, _ = stats.spearmanr(flattened_data[i], flattened_data[j])\n",
    "                rdm[i, j] = 1 - correlation\n",
    "            else:\n",
    "                rdm[i, j] = 0  # Diagonal elements are zero (perfect correlation with itself)\n",
    "    return rdm\n",
    "\n",
    "def make_neural_rdms_whole_roi(data_dict, sampling_rate=2048.0, window_size_ms=100, step_size_ms=20):\n",
    "    \"\"\"\n",
    "    Makes a neural RDM across all conditions for each ROI and time window.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dict: dict\n",
    "        A dictionary where each key is a condition and each value is another dict with keys as ROIs\n",
    "        and values as time series data arrays.\n",
    "    - sampling_rate: int\n",
    "        The sampling rate in the data, extracted from epochs.info['sfreq]. Should be 2048 Hz for all subjects for HG_ev1_rescaled.\n",
    "\n",
    "    Returns:\n",
    "    - dict\n",
    "        A dictionary where each key is an ROI and each value is a list of tuples, each containing a sample window (40 samples wide, ~19.5 ms) and its corresponding RDM.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve an example array to set up sample windows\n",
    "    example_condition = next(iter(data_dict.values()))  # Get the first condition's data\n",
    "    example_roi_array = next(iter(example_condition.values()))  # Get the first ROI's data array from that condition\n",
    "    example_electrode_data = example_roi_array[0, :] # get the first electrode (row) time series from the example roi array\n",
    "    sample_windows = create_time_windows(example_electrode_data, window_size_ms, step_size_ms, sampling_rate)\n",
    "\n",
    "    all_rdms = {}\n",
    "    # First organize the data by ROI, aggregating across all conditions\n",
    "    roi_data = {}\n",
    "    for output_name, rois in data_dict.items():\n",
    "        for roi, electrode_time_data in rois.items():\n",
    "            if roi not in roi_data:\n",
    "                roi_data[roi] = []\n",
    "            roi_data[roi].append(electrode_time_data)\n",
    "\n",
    "    # Calculate RDMs for each ROI\n",
    "    for roi, electrode_time_data_list in roi_data.items():\n",
    "        all_rdms[roi] = []\n",
    "        for window in sample_windows:\n",
    "            windowed_data_list = [condition_array[:, window[0]:window[1]] for condition_array in electrode_time_data_list]\n",
    "            rdm = make_spearman_rdm(windowed_data_list)\n",
    "            all_rdms[roi].append((window, rdm))\n",
    "    \n",
    "    return all_rdms\n",
    "\n",
    "def calculate_rsa_whole_roi(neural_rdms, model_rdms, rois):\n",
    "    \"\"\"\n",
    "    Calculates the RSA between neural RDMs and model RDMs, using only the lower triangular part of the RDMs (excluding the diagonal).\n",
    "\n",
    "    Parameters:\n",
    "    - neural_rdms: dict\n",
    "        A dictionary where keys are ROIs, each containing a list of tuples with sample windows and corresponding RDMs.\n",
    "    - model_rdms: dict\n",
    "        A dictionary containing the model RDMs with keys as model names.\n",
    "    - rois: list\n",
    "        List of ROIs to be analyzed.\n",
    "\n",
    "    Returns:\n",
    "    - dict\n",
    "        A structured dictionary containing RSA results mapped by ROI, model type, and sample window.\n",
    "    \"\"\"\n",
    "    rsa_results = {roi: {model_name: [] for model_name in model_rdms} for roi in rois}\n",
    "    \n",
    "    # Iterate over each ROI\n",
    "    for roi in neural_rdms:\n",
    "        for window, neural_rdm in neural_rdms[roi]:\n",
    "            # Extract the lower triangular part of the neural RDM, excluding the diagonal\n",
    "            tri_indices = np.tril_indices_from(neural_rdm, k=-1)\n",
    "            neural_rdm_tril = neural_rdm[tri_indices]\n",
    "\n",
    "            for model_name, model_rdm in model_rdms.items():\n",
    "                # Extract the lower triangular part of the model RDM, excluding the diagonal\n",
    "                model_rdm_tril = model_rdm[tri_indices]\n",
    "\n",
    "                # Calculate Spearman correlation between the flattened lower triangular parts of each RDM\n",
    "                correlation, _ = stats.spearmanr(neural_rdm_tril, model_rdm_tril)\n",
    "                rsa_results[roi][model_name].append((window, correlation))\n",
    "                \n",
    "    return rsa_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make neural RDM comparing conditions for time windows of 200 samples (~100 ms), with steps of 40 samples (~20 ms). Keys are ROIs. Returns a tuple where 0th element is the sample window (tuple), and 1st element is the neural RDM (np array).\n",
    "neural_rdms_by_samples_and_roi = make_neural_rdms_whole_roi(concatenated_trialAvg_data, sampling_rate=2048.0, window_size_ms=200, step_size_ms=20)\n",
    "\n",
    "# Define ROIs based on keys in your neural_rdms_by_samples_and_roi (if they are consistent)\n",
    "rois = list(neural_rdms_by_samples_and_roi.keys())\n",
    "\n",
    "# Calculate RSA\n",
    "rsa_results = calculate_rsa_whole_roi(neural_rdms_by_samples_and_roi, model_rdms, rois)\n",
    "\n",
    "# Example: Print the RSA result for a specific ROI and model\n",
    "print(\"RSA results for ROI 'lpfc' and model 'congruency':\", rsa_results['lpfc']['congruency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try to do rsa on each electrode individually too 5/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_index_mapping(sig_electrodes_per_subject_roi):\n",
    "    \"\"\"\n",
    "    Constructs a mapping from subject and electrode labels to indices for each ROI.\n",
    "    \"\"\"\n",
    "    index_mapping = {}\n",
    "    for roi, subjects in sig_electrodes_per_subject_roi.items():\n",
    "        index_mapping[roi] = {}\n",
    "        electrode_counter = 0  # Reset counter for each ROI\n",
    "        for subject, electrodes in subjects.items():\n",
    "            for electrode in electrodes:\n",
    "                index_mapping[roi][electrode_counter] = (subject, electrode)\n",
    "                electrode_counter += 1\n",
    "    return index_mapping\n",
    "\n",
    "def make_neural_rdms_per_electrode(data_dict, sig_electrodes_per_subject_roi, sampling_rate, window_size_ms, step_size_ms):\n",
    "    \"\"\"\n",
    "    Makes a neural RDM across all conditions for each subject-specific electrode in each ROI and time window, using ROI-specific indexing.\n",
    "    \"\"\"\n",
    "\n",
    "    example_condition = next(iter(data_dict.values()))  # Get the first condition's data\n",
    "    example_roi = next(iter(example_condition.values()))  # Get the first ROI's data array from that condition\n",
    "    example_electrode_data = example_roi[0, :]  # Get the first electrode (row) time series from the example roi array\n",
    "    sample_windows = create_time_windows(example_electrode_data, window_size_ms, step_size_ms, sampling_rate)\n",
    "\n",
    "    # Prepare the index mapping for accessing subject and electrode names\n",
    "    index_mapping = make_index_mapping(sig_electrodes_per_subject_roi)\n",
    "\n",
    "    # Initialize a dictionary to hold the RDMs\n",
    "    all_rdms = {roi: {} for roi in sig_electrodes_per_subject_roi}\n",
    "    # First organize the data by ROI, aggregating across all conditions\n",
    "    roi_data = {}\n",
    "    for output_name, rois in data_dict.items():\n",
    "        for roi, electrode_time_data in rois.items():\n",
    "            if roi not in roi_data:\n",
    "                roi_data[roi] = []\n",
    "            roi_data[roi].append(electrode_time_data)\n",
    "\n",
    "    # Calculate RDMs for each ROI.\n",
    "    for roi, electrode_time_data_list in roi_data.items():\n",
    "        for window in sample_windows:\n",
    "            for electrode_index in range(electrode_time_data_list[0].shape[0]):\n",
    "                windowed_data_list = [condition_array[electrode_index, window[0]:window[1]] for condition_array in electrode_time_data_list]\n",
    "                rdm = make_spearman_rdm(windowed_data_list)\n",
    "                subject, electrode = index_mapping[roi][electrode_index]\n",
    "                all_rdms[roi][subject] = {}\n",
    "                all_rdms[roi][subject][electrode] = []\n",
    "                all_rdms[roi][subject][electrode].append((window, rdm))\n",
    "\n",
    "    return all_rdms\n",
    "\n",
    "def calculate_rsa_per_electrode(neural_rdms, model_rdms, rois):\n",
    "    \"\"\"\n",
    "    Calculates the RSA between neural RDMs and model RDMs for each electrode in each ROI, including subject-specific data.\n",
    "    \n",
    "    Parameters:\n",
    "    - neural_rdms: dict\n",
    "        A dictionary of neural RDMs structured by ROI, then subject, then electrode.\n",
    "    - model_rdms: dict\n",
    "        A dictionary of model RDMs structured by ROI.\n",
    "    - rois: list\n",
    "        List of ROIs to be analyzed.\n",
    "    \n",
    "    Returns:\n",
    "    - dict\n",
    "        A structured dictionary containing RSA results mapped by ROI, subject, electrode, and sample window.\n",
    "    \"\"\"\n",
    "    rsa_results = {}\n",
    "    for roi in rois:\n",
    "        rsa_results[roi] = {}\n",
    "        for subject in neural_rdms[roi]:\n",
    "            rsa_results[roi][subject] = {}\n",
    "            for electrode in neural_rdms[roi][subject]:\n",
    "                rsa_results[roi][subject][electrode] = []\n",
    "                for window, neural_rdm in neural_rdms[roi][subject][electrode]:\n",
    "\n",
    "                    # Extract the lower triangular part of the neural RDM, excluding the diagonal\n",
    "                    tri_indices = np.tril_indices_from(neural_rdm, k=-1)\n",
    "                    neural_rdm_tril = neural_rdm[tri_indices]\n",
    "\n",
    "                    for model_name, model_rdm in model_rdms.items():\n",
    "                        # Extract the lower triangular part of the model RDM, excluding the diagonal\n",
    "                        model_rdm_tril = model_rdm[tri_indices]\n",
    "\n",
    "                        correlation, _ = stats.spearmanr(neural_rdm_tril, model_rdm_tril)\n",
    "                        rsa_results[roi][subject][electrode].append((window, correlation))\n",
    "               \n",
    "    return rsa_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_rdms_by_sub_and_elec = make_neural_rdms_per_electrode(concatenated_trialAvg_data, sig_electrodes_per_subject_roi, sampling_rate=2048.0, window_size_ms=100, step_size_ms=20)\n",
    "\n",
    "# Define ROIs based on keys in your neural_rdms_by_samples_and_roi (if they are consistent)\n",
    "rois = list(neural_rdms_by_sub_and_elec.keys())\n",
    "\n",
    "# Calculate RSA\n",
    "rsa_results_by_sub_and_elec = calculate_rsa_per_electrode(neural_rdms_by_sub_and_elec, model_rdms, rois)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot congruency rsa for each roi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot rsa results, different rois as different colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rsa_correlation_by_time(rsa_results, model_name, rois, colors, sampling_rate=2048.0, time_shift=1000):\n",
    "    \"\"\"\n",
    "    Plots RSA correlation over time for specified ROIs and model.\n",
    "\n",
    "    Parameters:\n",
    "    - rsa_results: dict\n",
    "        A dictionary containing RSA results.\n",
    "    - model_name: str\n",
    "        The model name to plot RSA results for.\n",
    "    - rois: list\n",
    "        List of ROIs to be included in the plot.\n",
    "    - colors: dict\n",
    "        A dictionary mapping ROIs to plot colors.\n",
    "    - sampling_rate: float, optional\n",
    "        The sampling rate of the data in Hz. Default is 2048.0.\n",
    "    - time_shift: int, optional\n",
    "        Time in milliseconds to shift the x-axis. Default is 1000 ms.\n",
    "    \"\"\"\n",
    "    # Create the figure and axis objects\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Loop over each ROI and its RSA results\n",
    "    for roi in rois:\n",
    "        if roi in rsa_results:\n",
    "            times = []\n",
    "            correlations = []\n",
    "            for window, correlation in rsa_results[roi][model_name]:\n",
    "                # Calculate the middle of the window\n",
    "                middle_sample = (window[0] + window[1]) / 2\n",
    "                time_ms = (middle_sample * 1000 / sampling_rate) - time_shift  # Subtract the shift to align times\n",
    "                times.append(time_ms)\n",
    "                correlations.append(correlation)\n",
    "\n",
    "            # Sort the data by time for plotting lines\n",
    "            sorted_indices = np.argsort(times)\n",
    "            times = np.array(times)[sorted_indices]\n",
    "            correlations = np.array(correlations)[sorted_indices]\n",
    "\n",
    "            # Plot each ROI's data with a specific color and connect points with lines\n",
    "            ax.scatter(times, correlations, color=colors[roi], label=f'{roi}')\n",
    "            ax.plot(times, correlations, color=colors[roi])  # This line connects the dots\n",
    "\n",
    "    # Adding labels and title\n",
    "    ax.set_xlabel('Time from Stim Onset (ms)')\n",
    "    ax.set_ylabel('Spearman Corr. Coeff.')\n",
    "    ax.set_title(f'{model_name.capitalize()} RSA Correlation over Time by ROI')\n",
    "    ax.legend()\n",
    "\n",
    "    # Set x-axis limits to reflect the new time range\n",
    "    ax.set_xlim(-1000, 1500)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rois = ['lpfc', 'v1']\n",
    "colors = {'lpfc': 'red', 'v1': 'blue'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rsa_correlation_by_time(rsa_results, 'congruency', rois, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "switch type rsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rsa_correlation_by_time(rsa_results, 'switchType', rois, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "congruency proportion rsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rsa_correlation_by_time(rsa_results, 'congruencyProportion', rois, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "switch proportion rsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rsa_correlation_by_time(rsa_results, 'switchProportion', rois, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stats  \n",
    "1. for each roi, for each neural rdm (time window), compute correlation between a randomly shuffled model rdm and neural rdm. Do this a lot of times (10,000 for now).\n",
    "2. Find proportion of random correlations that are greater than the actual correlation between the model rdm and the neural rdm. That's the p-value.\n",
    "3. Find clusters in time for each permutation where p<0.05. For each cluster get a cluster statistic (sum of correlation values within the cluster). Do this for the real data too.\n",
    "4. Get the cluster statistic for the largest cluster in each of the fake data permutations. Make a distribution of this.\n",
    "5. For each cluster in the real data, find its p-value as the proportion of fake data clusters that have a cluster statistic >= the real data cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test_rsa(neural_rdm, model_rdm, num_permutations=10000, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Performs a permutation test for RSA with cluster-based correction.\n",
    "    \"\"\"\n",
    "    actual_correlation, _ = stats.spearmanr(neural_rdm.flatten(), model_rdm.flatten())\n",
    "    \n",
    "    permuted_correlations = np.zeros(num_permutations)\n",
    "    for i in range(num_permutations):\n",
    "        shuffled_model_rdm = np.random.permutation(model_rdm.flatten())\n",
    "        permuted_corr, _ = stats.spearmanr(neural_rdm.flatten(), shuffled_model_rdm)\n",
    "        permuted_correlations[i] = permuted_corr\n",
    "\n",
    "    clusters, num_clusters = label(actual_correlation > threshold)\n",
    "    actual_cluster_size = np.sum(clusters > threshold)\n",
    "    \n",
    "    permuted_cluster_sizes = np.zeros(num_permutations)\n",
    "    for i in range(num_permutations):\n",
    "        perm_clusters, num_perm_clusters = label(permuted_correlations[i] > threshold)\n",
    "        if num_perm_clusters > 0:\n",
    "            permuted_cluster_sizes[i] = max([np.sum(permuted_correlations[i][perm_clusters == cluster]) for cluster in range(1, num_perm_clusters + 1)])\n",
    "\n",
    "    p_value = np.sum(permuted_cluster_sizes >= actual_cluster_size) / num_permutations\n",
    "    \n",
    "    return actual_correlation, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new rsa perm test with clustering, replaces roi rsa 5/16, untested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.ndimage import label\n",
    "\n",
    "def create_time_windows(data, window_size_ms, step_size_ms, sampling_rate):\n",
    "    window_size_samples = int(window_size_ms * sampling_rate / 1000)\n",
    "    step_size_samples = int(step_size_ms * sampling_rate / 1000)\n",
    "    total_samples = data.shape[0]\n",
    "    windows = [(start, start + window_size_samples) for start in range(0, total_samples - window_size_samples + 1, step_size_samples)]\n",
    "    return windows\n",
    "\n",
    "def make_spearman_rdm(data_list):\n",
    "    num_conditions = len(data_list)\n",
    "    rdm = np.zeros((num_conditions, num_conditions))\n",
    "    flattened_data = [data.flatten() if data.ndim > 1 else data for data in data_list]\n",
    "    for i in range(num_conditions):\n",
    "        for j in range(num_conditions):\n",
    "            if i != j:\n",
    "                correlation, _ = stats.spearmanr(flattened_data[i], flattened_data[j])\n",
    "                rdm[i, j] = 1 - correlation\n",
    "            else:\n",
    "                rdm[i, j] = 0\n",
    "    return rdm\n",
    "\n",
    "def make_neural_rdms_whole_roi(data_dict, sampling_rate=2048.0, window_size_ms=100, step_size_ms=20):\n",
    "    example_condition = next(iter(data_dict.values()))\n",
    "    example_roi_array = next(iter(example_condition.values()))\n",
    "    example_electrode_data = example_roi_array[0, :]\n",
    "    sample_windows = create_time_windows(example_electrode_data, window_size_ms, step_size_ms, sampling_rate)\n",
    "\n",
    "    all_rdms = {}\n",
    "    roi_data = {}\n",
    "    for output_name, rois in data_dict.items():\n",
    "        for roi, electrode_time_data in rois.items():\n",
    "            if roi not in roi_data:\n",
    "                roi_data[roi] = []\n",
    "            roi_data[roi].append(electrode_time_data)\n",
    "\n",
    "    for roi, electrode_time_data_list in roi_data.items():\n",
    "        all_rdms[roi] = []\n",
    "        for window in sample_windows:\n",
    "            windowed_data_list = [condition_array[:, window[0]:window[1]] for condition_array in electrode_time_data_list]\n",
    "            rdm = make_spearman_rdm(windowed_data_list)\n",
    "            all_rdms[roi].append((window, rdm))\n",
    "    \n",
    "    return all_rdms\n",
    "\n",
    "def permutation_test_rsa_with_clustering(neural_rdms, model_rdm, num_permutations=10000, threshold=0.05):\n",
    "    actual_correlations = []\n",
    "    permuted_correlations = np.zeros((num_permutations, len(neural_rdms)))\n",
    "\n",
    "    for idx, (window, neural_rdm) in enumerate(neural_rdms):\n",
    "        neural_rdm_tril = neural_rdm[np.tril_indices_from(neural_rdm, k=-1)]\n",
    "        model_rdm_tril = model_rdm[np.tril_indices_from(model_rdm, k=-1)]\n",
    "        \n",
    "        actual_corr, _ = stats.spearmanr(neural_rdm_tril, model_rdm_tril)\n",
    "        actual_correlations.append(actual_corr)\n",
    "        \n",
    "        for perm in range(num_permutations):\n",
    "            shuffled_model_rdm_tril = np.random.permutation(model_rdm_tril)\n",
    "            perm_corr, _ = stats.spearmanr(neural_rdm_tril, shuffled_model_rdm_tril)\n",
    "            permuted_correlations[perm, idx] = perm_corr\n",
    "    \n",
    "    actual_correlations = np.array(actual_correlations)\n",
    "    \n",
    "    clusters, num_clusters = label(actual_correlations > threshold)\n",
    "    cluster_sizes = np.array([np.sum(actual_correlations[clusters == cluster]) for cluster in range(1, num_clusters + 1)])\n",
    "    \n",
    "    max_permuted_cluster_sizes = np.zeros(num_permutations)\n",
    "    for perm in range(num_permutations):\n",
    "        perm_clusters, num_perm_clusters = label(permuted_correlations[perm, :] > threshold)\n",
    "        if num_perm_clusters > 0:\n",
    "            max_permuted_cluster_sizes[perm] = max([np.sum(permuted_correlations[perm, perm_clusters == cluster]) for cluster in range(1, num_perm_clusters + 1)])\n",
    "    \n",
    "    p_values = np.array([np.sum(max_permuted_cluster_sizes >= size) / num_permutations for size in cluster_sizes])\n",
    "    return p_values\n",
    "\n",
    "def calculate_rsa_with_permutation_testing(neural_rdms, model_rdms, rois, num_permutations=10000, threshold=0.05):\n",
    "    rsa_results = {roi: {model_name: [] for model_name in model_rdms} for roi in rois}\n",
    "    \n",
    "    for roi in rois:\n",
    "        for model_name, model_rdm in model_rdms.items():\n",
    "            p_values = permutation_test_rsa_with_clustering(neural_rdms[roi], model_rdm, num_permutations, threshold)\n",
    "            for idx, (window, neural_rdm) in enumerate(neural_rdms[roi]):\n",
    "                neural_rdm_tril = neural_rdm[np.tril_indices_from(neural_rdm, k=-1)]\n",
    "                model_rdm_tril = model_rdm[np.tril_indices_from(model_rdm, k=-1)]\n",
    "                correlation, _ = stats.spearmanr(neural_rdm_tril, model_rdm_tril)\n",
    "                rsa_results[roi][model_name].append((window, correlation, p_values[idx] if idx < len(p_values) else np.nan))\n",
    "    \n",
    "    return rsa_results\n",
    "\n",
    "def calculate_rsa_whole_roi(data_dict, model_rdms, rois, sampling_rate=2048.0, window_size_ms=100, step_size_ms=20, num_permutations=10000, threshold=0.05):\n",
    "    neural_rdms = make_neural_rdms_whole_roi(data_dict, sampling_rate, window_size_ms, step_size_ms)\n",
    "    rsa_results = calculate_rsa_with_permutation_testing(neural_rdms, model_rdms, rois, num_permutations, threshold)\n",
    "    return rsa_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_rdms_by_samples_and_roi_test = make_neural_rdms_whole_roi(concatenated_trialAvg_data, sampling_rate=2048.0, window_size_ms=200, step_size_ms=20)\n",
    "rois = list(neural_rdms_by_samples_and_roi.keys())\n",
    "rsa_results_test = calculate_rsa_whole_roi(concatenated_trialAvg_data, model_rdms, rois)\n",
    "\n",
    "# Example: Print the RSA result for a specific ROI and model\n",
    "print(\"RSA results for ROI 'lpfc' and model 'congruency':\", rsa_results_test['lpfc']['congruency'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
